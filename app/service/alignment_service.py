"""
å¤šè¯­å¯¹ç…§è®°å¿†å·¥å…· - Web Service
å°†åŸæ–‡ä¸è¯‘æ–‡æ–‡æ¡£ï¼ˆDOCX/PPTX/Excelï¼‰é€šè¿‡ LLM è¿›è¡Œå¥çº§å¯¹é½ï¼Œè¾“å‡º Excelã€‚
"""

import os
import re
import shutil
import asyncio
import uuid
import traceback
import threading
import importlib.util
from datetime import datetime
from pathlib import Path
from typing import Optional

import pandas as pd
from lxml import etree
from docx import Document
from docx.table import Table
from docx.text.paragraph import Paragraph
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE
from openai import OpenAI

from app.core.config import settings

# â”€â”€ å…¨å±€é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROW_BUCKET = 20_000
API_KEY = "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"
BASE_URL = "https://openrouter.ai/api/v1"

THRESHOLD_MAP = {
    2: 25_000, 3: 50_000, 4: 75_000, 5: 100_000,
    6: 125_000, 7: 150_000, 8: 175_000,
}
BUFFER_CHARS = 2000
OUTPUT_DIR = settings.OUTPUT_DIR

AVAILABLE_MODELS = {
    "Google Gemini 2.5 Flash": {
        "id": "google/gemini-2.5-flash",
        "description": "å»ºè®®å…ˆæ£€æŸ¥æ–‡ç« æ˜¯å¦æœ‰ç›®å½•ï¼Œå…ˆå°†ç›®å½•åˆ é™¤å†å¤„ç†",
        "max_output": 65536,
    },
        "Google gemini-3-flash-preview": {
        "id": "google/gemini-3-flash-preview",
        "description": "å»ºè®®å…ˆæ£€æŸ¥æ–‡ç« æ˜¯å¦æœ‰ç›®å½•ï¼Œå…ˆå°†ç›®å½•åˆ é™¤å†å¤„ç†",
        "max_output": 65536,
    },
    "Google Gemini 2.5 Pro": {
        "id": "google/gemini-2.5-pro",
        "description": "PPTæ¨è-å¢å¼ºï¼Œé€Ÿåº¦ç¨æ…¢ï¼Œ100ä¸‡ä¸Šä¸‹æ–‡ï¼Œ65Kè¾“å‡º",
        "max_output": 65536,
    },
    "Google: Gemini 3 Pro Preview": {
        "id": "google/gemini-3-pro-preview",
        "description": "æœ€å¼ºæ¨ç†ï¼Œ100ä¸‡ä¸Šä¸‹æ–‡ï¼Œ65Kè¾“å‡º",
        "max_output": 65536,
    },
        "Google: google/gemini-3.1-pro-preview": {
        "id": "google/gemini-3.1-pro-preview",
        "description": "æœ€å¼ºæ¨ç†ï¼Œ100ä¸‡ä¸Šä¸‹æ–‡ï¼Œ65Kè¾“å‡º",
        "max_output": 65536,
    },
}
DEFAULT_MODEL = "Google Gemini 2.5 Flash"

CHAPTER_PATTERNS = [
    r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+[ç« èŠ‚ç¯‡éƒ¨]', r'^Chapter\s*\d+', r'^CHAPTER\s*\d+',
    r'^\d+[\.ã€]\s*\S+', r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€.]\s*\S+',
    r'^Part\s*\d+', r'^PART\s*\d+', r'^Section\s*\d+',
]

SUPPORTED_LANGUAGES = {
    "ä¸­æ–‡": {"code": "zh", "english_name": "Chinese", "char_pattern": r'[\u4e00-\u9fa5]', "word_based": False, "description": "ä¸­æ–‡ï¼ˆç®€ä½“/ç¹ä½“ï¼‰"},
    "è‹±è¯­": {"code": "en", "english_name": "English", "char_pattern": r'\b[a-zA-Z]+\b', "word_based": True, "description": "English"},
    "è¥¿ç­ç‰™è¯­": {"code": "es", "english_name": "Spanish", "char_pattern": r'\b[a-zA-ZÃ¡Ã©Ã­Ã³ÃºÃ¼Ã±ÃÃ‰ÃÃ“ÃšÃœÃ‘]+\b', "word_based": True, "description": "EspaÃ±ol"},
    "è‘¡è¯­": {"code": "pt", "english_name": "Portuguese", "char_pattern": r'\b[a-zA-ZÃ¡Ã©Ã­Ã³ÃºÃ¢ÃªÃ´Ã£ÃµÃ§ÃÃ‰ÃÃ“ÃšÃ‚ÃŠÃ”ÃƒÃ•Ã‡]+\b', "word_based": True, "description": "PortuguÃªs"},
    "æ—¥è¯­": {"code": "ja", "english_name": "Japanese", "char_pattern": r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', "word_based": False, "description": "æ—¥æœ¬èª"},
    "ä¿„è¯­": {"code": "ru", "english_name": "Russian", "char_pattern": r'\b[Ğ°-ÑĞ-Ğ¯Ñ‘Ğ]+\b', "word_based": True, "description": "Ğ ÑƒÑÑĞºĞ¸Ğ¹"},
    "éŸ©è¯­": {"code": "ko", "english_name": "Korean", "char_pattern": r'[\uAC00-\uD7AF\u1100-\u11FF]', "word_based": False, "description": "í•œêµ­ì–´"},
    "é˜¿è¯­": {"code": "ar", "english_name": "Arabic", "char_pattern": r'[\u0600-\u06FF\u0750-\u077F]+', "word_based": True, "description": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"},
    "æ³•è¯­": {"code": "fr", "english_name": "French", "char_pattern": r'\b[a-zA-ZÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼Ã¿Å“Ã¦Ã§Ã€Ã‚Ã„Ã‰ÃˆÃŠÃ‹ÃÃÃ”Ã™Ã›ÃœÅ¸Å’Ã†Ã‡]+\b', "word_based": True, "description": "FranÃ§ais"},
    "æ³¢å…°è¯­": {"code": "pl", "english_name": "Polish", "char_pattern": r'\b[a-zA-ZÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼Ä„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»]+\b', "word_based": True, "description": "Polski"},
    "æ„å¤§åˆ©è¯­": {"code": "it", "english_name": "Italian", "char_pattern": r'\b[a-zA-ZÃ Ã¨Ã©Ã¬Ã­Ã®Ã²Ã³Ã¹ÃºÃ€ÃˆÃ‰ÃŒÃÃÃ’Ã“Ã™Ãš]+\b', "word_based": True, "description": "Italiano"},
    "å¾·è¯­": {"code": "de", "english_name": "German", "char_pattern": r'\b[a-zA-ZÃ¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ]+\b', "word_based": True, "description": "Deutsch"},
}

# â”€â”€ è¿›åº¦è¿½è¸ª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_task_progress: dict = {}
_memory_module = None
# å½“å‰æ‰§è¡Œä»»åŠ¡çš„ task_idï¼Œç”¨äºå°† memory çš„ log_stream å†™å…¥è¯¥ä»»åŠ¡çš„ stream_log
_stream_task_id = threading.local()


def _get_memory_module():
    global _memory_module
    if _memory_module is not None:
        return _memory_module

    memory_file = Path(__file__).resolve().parents[2] / "memory" / "memory.py"
    spec = importlib.util.spec_from_file_location("memory_legacy_module", str(memory_file))
    if spec is None or spec.loader is None:
        raise RuntimeError(f"æ— æ³•åŠ è½½ memory.py: {memory_file}")
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    _memory_module = module
    return _memory_module


def _update_progress(task_id: str, progress: int, message: str, **extra):
    data = {
        "status": "processing",
        "progress": progress,
        "message": message,
        **extra,
    }
    # ä¿ç•™å·²æœ‰ stream_logï¼Œé¿å…è¢«è¦†ç›–
    if task_id in _task_progress and "stream_log" in _task_progress[task_id]:
        data.setdefault("stream_log", _task_progress[task_id]["stream_log"])
    _task_progress[task_id] = data


def _complete_task(task_id: str, *, result: dict = None, error: str = None):
    if error:
        stream_log = _task_progress.get(task_id, {}).get("stream_log", "")
        _task_progress[task_id] = {
            "status": "failed", "progress": 100, "message": "å¤„ç†å¤±è´¥", "error": error,
            "stream_log": stream_log,
        }
    else:
        stream_log = _task_progress.get(task_id, {}).get("stream_log", "")
        if result is not None:
            result = dict(result)
            result["stream_log"] = stream_log
        _task_progress[task_id] = {"status": "done", "progress": 100, "message": "å¤„ç†å®Œæˆ", "result": result}


def get_alignment_progress(task_id: str) -> Optional[dict]:
    return _task_progress.get(task_id)


# â”€â”€ Prompt ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_ppt_alignment_prompt(source_lang="ä¸­æ–‡", target_lang="è‹±è¯­"):
    return f"""
ä½ æ˜¯ä¸€ä¸ªã€ŒPPT åŒè¯­æ–‡æœ¬å¯¹é½å™¨ã€ã€‚

å½“å‰ä»»åŠ¡ï¼šå°† {source_lang} åŸæ–‡ä¸ {target_lang} è¯‘æ–‡è¿›è¡Œå¯¹é½ã€‚

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ã€æ ¸å¿ƒé“å¾‹ - è¿åä»»æ„ä¸€æ¡å³ä¸ºå¤±è´¥ã€‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. ç¦æ­¢ç¼–é€ ï¼šå·¦ä¾§åªèƒ½å¡«åŸæ–‡ä¸­å®é™…å­˜åœ¨çš„æ–‡å­—ï¼Œå³ä¾§åªèƒ½å¡«è¯‘æ–‡ä¸­å®é™…å­˜åœ¨çš„æ–‡å­—
2. ç¦æ­¢å ä½ç¬¦ï¼šç»å¯¹ä¸å…è®¸å‡ºç° XXXã€---ã€...ã€[ç©º]ã€[æ— ]ã€N/A ç­‰ä»»ä½•å ä½ç¬¦å·
3. ç¦æ­¢è·¨é¡µï¼šå¹»ç¯ç‰‡ N çš„å†…å®¹åªèƒ½ä¸å¹»ç¯ç‰‡ N çš„å†…å®¹é…å¯¹
4. åˆ†éš”ç¬¦ç‹¬ç«‹ï¼šã€Œ---- å¹»ç¯ç‰‡ N ----ã€å¿…é¡»å•ç‹¬æˆè¡Œï¼Œä¸å‚ä¸ ||| é…å¯¹

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ã€è¾“å‡ºæ ¼å¼ - ä¸¥æ ¼éµå®ˆã€‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
---- å¹»ç¯ç‰‡ 1 ----
{source_lang}è¡ŒA ||| {target_lang}è¡ŒA
{source_lang}è¡ŒB ||| {target_lang}è¡ŒB
{source_lang}è¡ŒC |||
||| {target_lang}è¡ŒD
---- å¹»ç¯ç‰‡ 2 ----
...

æ ¼å¼è¯´æ˜ï¼š
- åˆ†éš”è¡Œã€Œ---- å¹»ç¯ç‰‡ N ----ã€ç‹¬ç«‹ä¸€è¡Œï¼Œå‰åä¸åŠ  |||
- åŒ¹é…æˆåŠŸï¼šåŸæ–‡ ||| è¯‘æ–‡
- åŸæ–‡æ— å¯¹åº”è¯‘æ–‡ï¼šåŸæ–‡ |||
- è¯‘æ–‡æ— å¯¹åº”åŸæ–‡ï¼š||| è¯‘æ–‡

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ã€å¯¹é½ç­–ç•¥ - æŒ‰ä¼˜å…ˆçº§æ‰§è¡Œã€‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ç¬¬ä¸€æ­¥ï¼šé”šç‚¹è¯†åˆ«ï¼ˆåŒé¡µå†…ï¼‰
  â”œâ”€ æ•°å­—/æ—¥æœŸé”šç‚¹ï¼š12.15 â†” December 15ï¼Œ2024å¹´ â†” 2024
  â”œâ”€ ä¸“åé”šç‚¹ï¼šå…¬å¸åã€äººåã€äº§å“åï¼ˆå³ä½¿ä¸€ä¾§æ˜¯éŸ³è¯‘/æ„è¯‘ï¼‰
  â”œâ”€ ç¼–å·é”šç‚¹ï¼šâ‘  â†” 1)ï¼Œ1. â†” (1)ï¼Œæ³¨1 â†” Note 1
  â””â”€ æ ‡é¢˜é”šç‚¹ï¼šé€šå¸¸å­—ä½“å¤§ã€ä½ç½®é ä¸Šã€å†…å®¹æ¦‚æ‹¬æ€§å¼º

ç¬¬äºŒæ­¥ï¼šè„šæ³¨ç‰¹æ®Šå¤„ç†
  â”œâ”€ è¯†åˆ«è„šæ³¨åŒºåŸŸï¼šé€šå¸¸åœ¨é¡µé¢åº•éƒ¨ï¼Œå¸¦ä¸Šæ ‡æ•°å­—æˆ–ç‰¹æ®Šæ ‡è®°
  â”œâ”€ ç¼–å·ä¸å†…å®¹ç»‘å®šï¼šã€Œ1 XXXè¯´æ˜æ–‡å­—ã€æ•´ä½“è§†ä¸ºä¸€ä¸ªè„šæ³¨å•å…ƒ
  â”œâ”€ é…å¯¹åŸåˆ™ï¼šåŸæ–‡è„šæ³¨1 â†” è¯‘æ–‡è„šæ³¨1ï¼ˆæŒ‰ç¼–å·ï¼ŒéæŒ‰ä½ç½®ï¼‰
  â””â”€ ä¸å¯æ··é…ï¼šè„šæ³¨ç¼–å·ä¸èƒ½ä¸æ­£æ–‡å†…å®¹é…å¯¹

ç¬¬ä¸‰æ­¥ï¼šè¯­ä¹‰é…å¯¹
  â”œâ”€ å«ä¹‰æœ€æ¥è¿‘çš„è¡Œ 1:1 é…å¯¹
  â”œâ”€ å…è®¸åˆç†æ‹†åˆ†ï¼šä¸€é•¿å¥ â†” ä¸¤çŸ­å¥ï¼ˆè¯­ä¹‰å®Œæ•´æ—¶ï¼‰
  â”œâ”€ å…è®¸åˆç†åˆå¹¶ï¼šä¸¤çŸ­å¥ â†” ä¸€é•¿å¥ï¼ˆè¯­ä¹‰å®Œæ•´æ—¶ï¼‰
  â””â”€ æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µåªèƒ½ä½¿ç”¨ä¸€æ¬¡

ç¬¬å››æ­¥ï¼šå¤„ç†å‰©ä½™
  â”œâ”€ åŸæ–‡æœ‰ã€è¯‘æ–‡æ—  â†’ ã€ŒåŸæ–‡ |||ã€
  â”œâ”€ è¯‘æ–‡æœ‰ã€åŸæ–‡æ—  â†’ ã€Œ||| è¯‘æ–‡ã€
  â””â”€ ç»ä¸å¡«å……ä»»ä½•è™šæ„å†…å®¹

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ã€æ‰§è¡Œæ£€æŸ¥æ¸…å•ã€‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
è¾“å‡ºå‰é€é¡¹ç¡®è®¤ï¼š
â–¡ åˆ†éš”ç¬¦æ˜¯å¦ç‹¬ç«‹æˆè¡Œï¼Ÿ
â–¡ æ˜¯å¦å­˜åœ¨ä»»ä½• XXX/---/.../[ç©º] ç­‰å ä½ç¬¦ï¼Ÿ
â–¡ è„šæ³¨ç¼–å·æ˜¯å¦ä¸å¯¹åº”ç¼–å·é…å¯¹ï¼ˆè€Œéä¸æ­£æ–‡é…å¯¹ï¼‰ï¼Ÿ
â–¡ æ¯ä¸ªåŸæ–‡ç‰‡æ®µæ˜¯å¦åªå‡ºç°ä¸€æ¬¡ï¼Ÿ
â–¡ æ¯ä¸ªè¯‘æ–‡ç‰‡æ®µæ˜¯å¦åªå‡ºç°ä¸€æ¬¡ï¼Ÿ
â–¡ æ˜¯å¦æœ‰è·¨é¡µé…å¯¹ï¼Ÿ

åªè¾“å‡ºå¯¹é½ç»“æœï¼Œä¸è¾“å‡ºä»»ä½•è§£é‡Šè¯´æ˜ã€‚
"""


def _get_docx_alignment_prompt(source_lang="ä¸­æ–‡", target_lang="è‹±è¯­"):
    source_info = SUPPORTED_LANGUAGES.get(source_lang, SUPPORTED_LANGUAGES["ä¸­æ–‡"])
    target_info = SUPPORTED_LANGUAGES.get(target_lang, SUPPORTED_LANGUAGES["è‹±è¯­"])

    source_is_cjk = source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]
    if source_is_cjk:
        punctuation_marks = "ã€‚ï¼ï¼Ÿ"
        punctuation_desc = "ã€‚ï¼ï¼Ÿï¼ˆå…¨è§’ä¸­æ—¥éŸ©å¥æœ«æ ‡ç‚¹ï¼‰"
    else:
        punctuation_marks = ". ! ?"
        punctuation_desc = ". ! ?ï¼ˆåŠè§’è¥¿æ–‡å¥æœ«æ ‡ç‚¹ï¼‰"

    sentence_rule = f"""2. ğŸ¯ æ–­å¥è§„åˆ™ï¼š{source_lang}ä¸»å¯¼ï¼Œ{target_lang}è·Ÿéšï¼ˆå¼ºä¼˜å…ˆçº§ï¼‰

   ç¬¬ä¸€æ­¥ï¼šåªçœ‹{source_lang}ï¼ŒæŒ‰ä»¥ä¸‹æ ‡ç‚¹æ–­å¥
   - æ–­å¥æ ‡ç‚¹ï¼š{punctuation_desc}
   - é‡åˆ°è¿™äº›æ ‡ç‚¹å°±æ–­å¼€ï¼Œå½¢æˆä¸€ä¸ª{source_lang}ç‰‡æ®µ

   ç¬¬äºŒæ­¥ï¼š{target_lang}åŒ¹é…{source_lang}
   - åœ¨ Stream B ä¸­æ‰¾å‡ºä¸è¯¥{source_lang}ç‰‡æ®µè¯­ä¹‰å¯¹åº”çš„{target_lang}éƒ¨åˆ†
   - {target_lang}å¯èƒ½æ˜¯1å¥ã€åŠå¥ã€æˆ–å¤šå¥ï¼Œéƒ½æ²¡å…³ç³»
   - è¯‘æ–‡ä½ç½®å¯èƒ½æœ‰é”™ä½ï¼ŒæŒ‰è¯­ä¹‰åŒ¹é…å³å¯
   - åªè¦è¯­ä¹‰å®Œå…¨è¦†ç›–è¯¥{source_lang}ç‰‡æ®µå³å¯

3. è´¨é‡æ£€æŸ¥
   - å·¦è¾¹çš„æ¯ä¸ª{source_lang}ç‰‡æ®µæ˜¯å¦éƒ½ä»¥æ­£ç¡®çš„å¥æœ«æ ‡ç‚¹ï¼ˆ{punctuation_marks}ï¼‰ç»“å°¾ï¼Ÿ
   - å³è¾¹{target_lang}çš„è¯­ä¹‰æ˜¯å¦å®Œå…¨å¯¹åº”å·¦è¾¹{source_lang}ï¼Ÿ
   - æ˜¯å¦æœ‰ä»»ä½•å†…å®¹è¢«ä¿®æ”¹æˆ–è‡ªè¡Œç¿»è¯‘ï¼Ÿï¼ˆå¿…é¡»ä¸ºå¦ï¼‰"""

    return f"""ä½ æ˜¯ä¸€ä¸ªã€ŒåŒæµæ–‡æœ¬å¯¹é½åŒæ­¥å™¨ã€(Dual-Stream Aligner)ã€‚

æ ¸å¿ƒä»»åŠ¡
å°† Stream A ({source_lang}åŸæ–‡) å’Œ Stream B ({target_lang}è¯‘æ–‡) è¿›è¡Œç²¾ç¡®å¯¹é½ã€‚

â›” ç»å¯¹é“å¾‹
1. æ¥æºé”å®šï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
   - "|||" å·¦ä¾§ = å¿…é¡»100%æ¥è‡ª Stream Aï¼Œä¸€ä¸ªå­—éƒ½ä¸èƒ½æ”¹
   - "|||" å³ä¾§ = å¿…é¡»100%æ¥è‡ª Stream Bï¼Œä¸€ä¸ªå­—éƒ½ä¸èƒ½æ”¹
   - ã€Œç¦æ­¢ã€è‡ªå·±ç¿»è¯‘æˆ–ç¼–é€ ä»»ä½•å†…å®¹
   - å³ä½¿å‘ç°åŸæ–‡æœ‰é”™åˆ«å­—ã€è¯‘æ–‡æœ‰ç¿»è¯‘é”™è¯¯ï¼Œä¹Ÿå¿…é¡»åŸæ ·ä¿ç•™

{sentence_rule}

4. ç©ºè¡Œå¤„ç†è§„åˆ™
   - å¦‚æœåŸæ–‡æˆ–è¯‘æ–‡ä¸­æœ‰ç©ºè¡Œï¼ˆè¿ç»­æ¢è¡Œç¬¦ï¼‰ï¼Œè¿™äº›ç©ºè¡Œå·²ç»è¢«é€‚å½“å‹ç¼©ä¿ç•™
   - ç©ºè¡Œé€šå¸¸ç”¨äºåˆ†éš”æ®µè½æˆ–ç« èŠ‚
   - åœ¨å¯¹é½æ—¶ï¼Œå¿½ç•¥ç©ºè¡Œçš„ä½ç½®å·®å¼‚ï¼Œä¸“æ³¨äºæœ‰å†…å®¹çš„æ–‡æœ¬å¯¹é½
   - ä¸è¦è¾“å‡ºç©ºè¡Œå¯¹åº”çš„å¯¹é½ç»“æœï¼ˆç©º|||ç©ºï¼‰

è¾“å‡ºæ ¼å¼
{source_lang}ç‰‡æ®µ ||| å¯¹åº”çš„{target_lang}å†…å®¹
"""


def _get_split_row_prompt(source_lang="ä¸­æ–‡"):
    source_is_cjk = source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]
    if source_is_cjk:
        punctuation_marks = "ã€‚ï¼ï¼Ÿ"
        punctuation_desc = "ã€‚ï¼ï¼Ÿï¼ˆå…¨è§’å¥æœ«æ ‡ç‚¹ï¼‰"
        abbreviation_rule = ""
        numbering_rule = "1. / 2. / 11. / 1ï¼‰/ 2ï¼‰/ â‘ â‘¡ ç­‰æ•°å­—ç¼–å·"
    else:
        punctuation_marks = ". ! ?"
        punctuation_desc = ". ! ?ï¼ˆåŠè§’å¥æœ«æ ‡ç‚¹ï¼‰"
        numbering_rule = """- æ•°å­—ç¼–å·ï¼š1. / 2. / 11. / (1) / (2) ç­‰
   - å­—æ¯ç¼–å·ï¼šA. / B. / C. / a. / b. / c. / (A) / (B) ç­‰
   - ç½—é©¬æ•°å­—ï¼šI. / II. / III. / i. / ii. / iii. ç­‰
   - æ··åˆç¼–å·ï¼š1.1 / 1.2 / A.1 / A.2 ç­‰ï¼ˆåœ¨æœ€åä¸€ä¸ªç‚¹åæ–­å¼€ï¼‰"""
        abbreviation_rule = """
âš ï¸ ç¼©å†™ä¾‹å¤–ï¼ˆè¿™äº›å¥å·ä¸è¡¨ç¤ºå¥æœ«ï¼Œä¸è¦åœ¨æ­¤æ–­å¼€ï¼‰ï¼š
- å­¦æœ¯å¼•ç”¨ï¼šet al. / etc. / e.g. / i.e. / vs. / cf. / ibid.
- ç§°è°“ï¼šMr. / Mrs. / Ms. / Dr. / Prof. / Jr. / Sr. / St.
- å…¬å¸ï¼šInc. / Ltd. / Co. / Corp. / L.L.C.
- å›½å®¶/ç»„ç»‡ï¼šU.S. / U.K. / U.N. / E.U.
- æ—¶é—´ï¼ša.m. / p.m. / A.M. / P.M.
- å¼•ç”¨æ ‡è®°ï¼šNo. / Vol. / Fig. / Ch. / Sec. / pp. / p.
- å…¶ä»–ï¼šapprox. / est. / max. / min. / avg.

ğŸ’¡ åŒºåˆ†ç¼–å·ä¸ç¼©å†™çš„æ–¹æ³•ï¼š
- ç¼–å·ç‰¹å¾ï¼šå•ç‹¬çš„æ•°å­—æˆ–å­—æ¯ + å¥å·ï¼Œå¦‚ "1." "A." "II."
- ç¼©å†™ç‰¹å¾ï¼šå¤šä¸ªå­—æ¯ç»„æˆçš„è¯ + å¥å·ï¼Œå¦‚ "et al." "Dr." "Inc."
- ç¼–å·åé¢é€šå¸¸è·Ÿç€æ­£æ–‡å†…å®¹ï¼Œç¼©å†™åé¢é€šå¸¸è¿˜æœ‰æ›´å¤šæ–‡å­—"""

    if not source_is_cjk:
        example_section = """
## ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šæ•°å­—ç¼–å· + ä½œè€…ä¿¡æ¯
è¾“å…¥ï¼š
ã€åŸæ–‡ã€‘11. Piacentini MG, et al. A randomized controlled trial on the effect of Solidago virgaurea extract.
ã€è¯‘æ–‡ã€‘11.Piacentini MGï¼Œç­‰ã€‚ä¸€é¡¹å…³äºä¸€æé»„èŠ±æå–ç‰©å½±å“çš„éšæœºå¯¹ç…§è¯•éªŒã€‚

æ­£ç¡®è¾“å‡ºï¼ˆ3è¡Œï¼‰ï¼š
11. ||| 11.
Piacentini MG, et al. ||| Piacentini MGï¼Œç­‰ã€‚
A randomized controlled trial on the effect of Solidago virgaurea extract. ||| ä¸€é¡¹å…³äºä¸€æé»„èŠ±æå–ç‰©å½±å“çš„éšæœºå¯¹ç…§è¯•éªŒã€‚

### ç¤ºä¾‹2ï¼šå­—æ¯ç¼–å·
è¾“å…¥ï¼š
ã€åŸæ–‡ã€‘A. Introduction. B. Methods. C. Results.
ã€è¯‘æ–‡ã€‘A. å¼•è¨€ã€‚B. æ–¹æ³•ã€‚C. ç»“æœã€‚

æ­£ç¡®è¾“å‡ºï¼ˆ6è¡Œï¼‰ï¼š
A. ||| A.
Introduction. ||| å¼•è¨€ã€‚
B. ||| B.
Methods. ||| æ–¹æ³•ã€‚
C. ||| C.
Results. ||| ç»“æœã€‚

### ç¤ºä¾‹3ï¼šæ··åˆç¼–å·
è¾“å…¥ï¼š
ã€åŸæ–‡ã€‘1.1 Background. 1.2 Objectives.
ã€è¯‘æ–‡ã€‘1.1 èƒŒæ™¯ã€‚1.2 ç›®æ ‡ã€‚

æ­£ç¡®è¾“å‡ºï¼ˆ4è¡Œï¼‰ï¼š
1.1 ||| 1.1
Background. ||| èƒŒæ™¯ã€‚
1.2 ||| 1.2
Objectives. ||| ç›®æ ‡ã€‚

### å…³é”®åˆ¤æ–­è§„åˆ™ï¼š
- "A." "B." "1." "11." "1.1" â†’ ç¼–å·ï¼Œåé¢æ–­å¼€
- "et al." "Dr." "U.S." "Inc." â†’ ç¼©å†™ï¼Œä¸åœ¨æ­¤æ–­å¼€
- ç¼–å· = å•ç‹¬æ•°å­—/å­—æ¯ + ç‚¹å·
- ç¼©å†™ = å¤šå­—æ¯è¯æ±‡ + ç‚¹å·
"""
    else:
        example_section = ""
        numbering_rule = "1. / 2. / 11. / 1ï¼‰/ 2ï¼‰/ â‘ â‘¡ ç­‰æ•°å­—ç¼–å·"

    return f"""ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„ã€Œå•è¡Œåˆ†å¥å¯¹é½å™¨ã€ã€‚

## ä»»åŠ¡
æ ¹æ®ã€åŸæ–‡ã€‘çš„åˆ†å‰²ç‚¹ï¼Œå°†ã€è¯‘æ–‡ã€‘å¯¹åº”æ‹†åˆ†å¯¹é½ã€‚

## â›” é“å¾‹

### 1. åˆ†å‰²ç‚¹è¯†åˆ«ï¼ˆå¿…é¡»åœ¨ä»¥ä¸‹ä½ç½®æ–­å¼€ï¼‰

#### 1.1 ç¼–å·ï¼ˆç¼–å·åå¿…é¡»æ–­å¼€ï¼Œç¼–å·å•ç‹¬æˆè¡Œï¼‰
   {numbering_rule}

#### 1.2 å¥æœ«æ ‡ç‚¹ï¼ˆ{punctuation_desc}ï¼‰
   æ¯ä¸ªå¥å·/æ„Ÿå¹å·/é—®å·åéƒ½è¦æ–­å¼€
{abbreviation_rule}

### 2. ç¦æ­¢ä¿®æ”¹å†…å®¹
- å·¦ä¾§å¿…é¡»100%æ¥è‡ªåŸæ–‡ï¼Œä¸€å­—ä¸æ”¹
- å³ä¾§å¿…é¡»100%æ¥è‡ªè¯‘æ–‡ï¼Œä¸€å­—ä¸æ”¹
- ç¦æ­¢ç¿»è¯‘ã€ç¼–é€ ã€è¡¥å……ä»»ä½•å†…å®¹

### 3. è¯‘æ–‡å¯¹é½è§„åˆ™
- æ ¹æ®è¯­ä¹‰å°†è¯‘æ–‡æ‹†åˆ†ï¼ŒåŒ¹é…åŸæ–‡çš„æ¯ä¸ªåˆ†å‰²å•å…ƒ
- å¦‚æœè¯‘æ–‡æ ‡ç‚¹ä¸åŸæ–‡ä¸ä¸€è‡´ï¼ŒæŒ‰è¯­ä¹‰å¯¹é½

### 4. åˆ†å‰²åŸåˆ™
- ç¼–å·å•ç‹¬æˆè¡Œï¼ˆå¦‚ "11." / "A." / "1.1"ï¼‰
- ä½œè€…ä¿¡æ¯å•ç‹¬æˆè¡Œï¼ˆå¦‚ "Piacentini MG, et al."ï¼‰
- æ¯ä¸ªå®Œæ•´å¥å­å•ç‹¬æˆè¡Œ
- å³ä½¿ç‰‡æ®µå¾ˆçŸ­ä¹Ÿè¦ç‹¬ç«‹æˆè¡Œ
- è¾“å‡ºè¡Œæ•° = åŸæ–‡åˆ†å‰²å•å…ƒæ•°
{example_section}
## è¾“å‡ºæ ¼å¼
æ¯è¡Œä¸€å¯¹ï¼Œç”¨ ||| åˆ†éš”ï¼š
åŸæ–‡ç‰‡æ®µ1 ||| è¯‘æ–‡å¯¹åº”éƒ¨åˆ†1
åŸæ–‡ç‰‡æ®µ2 ||| è¯‘æ–‡å¯¹åº”éƒ¨åˆ†2

åªè¾“å‡ºå¯¹é½ç»“æœï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚"""


def _get_table_cell_split_prompt(source_lang="ä¸­æ–‡"):
    source_is_cjk = source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]

    if source_is_cjk:
        punctuation_marks = "ã€‚ï¼ï¼Ÿ"
        punctuation_desc = "ã€‚ï¼ï¼Ÿï¼ˆå…¨è§’å¥æœ«æ ‡ç‚¹ï¼‰"
    else:
        punctuation_marks = ". ! ?"
        punctuation_desc = ". ! ?ï¼ˆåŠè§’å¥æœ«æ ‡ç‚¹ï¼‰"

    return f"""ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„ã€Œè¡¨æ ¼å•å…ƒæ ¼åˆ†å¥å¯¹é½å™¨ã€ã€‚

## ä»»åŠ¡
å¯¹è¡¨æ ¼å•å…ƒæ ¼å†…çš„åŸæ–‡å’Œè¯‘æ–‡è¿›è¡Œç»†ç²’åº¦åˆ†å¥å¯¹é½ã€‚

## åˆ†å¥è§„åˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

### 1. æ–­å¥ä½ç½®ï¼ˆåªåœ¨ä»¥ä¸‹ä½ç½®æ–­å¼€ï¼‰
- **å¥æœ«æ ‡ç‚¹**ï¼š{punctuation_desc}
- **æ¢è¡Œç¬¦**ï¼šå½“åŸæ–‡ä¸­æœ‰ç©ºè¡Œåˆ†éš”æ—¶ï¼Œåœ¨ç©ºè¡Œå¤„æ–­å¼€

### 2. ã€é‡è¦ã€‘ä¸æ˜¯æ–­å¥ç‚¹çš„æƒ…å†µ
- **åºå·æ ‡ç‚¹ä¸æ–­å¥**ï¼šå¦‚"1."ã€"2."ã€"3."ã€"â‘ "ã€"â‘¡"ã€"(1)"ã€"a."ç­‰åºå·åçš„ç‚¹å·ï¼Œè¿™æ˜¯åºå·çš„ä¸€éƒ¨åˆ†ï¼Œä¸æ˜¯å¥æœ«
- **åºå·å¿…é¡»ä¿ç•™åœ¨å¥å­å¼€å¤´**ï¼šåºå·å’Œåé¢çš„å†…å®¹å±äºåŒä¸€å¥
- åªæœ‰çœŸæ­£è¡¨ç¤ºå¥å­ç»“æŸçš„æ ‡ç‚¹ï¼ˆ{punctuation_marks}ï¼‰æ‰æ–­å¥

### 3. åˆ†å¥åŸåˆ™
- ä»¥åŸæ–‡çš„åˆ†å¥ç»“æ„ä¸ºå‡†
- æ¯ä¸ªå®Œæ•´å¥å­ï¼ˆä»¥{punctuation_marks}ç»“å°¾ï¼‰ç‹¬ç«‹æˆè¡Œ
- å½“é‡åˆ°ç©ºè¡Œåˆ†éš”çš„æ®µè½æ—¶ï¼ŒæŒ‰æ®µè½æ–­å¼€
- ä¿æŒåŸæ–‡å’Œè¯‘æ–‡çš„å¥å­ä¸€ä¸€å¯¹åº”

### 4. ç¦æ­¢ä¿®æ”¹å†…å®¹
- å·¦ä¾§å¿…é¡»100%æ¥è‡ªåŸæ–‡ï¼Œä¸€å­—ä¸æ”¹
- å³ä¾§å¿…é¡»100%æ¥è‡ªè¯‘æ–‡ï¼Œä¸€å­—ä¸æ”¹
- ç¦æ­¢ç¿»è¯‘ã€ç¼–é€ ã€è¡¥å……ä»»ä½•å†…å®¹
- ä¿ç•™æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å’Œåºå·

### 5. è¯‘æ–‡å¯¹é½è§„åˆ™
- æ ¹æ®è¯­ä¹‰å°†è¯‘æ–‡æ‹†åˆ†ï¼ŒåŒ¹é…åŸæ–‡çš„æ¯ä¸ªå¥å­
- å¦‚æœè¯‘æ–‡æ ‡ç‚¹ä¸åŸæ–‡ä¸å®Œå…¨ä¸€è‡´ï¼ŒæŒ‰è¯­ä¹‰å¯¹é½

## ç¤ºä¾‹1ï¼ˆå¸¦åºå·çš„å¤šæ¡å†…å®¹ï¼‰

### è¾“å…¥ï¼š
ã€åŸæ–‡ã€‘
1.è¿™æ˜¯ç¬¬ä¸€æ¡è¯´æ˜ã€‚éœ€è¦æ³¨æ„è¿™ä¸ªé—®é¢˜ã€‚

2.è¿™æ˜¯ç¬¬äºŒæ¡è¯´æ˜ï¼›

ã€è¯‘æ–‡ã€‘
1. This is the first instruction. Please note this issue.
2. This is the second instruction;

### æ­£ç¡®è¾“å‡ºï¼š
1.è¿™æ˜¯ç¬¬ä¸€æ¡è¯´æ˜ã€‚ ||| 1. This is the first instruction.
éœ€è¦æ³¨æ„è¿™ä¸ªé—®é¢˜ã€‚ ||| Please note this issue.
2.è¿™æ˜¯ç¬¬äºŒæ¡è¯´æ˜ï¼› ||| 2. This is the second instruction;

### é”™è¯¯è¾“å‡ºï¼ˆç»å¯¹ç¦æ­¢ï¼‰ï¼š
1. ||| 1.
è¿™æ˜¯ç¬¬ä¸€æ¡è¯´æ˜ã€‚ ||| This is the first instruction.
ï¼ˆé”™è¯¯åŸå› ï¼šæŠŠåºå·"1."å•ç‹¬æ‹†åˆ†äº†ï¼Œåºå·å¿…é¡»å’Œåé¢çš„å†…å®¹åœ¨ä¸€èµ·ï¼‰

## ç¤ºä¾‹2ï¼ˆæ™®é€šå¤šå¥å†…å®¹ï¼‰

### è¾“å…¥ï¼š
ã€åŸæ–‡ã€‘
è¿™æ˜¯ç¬¬ä¸€å¥ã€‚è¿™æ˜¯ç¬¬äºŒå¥ï¼
è¿™æ˜¯æ¢è¡Œåçš„ç¬¬ä¸‰å¥ï¼Ÿ

ã€è¯‘æ–‡ã€‘
This is the first sentence. This is the second sentence!
This is the third sentence after newline?

### æ­£ç¡®è¾“å‡ºï¼š
è¿™æ˜¯ç¬¬ä¸€å¥ã€‚ ||| This is the first sentence.
è¿™æ˜¯ç¬¬äºŒå¥ï¼ ||| This is the second sentence!
è¿™æ˜¯æ¢è¡Œåçš„ç¬¬ä¸‰å¥ï¼Ÿ ||| This is the third sentence after newline?

## è¾“å‡ºæ ¼å¼
æ¯è¡Œä¸€å¯¹ï¼Œç”¨ ||| åˆ†éš”ï¼š
åŸæ–‡å¥å­1 ||| è¯‘æ–‡å¯¹åº”éƒ¨åˆ†1
åŸæ–‡å¥å­2 ||| è¯‘æ–‡å¯¹åº”éƒ¨åˆ†2

åªè¾“å‡ºå¯¹é½ç»“æœï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚"""


# â”€â”€ æ–‡ä»¶å·¥å…· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_file_type(file_path: str) -> str:
    ext = os.path.splitext(file_path)[1].lower()
    return {'.docx': 'docx', '.pptx': 'pptx', '.xlsx': 'excel', '.xls': 'excel'}.get(ext, 'unknown')


def _get_text_count(text: str, lang_name: str) -> int:
    if not text:
        return 0
    lang_info = SUPPORTED_LANGUAGES.get(lang_name)
    if lang_info:
        return len(re.findall(lang_info['char_pattern'], text))
    return len(re.findall(r'\b[a-zA-Z0-9-]+\b', text))


def _iter_body_elements(body, doc):
    """é€’å½’éå† body ä¸‹æ‰€æœ‰æ®µè½å’Œè¡¨æ ¼ï¼ŒåŒ…æ‹¬ sdtï¼ˆå†…å®¹æ§ä»¶ï¼‰å†…éƒ¨çš„å…ƒç´ """
    for child in body.iterchildren():
        tag = child.tag.split('}')[-1] if '}' in child.tag else child.tag
        if tag == 'p':
            yield ('p', Paragraph(child, doc))
        elif tag == 'tbl':
            yield ('tbl', Table(child, doc))
        elif tag == 'sdt':
            for sub in child.iterchildren():
                sub_tag = sub.tag.split('}')[-1] if '}' in sub.tag else sub.tag
                if sub_tag == 'sdtContent':
                    yield from _iter_body_elements(sub, doc)
                elif sub_tag == 'p':
                    yield ('p', Paragraph(sub, doc))
                elif sub_tag == 'tbl':
                    yield ('tbl', Table(sub, doc))


def _get_all_content_elements(doc):
    elements = []
    if hasattr(doc, 'element') and hasattr(doc.element, 'body'):
        for elem_type, elem in _iter_body_elements(doc.element.body, doc):
            elements.append(elem)
    if not elements:
        for para in doc.paragraphs:
            elements.append(para)
        for table in doc.tables:
            elements.append(table)
    return elements


def _get_element_text_count(element, lang_name: str) -> int:
    text = ""
    if isinstance(element, Paragraph):
        text = element.text
    elif isinstance(element, Table):
        for row in element.rows:
            for cell in row.cells:
                text += cell.text + " "
    return _get_text_count(text, lang_name)


def _read_full_docx(file_path: str) -> str:
    """è¯»å–å®Œæ•´çš„docxæ–‡ä»¶å†…å®¹ï¼ŒåŒ…æ‹¬æ®µè½ã€è¡¨æ ¼ã€é¡µçœ‰é¡µè„šã€æ–‡æœ¬æ¡†ã€è„šæ³¨å°¾æ³¨
    ï¼ˆä¸ memory.py åŸç‰ˆ read_full_docx ä¿æŒä¸€è‡´ï¼‰
    """
    try:
        abs_path = os.path.abspath(file_path)
        print(f"[alignment-read] è¯»å– DOCX: {abs_path} (å¤§å°: {os.path.getsize(abs_path)} bytes)")
        doc = Document(abs_path)
        full_text = []
        consecutive_empty = 0

        # 1. æŒ‰æ–‡æ¡£é¡ºåºéå†æ‰€æœ‰å…ƒç´ ï¼ˆæ®µè½å’Œè¡¨æ ¼ï¼‰ï¼Œå« sdt å†…å®¹æ§ä»¶å†…å…ƒç´ 
        if hasattr(doc, 'element') and hasattr(doc.element, 'body'):
            for elem_type, elem in _iter_body_elements(doc.element.body, doc):
                if elem_type == 'p':
                    if elem.text.strip():
                        full_text.append(elem.text)
                        consecutive_empty = 0
                    else:
                        consecutive_empty += 1
                        if consecutive_empty <= 2 and full_text:
                            full_text.append("")
                elif elem_type == 'tbl':
                    consecutive_empty = 0
                    seen_cells = set()
                    for row in elem.rows:
                        for cell in row.cells:
                            cell_text = cell.text.strip()
                            if cell_text and cell_text not in seen_cells:
                                full_text.append(cell_text)
                                seen_cells.add(cell_text)
        else:
            consecutive_empty = 0
            for para in doc.paragraphs:
                if para.text.strip():
                    full_text.append(para.text)
                    consecutive_empty = 0
                else:
                    consecutive_empty += 1
                    if consecutive_empty <= 2 and full_text:
                        full_text.append("")
            for table in doc.tables:
                seen_cells = set()
                for row in table.rows:
                    for cell in row.cells:
                        cell_text = cell.text.strip()
                        if cell_text and cell_text not in seen_cells:
                            full_text.append(cell_text)
                            seen_cells.add(cell_text)

        # 2. é¡µçœ‰é¡µè„šï¼ˆæ®µè½ã€è¡¨æ ¼ã€æ–‡æœ¬æ¡†ï¼‰
        for section in doc.sections:
            for p in section.header.paragraphs:
                if p.text.strip():
                    full_text.append(p.text)
            for table in section.header.tables:
                seen_header_cells = set()
                for row in table.rows:
                    for cell in row.cells:
                        cell_text = cell.text.strip()
                        if cell_text and cell_text not in seen_header_cells:
                            full_text.append(cell_text)
                            seen_header_cells.add(cell_text)
            if section.header._element is not None:
                header_xml = etree.tostring(section.header._element, encoding='unicode')
                header_root = etree.fromstring(header_xml.encode('utf-8'))
                nsmap_header = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
                for txbx in header_root.xpath('.//w:txbxContent', namespaces=nsmap_header):
                    txbx_text = ''.join(txbx.xpath('.//w:t/text()', namespaces=nsmap_header))
                    if txbx_text.strip():
                        full_text.append(txbx_text.strip())

            for p in section.footer.paragraphs:
                if p.text.strip():
                    full_text.append(p.text)
            for table in section.footer.tables:
                seen_footer_cells = set()
                for row in table.rows:
                    for cell in row.cells:
                        cell_text = cell.text.strip()
                        if cell_text and cell_text not in seen_footer_cells:
                            full_text.append(cell_text)
                            seen_footer_cells.add(cell_text)
            if section.footer._element is not None:
                footer_xml = etree.tostring(section.footer._element, encoding='unicode')
                footer_root = etree.fromstring(footer_xml.encode('utf-8'))
                nsmap_footer = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
                for txbx in footer_root.xpath('.//w:txbxContent', namespaces=nsmap_footer):
                    txbx_text = ''.join(txbx.xpath('.//w:t/text()', namespaces=nsmap_footer))
                    if txbx_text.strip():
                        full_text.append(txbx_text.strip())

        # 3. æ–‡æœ¬æ¡†ï¼ˆä¼ ç»Ÿæ ¼å¼ + DrawingML æ ¼å¼ï¼‰
        if hasattr(doc.element, 'xml'):
            xml = doc.element.xml
            root = etree.fromstring(xml.encode('utf-8'))

            nsmap = {
                'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main',
                'wp': 'http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing',
                'a': 'http://schemas.openxmlformats.org/drawingml/2006/main',
                'wps': 'http://schemas.microsoft.com/office/word/2010/wordprocessingShape',
                'wpg': 'http://schemas.microsoft.com/office/word/2010/wordprocessingGroup',
                'mc': 'http://schemas.openxmlformats.org/markup-compatibility/2006',
                'w14': 'http://schemas.microsoft.com/office/word/2010/wordml',
            }

            textbox_texts = set()

            # æ–¹å¼1: ä¼ ç»Ÿæ–‡æœ¬æ¡† (w:txbxContent)
            try:
                for container in root.xpath('.//w:txbxContent', namespaces=nsmap):
                    para_texts = []
                    for p in container.xpath('.//w:p', namespaces=nsmap):
                        p_text = ''.join([t.text for t in p.xpath('.//w:t', namespaces=nsmap) if t.text])
                        if p_text.strip():
                            para_texts.append(p_text.strip())
                    merged = '\n'.join(para_texts)
                    if merged.strip():
                        textbox_texts.add(merged.strip())
            except Exception:
                pass

            # æ–¹å¼2: DrawingML æ–‡æœ¬æ¡† (wps:txbx)
            try:
                for container in root.xpath('.//wps:txbx', namespaces=nsmap):
                    para_texts = []
                    for p in container.xpath('.//a:p', namespaces=nsmap):
                        p_text = ''.join([t.text for t in p.xpath('.//a:t', namespaces=nsmap) if t.text])
                        if p_text.strip():
                            para_texts.append(p_text.strip())
                    merged = '\n'.join(para_texts)
                    if merged.strip():
                        textbox_texts.add(merged.strip())
            except Exception:
                pass

            # æ–¹å¼3: DrawingML å½¢çŠ¶æ–‡æœ¬ (a:txBody)
            try:
                for container in root.xpath('.//a:txBody', namespaces=nsmap):
                    para_texts = []
                    for p in container.xpath('.//a:p', namespaces=nsmap):
                        p_text = ''.join([t.text for t in p.xpath('.//a:t', namespaces=nsmap) if t.text])
                        if p_text.strip():
                            para_texts.append(p_text.strip())
                    merged = '\n'.join(para_texts)
                    if merged.strip():
                        textbox_texts.add(merged.strip())
            except Exception:
                pass

            # æ–¹å¼4: Word 2010+ æ–‡æœ¬æ¡† (wps:wsp//wps:txbx)
            try:
                for container in root.xpath('.//wps:wsp//wps:txbx', namespaces=nsmap):
                    para_texts = []
                    for p in container.xpath('.//w:p', namespaces=nsmap):
                        p_text = ''.join([t.text for t in p.xpath('.//w:t', namespaces=nsmap) if t.text])
                        if p_text.strip():
                            para_texts.append(p_text.strip())
                    merged = '\n'.join(para_texts)
                    if merged.strip():
                        textbox_texts.add(merged.strip())
            except Exception:
                pass

            for text in textbox_texts:
                full_text.append(text)

        # 4. è„šæ³¨ã€å°¾æ³¨
        if hasattr(doc, 'part'):
            nsmap_fn = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
            for rel in doc.part.rels.values():
                ref = rel.target_ref

                if "footnotes" in ref:
                    try:
                        fn_root = etree.fromstring(rel.target_part.blob)
                        for fn in fn_root.xpath('.//w:footnote', namespaces=nsmap_fn):
                            fn_type = fn.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}type')
                            if fn_type in ['separator', 'continuationSeparator']:
                                continue
                            fn_texts = []
                            for p in fn.xpath('.//w:p', namespaces=nsmap_fn):
                                p_text = ''.join([t.text for t in p.xpath('.//w:t', namespaces=nsmap_fn) if t.text])
                                if p_text.strip():
                                    fn_texts.append(p_text.strip())
                            fn_text = ' '.join(fn_texts)
                            if fn_text.strip():
                                full_text.append(fn_text.strip())
                    except Exception:
                        pass

                elif "endnotes" in ref:
                    try:
                        en_root = etree.fromstring(rel.target_part.blob)
                        for en in en_root.xpath('.//w:endnote', namespaces=nsmap_fn):
                            en_type = en.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}type')
                            if en_type in ['separator', 'continuationSeparator']:
                                continue
                            en_texts = []
                            for p in en.xpath('.//w:p', namespaces=nsmap_fn):
                                p_text = ''.join([t.text for t in p.xpath('.//w:t', namespaces=nsmap_fn) if t.text])
                                if p_text.strip():
                                    en_texts.append(p_text.strip())
                            en_text = ' '.join(en_texts)
                            if en_text.strip():
                                full_text.append(en_text.strip())
                    except Exception:
                        pass

        result = "\n".join(full_text)
        result = re.sub(r'\n{4,}', '\n\n\n', result)
        print(f"[alignment-read]   è¯»å–å®Œæˆ: {len(full_text)} æ®µ, æ€»é•¿ {len(result)} å­—ç¬¦")
        if not result.strip():
            print(f"[alignment-read]   è­¦å‘Š: æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼")
        return result
    except Exception as e:
        print(f"[alignment-read] è¯»å– DOCX å¤±è´¥: {e}")
        traceback.print_exc()
        return ""


def _iter_group_shapes(group_shape, base_top=0, base_left=0):
    for sub in group_shape.shapes:
        top = base_top + (sub.top or 0)
        left = base_left + (sub.left or 0)
        if sub.shape_type == MSO_SHAPE_TYPE.GROUP:
            yield from _iter_group_shapes(sub, top, left)
            continue
        if hasattr(sub, "has_text_frame") and sub.has_text_frame:
            para_texts = [p.text.strip() for p in sub.text_frame.paragraphs if p.text.strip()]
            if para_texts:
                yield (top, left, '\n'.join(para_texts))
        if hasattr(sub, "has_table") and sub.has_table:
            for r_idx, row in enumerate(sub.table.rows):
                for c_idx, cell in enumerate(row.cells):
                    para_texts = [p.text.strip() for p in cell.text_frame.paragraphs if p.text.strip()]
                    if para_texts:
                        yield (top + r_idx * 1000, left + c_idx * 1000, '\n'.join(para_texts))


def _extract_slide_items(slide):
    items = []
    for shape in slide.shapes:
        top = shape.top or 0
        left = shape.left or 0
        if hasattr(shape, "has_text_frame") and shape.has_text_frame:
            para_texts = [p.text.strip() for p in shape.text_frame.paragraphs if p.text.strip()]
            if para_texts:
                items.append((top, left, '\n'.join(para_texts)))
        if hasattr(shape, "has_table") and shape.has_table:
            for r_idx, row in enumerate(shape.table.rows):
                for c_idx, cell in enumerate(row.cells):
                    para_texts = [p.text.strip() for p in cell.text_frame.paragraphs if p.text.strip()]
                    if para_texts:
                        items.append((top + r_idx * 1000, left + c_idx * 1000, '\n'.join(para_texts)))
        if shape.shape_type == MSO_SHAPE_TYPE.GROUP:
            items.extend(_iter_group_shapes(shape, top, left))
    items.sort(key=lambda t: (t[0] // ROW_BUCKET, t[1]))
    return [t[2] for t in items]


def _read_full_pptx(file_path: str) -> str:
    try:
        prs = Presentation(file_path)
        lines = []
        for idx, slide in enumerate(prs.slides, 1):
            lines.append(f"---- å¹»ç¯ç‰‡ {idx} ----")
            lines.extend(_extract_slide_items(slide))
        return "\n".join(lines)
    except Exception as e:
        print(f"è¯»å– PPTX å¤±è´¥: {e}")
        return ""


def _read_full_excel(file_path: str) -> str:
    try:
        df = pd.read_excel(file_path)
        texts = []
        for col in df.columns:
            texts.append(f"[åˆ—: {col}]")
            for val in df[col].dropna():
                if str(val).strip():
                    texts.append(str(val).strip())
        return "\n".join(texts)
    except Exception as e:
        print(f"è¯»å– Excel å¤±è´¥: {e}")
        return ""


def _read_file_content(file_path: str) -> str:
    abs_path = os.path.abspath(file_path)
    ft = _get_file_type(abs_path)
    if ft == 'docx':
        return _read_full_docx(abs_path)
    elif ft == 'pptx':
        return _read_full_pptx(abs_path)
    elif ft == 'excel':
        return _read_full_excel(abs_path)
    print(f"[alignment-read] ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ft} ({abs_path})")
    return ""


# â”€â”€ Excel å¤šå·¥ä½œç°¿è¯»å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _read_excel_all_sheets(file_path: str):
    try:
        excel_file = pd.ExcelFile(file_path, engine='openpyxl')
        sheets = {}
        for name in excel_file.sheet_names:
            sheets[name] = pd.read_excel(file_path, sheet_name=name, header=None, engine='openpyxl')
        return sheets
    except Exception:
        pass
    try:
        from openpyxl import load_workbook
        wb = load_workbook(file_path, data_only=True, read_only=True)
        sheets = {}
        for name in wb.sheetnames:
            ws = wb[name]
            data = [[cell.value for cell in row] for row in ws.iter_rows()]
            sheets[name] = pd.DataFrame(data) if data else pd.DataFrame()
        wb.close()
        return sheets
    except Exception as e:
        print(f"è¯»å– Excel å¤±è´¥: {e}")
        return None


# â”€â”€ æ–‡æ¡£åˆ†æä¸åˆ†å‰² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _analyze_document(doc_path: str, lang_name: str):
    abs_path = os.path.abspath(doc_path)
    ft = _get_file_type(abs_path)
    if ft == 'pptx':
        text = _read_full_pptx(abs_path)
        return _get_text_count(text, lang_name), 0
    elif ft == 'excel':
        text = _read_full_excel(abs_path)
        return _get_text_count(text, lang_name), 0
    else:
        doc = Document(abs_path)
        elements = _get_all_content_elements(doc)
        total = sum(_get_element_text_count(el, lang_name) for el in elements)
        return total, len(elements)


def _find_element_index_by_chars(element_counts, target_chars):
    left, right = 0, len(element_counts) - 1
    best = 0
    while left <= right:
        mid = (left + right) // 2
        if element_counts[mid] < target_chars:
            best = mid
            left = mid + 1
        else:
            right = mid - 1
    if best + 1 < len(element_counts):
        if element_counts[best + 1] - target_chars < target_chars - element_counts[best]:
            return best + 1
    return best


def _find_buffer_end(element_counts, split_idx, buffer_chars, direction='right'):
    n = len(element_counts)
    if n == 0:
        return split_idx
    if direction == 'right':
        base = element_counts[split_idx] if split_idx < n else element_counts[-1]
        target = base + buffer_chars
        for i in range(split_idx + 1, n):
            if element_counts[i] >= target:
                return i + 1
        return n
    else:
        base = element_counts[split_idx] if split_idx < n else element_counts[-1]
        target = base - buffer_chars
        if target <= 0:
            return 0
        for i in range(split_idx - 1, -1, -1):
            if element_counts[i] <= target:
                return i
        return 0


def _extract_text_from_elements(elements, start_idx, end_idx):
    texts = []
    for i in range(start_idx, min(end_idx, len(elements))):
        elem = elements[i]
        if isinstance(elem, Paragraph) and elem.text.strip():
            texts.append(elem.text.strip())
        elif isinstance(elem, Table):
            for row in elem.rows:
                row_texts = [c.text.strip() for c in row.cells if c.text.strip()]
                if row_texts:
                    texts.append(" | ".join(row_texts))
    return "\n".join(texts)


def _delete_elements_in_range(doc, start_idx, end_idx):
    all_elements = _get_all_content_elements(doc)
    to_delete = []
    for i in range(len(all_elements)):
        if start_idx <= i < end_idx:
            if isinstance(all_elements[i], (Paragraph, Table)):
                to_delete.append(all_elements[i]._element)
    for el in to_delete:
        parent = el.getparent()
        if parent is not None:
            parent.remove(el)


def _smart_split_with_buffer(src_path, num_parts, output_dir, lang_type, buffer_chars=2000,
                             split_element_ratios=None):
    src_path = os.path.abspath(src_path)
    doc = Document(src_path)
    elements = _get_all_content_elements(doc)
    base_name = os.path.splitext(os.path.basename(src_path))[0]

    element_counts = []
    cumulative = 0
    for elem in elements:
        cumulative += _get_element_text_count(elem, lang_type)
        element_counts.append(cumulative)

    total_count = cumulative
    if total_count == 0:
        return [], [], []

    target_per_part = total_count // num_parts

    if split_element_ratios is not None:
        ideal_splits = [max(0, min(int(r * len(elements)), len(elements) - 1)) for r in split_element_ratios]
    else:
        ideal_splits = []
        for i in range(1, num_parts):
            idx = _find_element_index_by_chars(element_counts, target_per_part * i)
            ideal_splits.append(idx)

    for i in range(1, len(ideal_splits)):
        if ideal_splits[i] <= ideal_splits[i - 1]:
            ideal_splits[i] = ideal_splits[i - 1] + 1
    for i in range(len(ideal_splits)):
        ideal_splits[i] = min(ideal_splits[i], len(elements) - 1)

    element_ratios = [idx / len(elements) for idx in ideal_splits] if elements else []

    split_ranges = []
    for part_idx in range(num_parts):
        if part_idx == 0:
            start = 0
            end = _find_buffer_end(element_counts, ideal_splits[0], buffer_chars, 'right') if ideal_splits else len(elements)
        elif part_idx == num_parts - 1:
            start = _find_buffer_end(element_counts, ideal_splits[-1], buffer_chars, 'left')
            end = len(elements)
        else:
            start = _find_buffer_end(element_counts, ideal_splits[part_idx - 1], buffer_chars, 'left')
            end = _find_buffer_end(element_counts, ideal_splits[part_idx], buffer_chars, 'right')
        start = max(0, min(start, len(elements) - 1))
        end = max(start + 1, min(end, len(elements)))
        split_ranges.append((start, end))

    generated_files = []
    part_info = []
    for i, (s, e) in enumerate(split_ranges):
        dest = os.path.join(output_dir, f"{base_name}_Part{i + 1}.docx")
        shutil.copy2(src_path, dest)
        doc_copy = Document(dest)
        total_elems = len(_get_all_content_elements(doc_copy))
        _delete_elements_in_range(doc_copy, e, total_elems + 5000)
        _delete_elements_in_range(doc_copy, 0, s)
        doc_copy.save(dest)
        first_text = _extract_text_from_elements(elements, s, min(s + 3, e))
        last_text = _extract_text_from_elements(elements, max(s, e - 3), e)
        part_info.append({
            'path': dest,
            'first_anchor': first_text[:200] if first_text else "",
            'last_anchor': last_text[-200:] if last_text else "",
        })
        generated_files.append(dest)

    return generated_files, part_info, element_ratios


# â”€â”€ LLM è°ƒç”¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _call_llm(system_prompt: str, user_prompt: str, model_id: str, max_output: int = 65536) -> Optional[str]:
    client = OpenAI(base_url=BASE_URL, api_key=API_KEY)
    try:
        print(f"[alignment-llm] è¯·æ±‚ model={model_id}, sys_len={len(system_prompt)}, user_len={len(user_prompt)}")
        stream = client.chat.completions.create(
            model=model_id,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.1,
            max_tokens=max_output,
            stream=True,
            timeout=600.0,
            extra_headers={"HTTP-Referer": "local-debug", "X-Title": "Doc-Aligner"},
        )
        full = ""
        for chunk in stream:
            if not chunk.choices:
                continue
            delta = chunk.choices[0].delta
            if hasattr(delta, "content") and delta.content:
                full += delta.content
        print(f"[alignment-llm] å®Œæˆ, è¿”å› {len(full)} å­—ç¬¦")
        return full
    except Exception as e:
        print(f"[alignment-llm] è°ƒç”¨å¤±è´¥: {e}")
        import traceback as _tb
        _tb.print_exc()
        return None


def _get_model_max_output(model_id: str) -> int:
    for info in AVAILABLE_MODELS.values():
        if info.get("id") == model_id:
            return int(info.get("max_output", 65536))
    return 65536


# â”€â”€ è§£æå¯¹é½å“åº” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _parse_alignment_response(response_text: str) -> list:
    if not response_text:
        return []
    cleaned = response_text.replace('\r\n', '\n').replace('\r', '\n')
    cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
    lines = cleaned.splitlines()
    data = []
    pending = ""
    for line in lines:
        line = line.strip()
        if not line or line.startswith('```'):
            continue
        if pending:
            line = pending + " " + line
            pending = ""
        if "|||" in line:
            parts = line.split("|||", 1)
            original = parts[0].strip()
            trans = parts[1].strip() if len(parts) > 1 else ""
            if original or trans:
                data.append({"åŸæ–‡": original, "è¯‘æ–‡": trans})
        else:
            if len(line) < 50:
                pending = line
    return data


# â”€â”€ è‹±æ–‡åå¤„ç†åˆ†å¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ENGLISH_ABBREVIATIONS = {
    'et al', 'etc', 'e.g', 'i.e', 'vs', 'cf', 'ibid', 'op', 'cit',
    'mr', 'mrs', 'ms', 'dr', 'prof', 'jr', 'sr', 'st',
    'inc', 'ltd', 'co', 'corp', 'llc', 'l.l.c',
    'u.s', 'u.k', 'u.n', 'e.u', 'u.s.a',
    'a.m', 'p.m',
    'no', 'vol', 'fig', 'ch', 'sec', 'pp', 'approx', 'est', 'max', 'min', 'avg',
}


def _is_abbreviation_period(text, pos):
    if pos <= 0:
        return False
    before = text[:pos]
    after = text[pos + 1:] if pos + 1 < len(text) else ""

    if after and after[0].isdigit():
        return True
    if after and after[0].isalpha():
        return True

    words = before.split()
    if not words:
        return False

    last_token = words[-1]
    last_word_lower = last_token.lower().rstrip('.,;:')

    if last_word_lower in ENGLISH_ABBREVIATIONS:
        return True
    if '.' in last_token:
        return True

    if len(last_word_lower) == 1 and last_word_lower.isalpha():
        if len(words) >= 2:
            prev_token = words[-2].lower()
            if re.match(r'^[a-z]\.$', prev_token):
                return True
        return False

    if last_word_lower.isdigit():
        return False

    roman_pattern = r'^(i{1,3}|iv|vi{0,3}|ix|xi{0,3}|xiv|xvi{0,3}|xix|xxi{0,3})$'
    if re.match(roman_pattern, last_word_lower):
        return False

    return False


def _count_real_sentences(text):
    if not text:
        return 0
    count = 0
    for i, ch in enumerate(text):
        if ch in '.!?':
            if ch == '.' and _is_abbreviation_period(text, i):
                continue
            count += 1
    return max(1, count)


def _needs_post_split(orig_text, source_lang):
    if source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]:
        return False
    if not orig_text:
        return False
    numbering_patterns = [
        r'\d+\.\s',
        r'(?<![A-Za-z.])[A-Za-z]\.\s',
        r'\([0-9]+\)\s',
        r'\([A-Za-z]\)\s',
        r'(?<![A-Za-z])[IVXivx]+\.\s',
    ]
    for p in numbering_patterns:
        if re.search(p, orig_text):
            return True
    return _count_real_sentences(orig_text) > 1


def _split_row_with_ai(orig, trans, model_id, source_lang, row_num="åå¤„ç†"):
    prompt = f"""è¯·æ ¹æ®åŸæ–‡çš„æ ‡ç‚¹åˆ†å¥ï¼Œå°†è¯‘æ–‡å¯¹åº”æ‹†åˆ†å¯¹é½ï¼š

ã€åŸæ–‡ã€‘
{orig}

ã€è¯‘æ–‡ã€‘
{trans}

è¾“å‡ºåˆ†å¥å¯¹é½ç»“æœï¼ˆä»¥åŸæ–‡åˆ†å¥æ•°é‡ä¸ºå‡†ï¼‰ï¼š"""
    resp = _call_llm(
        _get_split_row_prompt(source_lang),
        prompt,
        model_id,
        max_output=_get_model_max_output(model_id),
    )
    if resp:
        results = _parse_alignment_response(resp)
        if results:
            return results
    return None


def _post_process_split(data, model_id, source_lang):
    if source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]:
        return data
    result = []
    for idx, row in enumerate(data):
        orig = row.get('åŸæ–‡', '')
        trans = row.get('è¯‘æ–‡', '')
        if _needs_post_split(orig, source_lang):
            split_results = _split_row_with_ai(orig, trans, model_id, source_lang, f"åå¤„ç†-{idx + 1}")
            if split_results and len(split_results) > 1:
                result.extend(split_results)
            else:
                result.append(row)
        else:
            result.append(row)
    return result


# â”€â”€ è´¨é‡æ£€æŸ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _quality_check(df, source_lang, target_lang):
    issues = []
    source_info = SUPPORTED_LANGUAGES.get(source_lang, SUPPORTED_LANGUAGES["ä¸­æ–‡"])
    target_info = SUPPORTED_LANGUAGES.get(target_lang, SUPPORTED_LANGUAGES["è‹±è¯­"])
    source_pattern = source_info['char_pattern']
    target_pattern = target_info['char_pattern']

    for idx, row in df.iterrows():
        orig = str(row.get('åŸæ–‡', ''))
        trans = str(row.get('è¯‘æ–‡', ''))
        if not orig or not trans:
            continue

        source_chars_in_original = len(re.findall(source_pattern, orig))
        target_chars_in_original = len(re.findall(target_pattern, orig))
        source_chars_in_trans = len(re.findall(source_pattern, trans))
        target_chars_in_trans = len(re.findall(target_pattern, trans))

        total_original = len(orig) if orig else 1
        total_trans = len(trans) if trans else 1

        source_ratio_original = source_chars_in_original / total_original
        target_ratio_trans = target_chars_in_trans / total_trans
        source_ratio_trans = source_chars_in_trans / total_trans
        target_ratio_original = target_chars_in_original / total_original

        if source_ratio_original < 0.2 and source_ratio_trans > 0.4:
            issues.append(f"è¡Œ{idx + 1}: è¯­è¨€é”™ä½ï¼ˆåŸæ–‡ç–‘ä¼¼{target_lang}ï¼Œè¯‘æ–‡ç–‘ä¼¼{source_lang}ï¼‰")
        elif target_ratio_original > 0.4 and target_ratio_trans < 0.2:
            issues.append(f"è¡Œ{idx + 1}: è¯­è¨€é”™ä½ï¼ˆåŸæ–‡ç–‘ä¼¼{target_lang}ï¼Œè¯‘æ–‡ç–‘ä¼¼{source_lang}ï¼‰")

        lo, lt = len(orig), len(trans)
        if lo > 0 and lt > 0:
            ratio = max(lo, lt) / min(lo, lt)
            if ratio > 5:
                issues.append(f"è¡Œ{idx + 1}: é•¿åº¦æ¯” {ratio:.1f}:1")
    return issues


# â”€â”€ Excel åŒæ–‡ä»¶å¯¹é½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _needs_table_cell_split(text, source_lang):
    if not text or not text.strip():
        return False
    import re

    source_is_cjk = source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]
    has_newline = '\n' in text or '\r' in text
    has_consecutive_spaces = '  ' in text

    if source_is_cjk:
        period_count = text.count('ã€‚')
        exclamation_count = text.count('ï¼')
        question_count = text.count('ï¼Ÿ')
    else:
        period_count = text.count('.')
        exclamation_count = text.count('!')
        question_count = text.count('?')

    total_punct_count = period_count + exclamation_count + question_count

    has_numbered_list = bool(re.search(
        r'ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ï¼‰|'
        r'[ï¼ˆ\(]\d+[ï¼‰\)]|'
        r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]|'
        r'\d+\.(?!\d)\s*\S|'
        r'\d+ã€\s*\S|'
        r'\d+[ï¼‰\)]\s*\S|'
        r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼.]',
        text
    ))

    return (
        total_punct_count > 1 or
        exclamation_count > 0 or
        question_count > 0 or
        has_newline or
        has_consecutive_spaces or
        has_numbered_list
    )


def _split_table_cell_with_ai(orig, trans, model_id, source_lang):
    prompt = f"""è¯·æ ¹æ®åŸæ–‡çš„å¥æœ«æ ‡ç‚¹ï¼ˆã€‚ï¼ï¼Ÿï¼‰å’Œæ¢è¡Œç¬¦è¿›è¡Œåˆ†å¥ï¼Œå°†è¯‘æ–‡å¯¹åº”æ‹†åˆ†å¯¹é½ï¼š

ã€åŸæ–‡ã€‘
{orig}

ã€è¯‘æ–‡ã€‘
{trans}

è¾“å‡ºåˆ†å¥å¯¹é½ç»“æœï¼š"""
    resp = _call_llm(
        _get_table_cell_split_prompt(source_lang),
        prompt,
        model_id,
        max_output=_get_model_max_output(model_id),
    )
    if resp:
        results = _parse_alignment_response(resp)
        if results:
            return results
    return None


def _process_excel_dual(orig_path, trans_path, output_path, model_id, source_lang, target_lang, task_id):
    orig_sheets = _read_excel_all_sheets(orig_path)
    trans_sheets = _read_excel_all_sheets(trans_path)
    if not orig_sheets or not trans_sheets:
        return False

    common = set(orig_sheets.keys()) & set(trans_sheets.keys())
    if common:
        pairs = [(n, n) for n in sorted(common)]
    else:
        pairs = list(zip(orig_sheets.keys(), trans_sheets.keys()))

    all_results = []
    total_pairs = len(pairs)

    for si, (orig_name, trans_name) in enumerate(pairs):
        _update_progress(task_id, 20 + int(60 * si / total_pairs),
                         f"å¤„ç†å·¥ä½œç°¿ {si + 1}/{total_pairs}: {orig_name}")
        df_o = orig_sheets[orig_name]
        df_t = trans_sheets[trans_name]
        max_rows = max(df_o.shape[0], df_t.shape[0])
        max_cols = max(df_o.shape[1], df_t.shape[1])

        for r in range(max_rows):
            for c in range(max_cols):
                orig_text = ""
                if r < df_o.shape[0] and c < df_o.shape[1]:
                    v = df_o.iloc[r, c]
                    if pd.notna(v):
                        orig_text = str(v).strip()
                trans_text = ""
                if r < df_t.shape[0] and c < df_t.shape[1]:
                    v = df_t.iloc[r, c]
                    if pd.notna(v):
                        trans_text = str(v).strip()
                if not orig_text and not trans_text:
                    continue
                if orig_text and trans_text and _needs_table_cell_split(orig_text, source_lang):
                    split = _split_table_cell_with_ai(orig_text, trans_text, model_id, source_lang)
                    if split:
                        all_results.extend(split)
                    else:
                        all_results.append({"åŸæ–‡": orig_text, "è¯‘æ–‡": trans_text})
                else:
                    if orig_text or trans_text:
                        all_results.append({"åŸæ–‡": orig_text, "è¯‘æ–‡": trans_text})

    if not all_results:
        return False
    pd.DataFrame(all_results).to_excel(output_path, index=False)
    return True


# â”€â”€ åˆå¹¶å»é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _merge_excels(paths, final_path, source_lang, target_lang):
    dfs = []
    for p in paths:
        if os.path.exists(p):
            try:
                df = pd.read_excel(p)
                if not df.empty:
                    dfs.append(df)
            except Exception:
                pass
    if not dfs:
        return None
    combined = pd.concat(dfs, ignore_index=True)
    combined.drop_duplicates(subset=['åŸæ–‡', 'è¯‘æ–‡'], keep='first', inplace=True)
    combined.to_excel(final_path, index=False)

    try:
        from openpyxl import load_workbook
        from openpyxl.styles import PatternFill
        wb = load_workbook(final_path)
        ws = wb.active
        yellow = PatternFill(start_color='FFFF00', end_color='FFFF00', fill_type='solid')
        orig_dup = combined.duplicated(subset=['åŸæ–‡'], keep=False)
        trans_dup = combined.duplicated(subset=['è¯‘æ–‡'], keep=False)
        for idx in combined[orig_dup | trans_dup].index:
            for col in range(1, 3):
                ws.cell(row=idx + 2, column=col).fill = yellow
        wb.save(final_path)
    except Exception:
        pass

    return final_path


# â”€â”€ å•æ–‡ä»¶å¯¹é½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _run_single_alignment(orig_path, trans_path, output_path, model_id,
                          source_lang, target_lang, enable_post_split,
                          system_prompt_override=None,
                          anchor_orig=None, anchor_trans=None,
                          max_output=65536):
    print(f"[alignment] _run_single_alignment: orig={orig_path}")
    print(f"[alignment]   orig exists={os.path.exists(orig_path)}, trans exists={os.path.exists(trans_path)}")

    text_original = _read_file_content(orig_path)
    text_trans = _read_file_content(trans_path)

    print(f"[alignment]   åŸæ–‡é•¿åº¦={len(text_original)}, è¯‘æ–‡é•¿åº¦={len(text_trans)}")

    if not text_original or not text_trans:
        print(f"[alignment]   å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
        return False

    if system_prompt_override:
        sys_prompt = system_prompt_override
    else:
        sys_prompt = _get_docx_alignment_prompt(source_lang, target_lang)

    anchor_hint = ""
    if anchor_orig and anchor_trans:
        anchor_hint = f"""
## ä¸Šä¸‹æ–‡æç¤º
- åŸæ–‡å¼€å¤´: "{anchor_orig.get('first_anchor', '')[:100]}"
- è¯‘æ–‡å¼€å¤´: "{anchor_trans.get('first_anchor', '')[:100]}"
"""

    user_prompt = f"""{anchor_hint}

<Stream_A_Original>
{text_original}
</Stream_A_Original>

<Stream_B_Translation>
{text_trans}
</Stream_B_Translation>

è¯·ä¸¥æ ¼æŒ‰è§„åˆ™è¾“å‡ºå¯¹é½ç»“æœï¼š
"""

    print(f"[alignment]   è°ƒç”¨ LLM (model={model_id})...")
    try:
        response = _call_llm(sys_prompt, user_prompt, model_id, max_output=max_output)
    except Exception as e:
        print(f"[alignment]   LLM è°ƒç”¨å¼‚å¸¸: {e}")
        traceback.print_exc()
        return False
    print(f"[alignment]   LLM è¿”å›é•¿åº¦={len(response) if response else 0}")

    if not response:
        print(f"[alignment]   LLM è¿”å›ä¸ºç©º (None æˆ–ç©ºå­—ç¬¦ä¸²)")
        return False

    print(f"[alignment]   LLM å‰300å­—: {response[:300]}")
    last_500 = response[-500:] if len(response) > 500 else response
    if '|||' not in last_500:
        print("[alignment]   è­¦å‘Š: è¾“å‡ºå¯èƒ½è¢«æˆªæ–­ï¼ˆæœ€å500å­—ç¬¦æ— åˆ†éš”ç¬¦ï¼‰")

    data = _parse_alignment_response(response)
    print(f"[alignment]   è§£æå¾—åˆ° {len(data)} è¡Œ")

    if not data:
        print(f"[alignment]   è§£æç»“æœä¸ºç©ºï¼Œè¿”å› False")
        print(f"[alignment]   LLM åŸå§‹å“åº”å‰500å­—: {response[:500]}")
        return False

    if enable_post_split and source_lang not in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]:
        data = _post_process_split(data, model_id, source_lang)

    df = pd.DataFrame(data)
    df.to_excel(output_path, index=False)
    print(f"[alignment]   å·²ä¿å­˜: {output_path} ({len(df)} è¡Œ)")
    return True


# â”€â”€ åˆ—å‡ºä¸­é—´æ–‡ä»¶ â”€â”€
def _list_intermediate_files(temp_dir: str) -> list:
    """æ‰«æä¸­é—´æ–‡ä»¶ç›®å½•ï¼Œè¿”å› [{name, path, type}] åˆ—è¡¨"""
    files = []
    if not os.path.isdir(temp_dir):
        return files
    for f in sorted(os.listdir(temp_dir)):
        full = os.path.join(temp_dir, f)
        if os.path.isfile(full):
            rel = os.path.relpath(full, ".").replace("\\", "/")
            ext = os.path.splitext(f)[1].lower()
            ftype = "excel" if ext in (".xlsx", ".xls") else "word" if ext in (".docx", ".doc") else "other"
            files.append({"name": f, "path": rel, "type": ftype})
    return files


# â”€â”€ åŒæ­¥ä¸»å¤„ç†ï¼ˆ1:1 å¤åˆ» memory.py GUI çš„ run_processingï¼‰â”€â”€â”€â”€â”€
def _run_alignment_sync(
    original_path: str,
    translated_path: str,
    task_id: str,
    source_lang: str,
    target_lang: str,
    model_name: str,
    enable_post_split: bool,
    threshold_2: int = 25000,
    threshold_3: int = 50000,
    threshold_4: int = 75000,
    threshold_5: int = 100000,
    threshold_6: int = 125000,
    threshold_7: int = 150000,
    threshold_8: int = 175000,
    buffer_chars: int = 2000,
):
    """åŒæ­¥æ‰§è¡Œå¯¹é½ä»»åŠ¡ - ç›´æ¥è°ƒç”¨ memory.py åŸç”Ÿå‡½æ•°ï¼Œé›¶ monkey-patch"""
    memory_module = None
    original_log_stream = None
    original_log = None
    original_log_exception = None
    try:
        _update_progress(task_id, 5, "æ­£åœ¨åŠ è½½å¤„ç†å¼•æ“...", stream_log="")

        memory_module = _get_memory_module()

        # ä»… patch log_stream ç”¨äºå‰ç«¯å±•ç¤ºï¼ˆä¸æ”¹ä»»ä½•å¤„ç†é€»è¾‘ï¼‰
        _stream_task_id.task_id = task_id
        original_log_stream = memory_module.log_manager.log_stream

        def _patched_log_stream(content):
            original_log_stream(content)
            tid = getattr(_stream_task_id, "task_id", None)
            if tid and tid in _task_progress:
                cur = _task_progress[tid].get("stream_log", "")
                _task_progress[tid]["stream_log"] = cur + (content if isinstance(content, str) else str(content))

        memory_module.log_manager.log_stream = _patched_log_stream

        # ä¹Ÿæ•è· log å’Œ exception åˆ° stream_log
        original_log = memory_module.log_manager.log
        original_log_exception = memory_module.log_manager.log_exception

        def _patched_log(message, level="INFO"):
            original_log(message, level)
            print(f"[memory-log] {message}")
            tid = getattr(_stream_task_id, "task_id", None)
            if tid and tid in _task_progress:
                ts = datetime.now().strftime("%H:%M:%S")
                line = f"[{ts}] {message}\n"
                cur = _task_progress[tid].get("stream_log", "")
                _task_progress[tid]["stream_log"] = cur + line

        def _patched_log_exception(message, data=None):
            original_log_exception(message, data)
            print(f"[memory-ERR] {message}" + (f" | {str(data)[:300]}" if data else ""))
            tid = getattr(_stream_task_id, "task_id", None)
            if tid and tid in _task_progress:
                ts = datetime.now().strftime("%H:%M:%S")
                line = f"[{ts}] âš ï¸ {message}"
                if data:
                    line += f" | {str(data)[:200]}"
                cur = _task_progress[tid].get("stream_log", "")
                _task_progress[tid]["stream_log"] = cur + line + "\n"

        memory_module.log_manager.log = _patched_log
        memory_module.log_manager.log_exception = _patched_log_exception

        # ç»å¯¹è·¯å¾„
        original_path = os.path.abspath(original_path)
        translated_path = os.path.abspath(translated_path)

        model_info = AVAILABLE_MODELS.get(model_name, AVAILABLE_MODELS[DEFAULT_MODEL])
        model_id = model_info['id']

        file_type = memory_module.get_file_type(original_path)
        base_name = os.path.splitext(os.path.basename(original_path))[0]
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        task_dir = os.path.join(OUTPUT_DIR, "alignment", f"{base_name}_{timestamp}")
        temp_dir = os.path.join(task_dir, "ä¸­é—´æ–‡ä»¶")
        os.makedirs(task_dir, exist_ok=True)
        os.makedirs(temp_dir, exist_ok=True)

        print(f"[alignment] task_id={task_id}")
        print(f"[alignment] original={original_path}, exists={os.path.exists(original_path)}, size={os.path.getsize(original_path) if os.path.exists(original_path) else 'N/A'}")
        print(f"[alignment] translated={translated_path}, exists={os.path.exists(translated_path)}, size={os.path.getsize(translated_path) if os.path.exists(translated_path) else 'N/A'}")
        print(f"[alignment] file_type={file_type}, model={model_name} ({model_id})")
        print(f"[alignment] lang: {source_lang} â†’ {target_lang}")
        _patched_log(f"è¯­è¨€å¯¹: {source_lang} â†’ {target_lang}")
        _patched_log(f"åå¤„ç†åˆ†å¥: {'å¯ç”¨' if enable_post_split else 'ç¦ç”¨'}")
        _patched_log(f"æ¨¡å‹: {model_name} ({model_id})")
        _patched_log(f"æ–‡ä»¶ç±»å‹: {file_type.upper()}")

        _update_progress(task_id, 8, "æ­£åœ¨åˆ†ææ–‡æ¡£...")

        # .doc â†’ .docx è½¬æ¢
        if file_type == 'doc':
            _update_progress(task_id, 8, "æ£€æµ‹åˆ° .doc æ–‡ä»¶ï¼Œæ­£åœ¨è½¬æ¢...")
            converted_orig = memory_module.convert_doc_to_docx(original_path, temp_dir)
            converted_trans = memory_module.convert_doc_to_docx(translated_path, temp_dir)
            if converted_orig is None or converted_trans is None:
                _complete_task(task_id, error="æ— æ³•è½¬æ¢ .doc æ–‡ä»¶ï¼Œè¯·å®‰è£… pywin32")
                return
            original_path = converted_orig
            translated_path = converted_trans
            file_type = 'docx'

        # â”€â”€ Excel åŒæ–‡ä»¶æ¨¡å¼ â”€â”€
        if file_type == 'excel':
            _update_progress(task_id, 10, "Excel åŒæ–‡ä»¶å¯¹é½æ¨¡å¼...")
            out_path = os.path.join(task_dir, f"{base_name}_å¯¹é½ç»“æœ.xlsx")
            success = memory_module.process_excel_dual_file_alignment(
                original_path, translated_path, out_path, model_id,
                source_lang=source_lang, target_lang=target_lang,
            )
            if success and os.path.exists(out_path):
                rel = os.path.relpath(out_path, ".").replace("\\", "/")
                _complete_task(task_id, result={
                    "output_excel": rel,
                    "row_count": len(pd.read_excel(out_path)),
                    "file_type": "excel",
                    "intermediate_files": _list_intermediate_files(temp_dir),
                })
            else:
                _complete_task(task_id, error="Excel å¯¹é½å¤„ç†å¤±è´¥")
            return

        # â”€â”€ Word / PPT â”€â”€
        _update_progress(task_id, 10, "åˆ†ææ–‡æ¡£ç»“æ„...")
        count_a, _ = memory_module.analyze_document_structure(original_path, source_lang)
        count_b, _ = memory_module.analyze_document_structure(translated_path, target_lang)

        s_info = SUPPORTED_LANGUAGES.get(source_lang, {})
        t_info = SUPPORTED_LANGUAGES.get(target_lang, {})
        s_unit = "å­—" if not s_info.get('word_based', True) else "è¯"
        t_unit = "å­—" if not t_info.get('word_based', True) else "è¯"

        print(f"[alignment] åŸæ–‡: {count_a:,} {s_unit}, è¯‘æ–‡: {count_b:,} {t_unit}")
        _patched_log(f"åŸæ–‡: {count_a:,} {s_unit}")
        _patched_log(f"è¯‘æ–‡: {count_b:,} {t_unit}")
        _update_progress(task_id, 15, f"åŸæ–‡ {count_a:,} {s_unit}ï¼Œè¯‘æ–‡ {count_b:,} {t_unit}")

        # åˆ†å‰²ç­–ç•¥ï¼ˆä¸ GUI run_processing 1:1ï¼‰
        if file_type == 'pptx':
            split_parts = 1
            _patched_log("PPTæ–‡ä»¶ï¼šä¸è¿›è¡Œåˆ†å‰²")
        else:
            max_count = max(count_a, count_b)
            split_parts = 1
            if max_count > threshold_8:
                split_parts = 8
            elif max_count > threshold_7:
                split_parts = 7
            elif max_count > threshold_6:
                split_parts = 6
            elif max_count > threshold_5:
                split_parts = 5
            elif max_count > threshold_4:
                split_parts = 4
            elif max_count > threshold_3:
                split_parts = 3
            elif max_count > threshold_2:
                split_parts = 2
            _patched_log(f"åˆ†å‰²ç­–ç•¥: {split_parts} ä»½")

        _update_progress(task_id, 20, f"åˆ†å‰²ç­–ç•¥: {split_parts} ä»½")

        tasks_queue = []
        generated_excel_paths = []

        if split_parts > 1 and file_type == 'docx':
            _update_progress(task_id, 25, f"æ­£åœ¨åˆ†å‰²æ–‡æ¡£ï¼ˆ{split_parts} ä»½ï¼Œç¼“å†²åŒº {buffer_chars} å­—ï¼‰...")
            _patched_log(f"åˆ†å‰²åŸæ–‡ï¼ˆä¸»æ–‡æ¡£ï¼Œè‡ªä¸»è®¡ç®—åˆ†å‰²ç‚¹ï¼‰...")
            files_a, part_info_a, split_ratios = memory_module.smart_split_with_buffer(
                original_path, split_parts, temp_dir, source_lang, buffer_chars)
            _patched_log(f"åˆ†å‰²è¯‘æ–‡ï¼ˆä»æ–‡æ¡£ï¼Œä½¿ç”¨åŸæ–‡çš„åˆ†å‰²æ¯”ä¾‹ï¼‰...")
            files_b, part_info_b, _ = memory_module.smart_split_with_buffer(
                translated_path, split_parts, temp_dir, target_lang, buffer_chars,
                split_element_ratios=split_ratios)

            for i in range(len(files_a)):
                out = os.path.join(temp_dir, f"Part{i + 1}_å¯¹é½ç»“æœ.xlsx")
                tasks_queue.append({
                    'original': files_a[i], 'trans': files_b[i], 'output': out,
                    'anchor_orig': part_info_a[i] if part_info_a else None,
                    'anchor_trans': part_info_b[i] if part_info_b else None,
                })
                generated_excel_paths.append(out)
        else:
            out = os.path.join(task_dir, f"{base_name}_å¯¹é½ç»“æœ.xlsx")
            ppt_prompt = memory_module.get_ppt_alignment_prompt(source_lang, target_lang) if file_type == 'pptx' else None
            tasks_queue.append({
                'original': original_path, 'trans': translated_path, 'output': out,
                'anchor_orig': None, 'anchor_trans': None,
                'system_prompt_override': ppt_prompt,
            })
            generated_excel_paths.append(out)

        # â”€â”€ AI å¯¹é½ â”€â”€
        _update_progress(task_id, 30, f"AI å¯¹é½ä¸­ï¼ˆå…± {len(tasks_queue)} ä¸ªä»»åŠ¡ï¼‰...")
        _patched_log(f"å¾…å¤„ç†ä»»åŠ¡æ•°: {len(tasks_queue)}")
        progress_per_task = 50 / len(tasks_queue) if tasks_queue else 50

        for idx, task in enumerate(tasks_queue):
            progress = 30 + int((idx + 1) * progress_per_task)
            _update_progress(task_id, progress,
                             f"AI å¯¹é½ä¸­ ({idx + 1}/{len(tasks_queue)})...")

            _patched_log(f"å¤„ç†ä»»åŠ¡ {idx + 1}/{len(tasks_queue)}: {os.path.basename(task['output'])}")
            print(f"[alignment] å¤„ç†ä»»åŠ¡ {idx + 1}/{len(tasks_queue)}")
            print(f"[alignment]   original: {task['original']}")
            print(f"[alignment]   trans: {task['trans']}")
            print(f"[alignment]   output: {task['output']}")

            # è¯Šæ–­ï¼šç›´æ¥ç”¨ memory çš„ read_file_content æµ‹è¯•è¯»å–
            try:
                test_orig = memory_module.read_file_content(task['original'])
                test_trans = memory_module.read_file_content(task['trans'])
                print(f"[alignment]   read_file_content åŸæ–‡: {len(test_orig)} å­—ç¬¦")
                print(f"[alignment]   read_file_content è¯‘æ–‡: {len(test_trans)} å­—ç¬¦")
                if test_orig:
                    print(f"[alignment]   åŸæ–‡å‰200å­—: {test_orig[:200]}")
                if test_trans:
                    print(f"[alignment]   è¯‘æ–‡å‰200å­—: {test_trans[:200]}")
            except Exception as diag_e:
                print(f"[alignment]   read_file_content è¯Šæ–­å¼‚å¸¸: {diag_e}")

            success = memory_module.run_llm_alignment(
                task['original'],
                task['trans'],
                task['output'],
                model_id,
                anchor_info_orig=task.get('anchor_orig'),
                anchor_info_trans=task.get('anchor_trans'),
                system_prompt_override=task.get('system_prompt_override'),
                source_lang=source_lang,
                target_lang=target_lang,
                enable_post_split=enable_post_split,
            )

            print(f"[alignment] ä»»åŠ¡ {idx + 1} ç»“æœ: success={success}, output_exists={os.path.exists(task['output'])}")
            if success and os.path.exists(task['output']):
                _patched_log(f"ä»»åŠ¡ {idx + 1} æˆåŠŸ: {os.path.basename(task['output'])}")
            else:
                _patched_log(f"ä»»åŠ¡ {idx + 1} å¤±è´¥: {os.path.basename(task['output'])}")
                if task['output'] in generated_excel_paths:
                    generated_excel_paths.remove(task['output'])

        # â”€â”€ åˆå¹¶ â”€â”€
        _update_progress(task_id, 85, "åˆå¹¶ä¸å»é‡...")
        final_path = None
        if split_parts > 1 and len(generated_excel_paths) > 0:
            final_path = os.path.join(task_dir, f"ã€Œæœ€ç»ˆç»“æœã€{base_name}_å¯¹é½.xlsx")
            memory_module.merge_and_deduplicate_excels(
                generated_excel_paths, final_path,
                source_lang=source_lang, target_lang=target_lang,
            )
        else:
            final_path = generated_excel_paths[0] if generated_excel_paths else None

        print(f"[alignment] final_path={final_path}, exists={os.path.exists(final_path) if final_path else 'N/A'}")
        print(f"[alignment] generated_excel_paths={generated_excel_paths}")
        if final_path and os.path.exists(final_path):
            rel = os.path.relpath(final_path, ".").replace("\\", "/")
            row_count = len(pd.read_excel(final_path))
            issues = _quality_check(pd.read_excel(final_path), source_lang, target_lang)
            print(f"[alignment] å®Œæˆ! {row_count} è¡Œ")
            _patched_log(f"å¤„ç†å®Œæˆï¼{row_count} è¡Œ, è·¯å¾„: {rel}")
            _complete_task(task_id, result={
                "output_excel": rel,
                "row_count": row_count,
                "file_type": file_type,
                "split_parts": split_parts,
                "issues": issues[:10] if issues else [],
                "intermediate_files": _list_intermediate_files(temp_dir),
            })
        else:
            print(f"[alignment] å¤±è´¥: æ— æœ‰æ•ˆè¾“å‡ºæ–‡ä»¶")
            _complete_task(task_id, error="å¯¹é½å¤„ç†å¤±è´¥ï¼Œæœªç”Ÿæˆç»“æœæ–‡ä»¶ã€‚è¯·æŸ¥çœ‹å®æ—¶è¾“å‡ºäº†è§£è¯¦æƒ…")

    except Exception as e:
        tb = traceback.format_exc()
        print(f"[alignment] å¼‚å¸¸:\n{tb}")
        _complete_task(task_id, error=str(e))
    finally:
        if memory_module is not None:
            if original_log_stream is not None:
                memory_module.log_manager.log_stream = original_log_stream
            if original_log is not None:
                memory_module.log_manager.log = original_log
            if original_log_exception is not None:
                memory_module.log_manager.log_exception = original_log_exception
        if hasattr(_stream_task_id, "task_id"):
            del _stream_task_id.task_id


# â”€â”€ å¼‚æ­¥å…¥å£ï¼ˆä¾› BackgroundTasks è°ƒç”¨ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def run_alignment_task(
    original_path: str,
    translated_path: str,
    task_id: str,
    source_lang: str = "ä¸­æ–‡",
    target_lang: str = "è‹±è¯­",
    model_name: str = DEFAULT_MODEL,
    enable_post_split: bool = True,
    threshold_2: int = 25000,
    threshold_3: int = 50000,
    threshold_4: int = 75000,
    threshold_5: int = 100000,
    threshold_6: int = 125000,
    threshold_7: int = 150000,
    threshold_8: int = 175000,
    buffer_chars: int = 2000,
):
    """åœ¨åå°çº¿ç¨‹æ± ä¸­æ‰§è¡Œå¯¹é½ä»»åŠ¡"""
    import functools
    loop = asyncio.get_running_loop()
    await loop.run_in_executor(
        None,
        functools.partial(
            _run_alignment_sync,
            original_path, translated_path, task_id,
            source_lang, target_lang, model_name, enable_post_split,
            threshold_2=threshold_2, threshold_3=threshold_3,
            threshold_4=threshold_4, threshold_5=threshold_5,
            threshold_6=threshold_6, threshold_7=threshold_7,
            threshold_8=threshold_8, buffer_chars=buffer_chars,
        ),
    )
