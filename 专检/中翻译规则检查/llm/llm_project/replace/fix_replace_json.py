import os
import shutil
from typing import List, Dict, Any, Tuple
from datetime import datetime
from docx import Document
from lxml import etree
from zipfile import ZipFile
import warnings
from docx.oxml.shared import OxmlElement
from docx.oxml.ns import qn
from docx.opc.constants import RELATIONSHIP_TYPE as RT
import json
from llm.llm_project.llm_check.check import Match
from llm.llm_project.parsers.body_extractor import extract_body_text
from llm.llm_project.parsers.footer_extractor import extract_footers
from llm.llm_project.parsers.header_extractor import extract_headers
from llm.utils.clean_json import clean_markdown_json, load_json_file
from llm.utils.json_files import write_json_with_timestamp

warnings.filterwarnings("ignore")

# =========================
# 0) åŸºç¡€é…ç½®
# =========================

BACKUP_DIR_NAME = "backup"

# =========================
# 1) DOCX é¢„åŠ è½½
# =========================

W_NS = "http://schemas.openxmlformats.org/wordprocessingml/2006/main"
NAMESPACES = {"w": W_NS}


def get_xml_text(element) -> str:
    """ä»ä»»æ„XMLå…ƒç´ åŠå…¶å­å…ƒç´ ä¸­æå–çº¯æ–‡æœ¬"""
    texts = []
    for t in element.iter(f"{{{W_NS}}}t"):
        if t.text:
            texts.append(t.text)
    return "".join(texts)



class DocContentLoader:
    def __init__(self, doc_path: str):
        self.doc_path = doc_path
        self.footnotes = {}
        self.endnotes = {}
        self.comments = {}
        self.headers = []
        self.footers = []
        self._load_all()

    def _load_xml_map(self, zip_file, filename, tag_name, id_attr="id"):
        data_map = {}
        if filename not in zip_file.namelist():
            return data_map

        with zip_file.open(filename) as f:
            tree = etree.parse(f)
            for elem in tree.findall(f".//w:{tag_name}", NAMESPACES):
                eid = elem.get(f"{{{W_NS}}}{id_attr}")
                if eid in ["-1", "0"]:
                    continue
                texts = [t.text for t in elem.iter(f"{{{W_NS}}}t") if t.text]
                full_text = "".join(texts).strip()
                if full_text:
                    data_map[eid] = full_text
        return data_map

    def _load_headers_footers(self, zip_file, prefix):
        contents = []
        files = [f for f in zip_file.namelist() if f.startswith(f"word/{prefix}") and f.endswith(".xml")]
        files.sort(key=lambda x: int("".join(filter(str.isdigit, x)) or 0))
        for f_name in files:
            try:
                with zip_file.open(f_name) as f:
                    tree = etree.parse(f)
                    texts = [t.text for t in tree.iter(f"{{{W_NS}}}t") if t.text]
                    full_text = "".join(texts).strip()
                    if full_text:
                        contents.append(full_text)
            except Exception:
                pass
        return contents

    def _load_all(self):
        with ZipFile(self.doc_path, "r") as zf:
            self.footnotes = self._load_xml_map(zf, "word/footnotes.xml", "footnote")
            self.endnotes = self._load_xml_map(zf, "word/endnotes.xml", "endnote")
            self.comments = self._load_xml_map(zf, "word/comments.xml", "comment")
            self.headers = self._load_headers_footers(zf, "header")
            self.footers = self._load_headers_footers(zf, "footer")


# =========================
# 2) é”™è¯¯æŠ¥å‘Šè§£æï¼ˆå¢å¼ºç‰ˆï¼‰
# =========================

def _normalize_spaces(s: str) -> str:
    """æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦"""
    return re.sub(r"\s+", " ", (s or "")).strip()


import re
import ast
from typing import Any, Dict, List, Optional


def _unify_labels(raw: str) -> str:
    """
    ç»Ÿä¸€â€œå­—æ®µå + å†’å·â€çš„å„ç§æ–­è¡Œ/ç©ºæ ¼å½¢æ€ï¼Œå‡å°‘è§£ææ­§ä¹‰ã€‚
    ç›®æ ‡ï¼šæŠŠ 'è¯‘æ–‡\\nä¿®æ”¹å»ºè®®å€¼\\n: xxx'ã€'è¯‘ æ–‡ä¿®æ”¹å»ºè®®å€¼: xxx' ç­‰ç»Ÿä¸€æˆ 'è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼: xxx'
    """
    raw = raw or ""

    # å…ˆæŠŠå…¨å±€çš„å›è½¦/åˆ¶è¡¨ç­‰ç»Ÿä¸€ä¸º \n / ç©ºæ ¼
    raw = raw.replace("\r\n", "\n").replace("\r", "\n").replace("\t", " ")

    # è¿™äº›å­—æ®µåæœ€å…³é”®ï¼šå…è®¸ä¸­é—´ä»»æ„ç©ºç™½ï¼ˆå«æ¢è¡Œï¼‰
    label_patterns = [
        "é”™è¯¯ç¼–å·",
        "é”™è¯¯ç±»å‹",
        "åŸæ–‡æ•°å€¼",
        "è¯‘æ–‡æ•°å€¼",
        "è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼",
        "ä¿®æ”¹ç†ç”±",
        "è¿åçš„è§„åˆ™",
        "åŸæ–‡ä¸Šä¸‹æ–‡",
        "è¯‘æ–‡ä¸Šä¸‹æ–‡",
        "åŸæ–‡ä½ç½®",
        "è¯‘æ–‡ä½ç½®",
        "æ›¿æ¢é”šç‚¹",
    ]

    # æŠŠâ€œå­—æ®µåâ€å†…éƒ¨çš„ç©ºç™½å‹å¹³ï¼Œå¹¶è®©å†’å·ç´§è·Ÿå­—æ®µå
    for lab in label_patterns:
        # ä¾‹å¦‚ï¼šè¯‘æ–‡\s*ä¿®æ”¹\s*å»ºè®®\s*å€¼\s*[:ï¼š]  ->  è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼:
        chars = list(lab)
        pat = r"".join(re.escape(c) + r"\s*" for c in chars) + r"[:ï¼š]?"
        raw = re.sub(pat, lab, raw, flags=re.IGNORECASE)

        # ç»Ÿä¸€å†’å·ï¼šå­—æ®µååå¦‚æœç´§è·Ÿçš„ä¸æ˜¯å†’å·ï¼Œå°±è¡¥é½ï¼ˆä»…å½“åé¢ç¡®å®æœ‰å†…å®¹æ—¶ï¼‰
        raw = re.sub(rf"({re.escape(lab)})\s*\n?\s*[:ï¼š]\s*", rf"\1: ", raw)

    # ä¿®å¤ä½ åŸæ¥å¯¹â€œé”™è¯¯ç¼–å·â€åšçš„ç‰¹æ®Šæ›¿æ¢ï¼ˆä¿ç•™ï¼‰
    raw = raw.replace("é”™è¯¯\nç¼–å·", "é”™è¯¯ç¼–å·")
    raw = raw.replace("é”™è¯¯\nç¼–å·:", "é”™è¯¯ç¼–å·:")
    raw = raw.replace("ç¼–å·:", "é”™è¯¯ç¼–å·:")

    return raw

def _split_merged_fields(one: Dict[str, Any]) -> Dict[str, Any]:
    """
    å…œåº•ï¼šè‹¥è¯‘æ–‡æ•°å€¼é‡Œåäº†â€œè¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼: ...â€ï¼Œåˆ™æ‹†åˆ†å¹¶å›å¡«ä¸¤ä¸ªå­—æ®µã€‚
    å…è®¸å‡ºç°ï¼š'è¯‘ æ–‡ä¿®æ”¹å»ºè®®å€¼'ã€æ¢è¡Œã€å¤šä¸ªç©ºæ ¼ç­‰ã€‚
    """
    tv = (one.get("è¯‘æ–‡æ•°å€¼") or "").strip()
    sv = (one.get("è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼") or "").strip()

    if not tv:
        return one

    # å…è®¸å„ç§ç©ºç™½ï¼šè¯‘\s*æ–‡\s*ä¿®\s*æ”¹\s*å»º\s*è®®\s*å€¼
    marker_pat = r"(è¯‘\s*æ–‡\s*ä¿®\s*æ”¹\s*å»º\s*è®®\s*å€¼)\s*[:ï¼š]\s*"
    m = re.search(marker_pat, tv, flags=re.IGNORECASE)
    if not m:
        return one

    left = tv[:m.start()].strip()
    right = tv[m.end():].strip()

    if left:
        one["è¯‘æ–‡æ•°å€¼"] = re.sub(r"\s+", " ", left).strip()

    # å¦‚æœåŸæœ¬å»ºè®®å€¼ä¸ºç©ºï¼Œåˆ™ç”¨æ‹†å‡ºæ¥çš„è¡¥ä¸Šï¼›å¦‚æœä¸ä¸ºç©ºï¼Œä¹Ÿå¯ä»¥é€‰æ‹©ä¿ç•™åŸå€¼ï¼ˆæ›´å®‰å…¨ï¼‰
    if right and not sv:
        one["è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼"] = re.sub(r"\s+", " ", right).strip()

    return one


import re
import ast
from typing import Any, Dict, List, Optional


def _extract_first_braced_block(raw: str) -> Optional[str]:
    """
    ä» raw ä¸­æå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„ {...} æˆ– [...] å­—é¢é‡å—ï¼ˆåé¢å…è®¸è·Ÿè¯´æ˜æ–‡å­—ï¼‰ã€‚
    ç”¨äºå…¼å®¹ä½ ç»™çš„é‚£ç§ dict/list ç»“æ„åŒ–è¾“å…¥ã€‚
    """
    if not raw:
        return None

    m = re.search(r"[\{\[]", raw)
    if not m:
        return None

    start = m.start()
    open_ch = raw[start]
    close_ch = "}" if open_ch == "{" else "]"

    depth = 0
    in_str = False
    str_ch = ""
    escape = False

    for i in range(start, len(raw)):
        ch = raw[i]

        if in_str:
            if escape:
                escape = False
            elif ch == "\\":
                escape = True
            elif ch == str_ch:
                in_str = False
            continue

        if ch in ("'", '"'):
            in_str = True
            str_ch = ch
            continue

        if ch == open_ch:
            depth += 1
        elif ch == close_ch:
            depth -= 1
            if depth == 0:
                return raw[start:i + 1]

    return None


def _unify_labels_keep_colon(raw: str) -> str:
    """
    å…³é”®ï¼šæŠŠå­—æ®µåé‡Œçš„å„ç§ç©ºæ ¼/æ¢è¡Œç»Ÿä¸€æ‰ï¼Œå¹¶ä¸”ã€å¼ºåˆ¶ä¿ç•™/è¡¥é½å†’å·ã€‘ä¸º 'å­—æ®µå:'ã€‚
    è§£å†³ Word è‡ªåŠ¨æ¢è¡Œï¼š'é”™è¯¯\\nç±»å‹:'ã€'è¯‘ æ–‡ä¿®æ”¹å»ºè®®å€¼\\n:' ç­‰ã€‚
    """
    raw = raw or ""
    raw = raw.replace("\r\n", "\n").replace("\r", "\n").replace("\t", " ")

    labels = [
        "é”™è¯¯ç¼–å·",
        "é”™è¯¯ç±»å‹",
        "åŸæ–‡æ•°å€¼",
        "è¯‘æ–‡æ•°å€¼",
        "è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼",
        "ä¿®æ”¹ç†ç”±",
        "è¿åçš„è§„åˆ™",
        "åŸæ–‡ä¸Šä¸‹æ–‡",
        "è¯‘æ–‡ä¸Šä¸‹æ–‡",
        "åŸæ–‡ä½ç½®",
        "è¯‘æ–‡ä½ç½®",
        "æ›¿æ¢é”šç‚¹",
    ]

    for lab in labels:
        pat = (
                r"(?m)"  # å¤šè¡Œæ¨¡å¼
                r"(^|\n)"  # è¡Œé¦–
                + r"\s*"
                + r"".join(re.escape(c) + r"\s*" for c in lab)
                + r"\s*[:ï¼š]?"
        )
        raw = re.sub(pat, r"\1" + lab + ":", raw, flags=re.IGNORECASE)

    # å†’å·åç»Ÿä¸€ä¸€ä¸ªç©ºæ ¼ï¼ˆå«ä¸­æ–‡å†’å·ï¼‰
    raw = re.sub(r"[:ï¼š]\s*", ": ", raw)
    return raw


def _split_merged_translation_fields(one: Dict[str, Any]) -> Dict[str, Any]:
    """
    å…œåº•ï¼šå¦‚æœè¯‘æ–‡æ•°å€¼åäº†â€œè¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼: ...â€ï¼Œåˆ™æ‹†å¼€ä¸¤ä¸ªå­—æ®µã€‚
    _unify_labels_keep_colon ä¼šæŠŠå„ç§å½¢æ€ç»Ÿä¸€æˆ 'è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼:'ï¼Œå› æ­¤è¿™é‡Œæ‹†åˆ†å¾ˆç¨³ã€‚
    """
    tv = (one.get("è¯‘æ–‡æ•°å€¼") or "").strip()
    sv = (one.get("è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼") or "").strip()
    if not tv:
        return one

    marker = "è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼:"
    idx = tv.find(marker)
    if idx == -1:
        return one

    left = tv[:idx].strip()
    right = tv[idx + len(marker):].strip()

    if left:
        one["è¯‘æ–‡æ•°å€¼"] = re.sub(r"\s+", " ", left).strip()

    # å¦‚æœåŸæœ¬å»ºè®®å€¼ä¸ºç©ºï¼Œç”¨ right è¡¥é½ï¼ˆæ›´å®‰å…¨ï¼›ä¸è¦†ç›–å·²æœ‰å€¼ï¼‰
    if right and not sv:
        one["è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼"] = re.sub(r"\s+", " ", right).strip()

    return one


def load_error_list_from_text(raw: str) -> List[Dict[str, Any]]:
    """
    è§£æé”™è¯¯æŠ¥å‘Šï¼ˆå¢å¼ºç‰ˆï¼‰ï¼š
    1) ä¼˜å…ˆè§£æç»“æ„åŒ– dict/listï¼ˆä½ çš„ç¤ºä¾‹é‚£ç§ï¼‰
    2) å†è§£ææ–‡æœ¬å‹æŠ¥å‘Šï¼ˆæ”¯æŒå­—æ®µåæ–­è¡Œ/ç©ºæ ¼ã€åŒä¸€è¡Œå­—æ®µã€å†’å·æ–­è¡Œï¼‰
    3) å…œåº•æ‹†åˆ†ï¼šè¯‘æ–‡æ•°å€¼åæ‰è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼
    """
    raw = raw or ""

    # ========== 1) ç»“æ„åŒ–ä¼˜å…ˆï¼šæå–ç¬¬ä¸€ä¸ª {...} / [...] å literal_eval ==========
    block = _extract_first_braced_block(raw)
    if block:
        try:
            obj = ast.literal_eval(block)
            if isinstance(obj, dict):
                obj = [obj]
            if isinstance(obj, list):
                fixed = []
                for it in obj:
                    if isinstance(it, dict):
                        fixed.append(_split_merged_translation_fields(it))
                if fixed:
                    return fixed
        except Exception:
            pass  # å¤±è´¥å°±èµ°æ–‡æœ¬è§£æ

    # ========== 2) æ–‡æœ¬è§£æï¼šç»Ÿä¸€æ ‡ç­¾ï¼ˆä¿ç•™/è¡¥é½å†’å·ï¼‰ ==========
    cleaned = _unify_labels_keep_colon(raw)

    # æ¯æ¡é”™è¯¯ä» â€œé”™è¯¯ç¼–å·: æ•°å­—â€ å¼€å§‹
    parts = re.split(r"(?=é”™è¯¯ç¼–å·:\s*\d+)", cleaned)
    errors: List[Dict[str, Any]] = []

    # å…³é”®ï¼šè¾¹ç•Œä¸è¦ä¾èµ–æ¢è¡Œï¼Œç›´æ¥ç”¨ â€œä¸‹ä¸€ä¸ªå­—æ®µå:â€ åš lookahead
    key_map = [
        ("é”™è¯¯ç¼–å·", r"é”™è¯¯ç¼–å·:\s*(.+?)(?=é”™è¯¯ç±»å‹:|$)"),
        ("é”™è¯¯ç±»å‹", r"é”™è¯¯ç±»å‹:\s*(.+?)(?=åŸæ–‡æ•°å€¼:|$)"),
        ("åŸæ–‡æ•°å€¼", r"åŸæ–‡æ•°å€¼:\s*(.+?)(?=è¯‘æ–‡æ•°å€¼:|$)"),
        ("è¯‘æ–‡æ•°å€¼", r"è¯‘æ–‡æ•°å€¼:\s*(.+?)(?=è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼:|ä¿®æ”¹ç†ç”±:|$)"),
        ("è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼", r"è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼:\s*(.+?)(?=ä¿®æ”¹ç†ç”±:|$)"),
        ("ä¿®æ”¹ç†ç”±", r"ä¿®æ”¹ç†ç”±:\s*(.+?)(?=è¿åçš„è§„åˆ™:|$)"),
        ("è¿åçš„è§„åˆ™", r"è¿åçš„è§„åˆ™:\s*(.+?)(?=åŸæ–‡ä¸Šä¸‹æ–‡:|$)"),
        ("åŸæ–‡ä¸Šä¸‹æ–‡", r"åŸæ–‡ä¸Šä¸‹æ–‡:\s*(.+?)(?=è¯‘æ–‡ä¸Šä¸‹æ–‡:|$)"),
        ("è¯‘æ–‡ä¸Šä¸‹æ–‡", r"è¯‘æ–‡ä¸Šä¸‹æ–‡:\s*(.+?)(?=åŸæ–‡ä½ç½®:|$)"),
        ("åŸæ–‡ä½ç½®", r"åŸæ–‡ä½ç½®:\s*(.+?)(?=è¯‘æ–‡ä½ç½®:|$)"),
        ("è¯‘æ–‡ä½ç½®", r"è¯‘æ–‡ä½ç½®:\s*(.+?)(?=æ›¿æ¢é”šç‚¹:|$)"),
        ("æ›¿æ¢é”šç‚¹", r"æ›¿æ¢é”šç‚¹:\s*(.+?)(?=é”™è¯¯ç¼–å·:|$)"),
    ]

    for p in parts:
        if "é”™è¯¯ç¼–å·:" not in p:
            continue

        one: Dict[str, Any] = {}
        for k, pat in key_map:
            mm = re.search(pat, p, flags=re.DOTALL | re.IGNORECASE)
            if mm:
                v = mm.group(1).strip()
                v = re.sub(r"\s+", " ", v).strip()
                one[k] = v

        # å…œåº•æ‹†åˆ†åˆå¹¶å­—æ®µ
        one = _split_merged_translation_fields(one)

        if one.get("é”™è¯¯ç¼–å·") or one.get("è¯‘æ–‡æ•°å€¼") or one.get("è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼"):
            errors.append(one)

    if not errors:
        print("æœªåœ¨é”™è¯¯æŠ¥å‘Šæ–‡æœ¬ä¸­æ‰¾åˆ°å¯è§£æçš„é”™è¯¯åˆ—è¡¨")

    return errors




# =========================
# 3) æ”¹è¿›çš„åŒ¹é…ç­–ç•¥
# =========================

def build_smart_pattern(s: str, mode: str = "balanced") -> str:
    """
    æ„å»ºæ™ºèƒ½åŒ¹é…æ¨¡å¼

    Args:
        s: å¾…åŒ¹é…å­—ç¬¦ä¸²
        mode: åŒ¹é…æ¨¡å¼
            - "strict": ä¸¥æ ¼åŒ¹é…ï¼ˆå®Œå…¨ç²¾ç¡®ï¼‰
            - "balanced": å¹³è¡¡æ¨¡å¼ï¼ˆæ•°å­—/æ ‡ç‚¹ä¿æŒè¿ç»­ï¼Œå•è¯é—´å…è®¸ç©ºæ ¼ï¼‰
            - "loose": å®½æ¾æ¨¡å¼ï¼ˆå­—ç¬¦é—´å…è®¸ç©ºæ ¼ï¼Œä½†æ•°å­—è¿ç»­ï¼‰
    """
    s = (s or "").strip()
    if not s:
        return ""

    if mode == "strict":
        # ä¸¥æ ¼æ¨¡å¼ï¼šå®Œå…¨ç²¾ç¡®åŒ¹é…ï¼ˆè½¬ä¹‰ç‰¹æ®Šå­—ç¬¦ï¼‰
        return re.escape(s)

    elif mode == "balanced":
        # å¹³è¡¡æ¨¡å¼ï¼šæ•°å­—å’Œæ ‡ç‚¹ä¿æŒè¿ç»­ï¼Œå•è¯é—´å…è®¸ç©ºæ ¼
        pieces = []
        i = 0
        while i < len(s):
            ch = s[i]

            # è·³è¿‡ç©ºæ ¼
            if ch.isspace():
                if pieces and not pieces[-1].endswith(r"\s*"):
                    pieces.append(r"\s*")
                i += 1
                continue

            # æ•°å­—åºåˆ—ï¼šä¿æŒè¿ç»­ï¼ˆåŒ…æ‹¬å°æ•°ç‚¹ã€é€—å·ï¼‰
            if ch.isdigit():
                num_str = ""
                while i < len(s) and (s[i].isdigit() or s[i] in ".,"):
                    num_str += s[i]
                    i += 1
                pieces.append(re.escape(num_str))
                continue

            # æ ‡ç‚¹ç¬¦å·ï¼šä¿æŒè¿ç»­
            if ch in ".,;:!?()[]{}\"'-/":
                pieces.append(re.escape(ch))
                i += 1
                continue

            # å­—æ¯åºåˆ—ï¼šå•è¯é—´å…è®¸ç©ºæ ¼
            if ch.isalpha():
                word = ""
                while i < len(s) and s[i].isalpha():
                    word += s[i]
                    i += 1
                pieces.append(re.escape(word))
                if i < len(s) and not s[i].isspace():
                    continue
                if i < len(s):
                    pieces.append(r"\s*")
                continue

            # å…¶ä»–å­—ç¬¦
            pieces.append(re.escape(ch))
            i += 1

        return "".join(pieces).strip()

    else:  # loose
        # å®½æ¾æ¨¡å¼ï¼šå­—ç¬¦é—´å…è®¸ç©ºæ ¼ï¼Œä½†æ•°å­—ä¿æŒè¿ç»­
        pieces = []
        i = 0
        while i < len(s):
            ch = s[i]

            if ch.isspace():
                i += 1
                continue

            # æ•°å­—åºåˆ—ä¿æŒè¿ç»­
            if ch.isdigit():
                num_str = ""
                while i < len(s) and (s[i].isdigit() or s[i] in ".,"):
                    num_str += s[i]
                    i += 1
                pieces.append(re.escape(num_str) + r"\s*")
                continue

            # å…¶ä»–å­—ç¬¦é—´å…è®¸ç©ºæ ¼
            pieces.append(re.escape(ch) + r"\s*")
            i += 1

        return "".join(pieces).strip()


def extract_anchor_with_target(context: str, target_value: str, window: int = 30) -> Optional[str]:
    """
    ä»ä¸Šä¸‹æ–‡ä¸­æå–åŒ…å«ç›®æ ‡æ•°å€¼çš„é”šç‚¹çŸ­è¯­ï¼ˆæ›´å¤§çª—å£ï¼‰
    """
    if not context or not target_value:
        return None

    context = _normalize_spaces(context)
    target_value = target_value.strip()

    # å…ˆå°è¯•ä¸¥æ ¼åŒ¹é…
    if target_value in context:
        idx = context.index(target_value)
        start = max(0, idx - window)
        end = min(len(context), idx + len(target_value) + window)
        anchor = context[start:end]
        # ä¿®å‰ªè¾¹ç•Œå•è¯
        anchor = re.sub(r"^\S*\s+", "", anchor)
        anchor = re.sub(r"\s+\S*$", "", anchor)
        return anchor.strip()

    # å†å°è¯•å¹³è¡¡æ¨¡å¼åŒ¹é…
    pattern = build_smart_pattern(target_value, mode="balanced")
    if not pattern:
        return None

    match = re.search(pattern, context, flags=re.IGNORECASE)
    if not match:
        return None

    start, end = match.span()
    prefix_start = max(0, start - window)
    suffix_end = min(len(context), end + window)

    anchor = context[prefix_start:suffix_end]
    anchor = re.sub(r"^\S*\s+", "", anchor)
    anchor = re.sub(r"\s+\S*$", "", anchor)

    return anchor.strip() if anchor.strip() else None


# =========================
# 4) Wordæ‰¹æ³¨åŠŸèƒ½ï¼ˆæ ¸å¿ƒä¿®æ”¹ - æ›¿æ¢+æ‰¹æ³¨ç‰ˆï¼‰
# =========================

class CommentManager:
    """Wordæ‰¹æ³¨ç®¡ç†å™¨ï¼ˆå®Œæ•´XMLæ”¯æŒï¼‰"""

    def __init__(self, doc: Document):
        self.doc = doc
        self._comment_id = 0
        self._comments_part = None
        self._init_comments_part()

    def _init_comments_part(self):
        """åˆå§‹åŒ–comments.xmléƒ¨åˆ†ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰"""
        try:
            package = self.doc.part.package

            # æŸ¥æ‰¾ç°æœ‰çš„comments part
            for rel in self.doc.part.rels.values():
                if "comments" in rel.target_ref:
                    self._comments_part = rel.target_part
                    break

            if self._comments_part:
                # è§£æç°æœ‰æ‰¹æ³¨ID
                root = self._comments_part.element
                for comment in root.findall(f".//{{{W_NS}}}comment"):
                    cid = comment.get(f"{{{W_NS}}}id")
                    if cid and cid.isdigit():
                        self._comment_id = max(self._comment_id, int(cid))
            else:
                # åˆ›å»ºæ–°çš„comments part
                self._create_comments_part()

            self._comment_id += 1

        except Exception as e:
            print(f"âš ï¸ åˆå§‹åŒ–æ‰¹æ³¨éƒ¨åˆ†å¤±è´¥: {e}")
            self._comment_id = 1

    def _create_comments_part(self):
        """åˆ›å»ºcomments.xmlæ–‡ä»¶å’Œå…³ç³»"""
        try:
            from docx.opc.part import XmlPart
            from docx.opc.packuri import PackURI

            # åˆ›å»ºcomments.xmlçš„XMLç»“æ„
            comments_xml = f'''<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<w:comments xmlns:w="{W_NS}" 
            xmlns:wpc="http://schemas.microsoft.com/office/word/2010/wordprocessingCanvas"
            xmlns:mc="http://schemas.openxmlformats.org/markup-compatibility/2006"
            xmlns:o="urn:schemas-microsoft-com:office:office"
            xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships"
            xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
            xmlns:v="urn:schemas-microsoft-com:vml"
            xmlns:wp14="http://schemas.microsoft.com/office/word/2010/wordprocessingDrawing"
            xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing"
            xmlns:w10="urn:schemas-microsoft-com:office:word"
            xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"
            xmlns:w15="http://schemas.microsoft.com/office/word/2012/wordml"
            xmlns:wpg="http://schemas.microsoft.com/office/word/2010/wordprocessingGroup"
            xmlns:wpi="http://schemas.microsoft.com/office/word/2010/wordprocessingInk"
            xmlns:wne="http://schemas.microsoft.com/office/word/2006/wordml"
            xmlns:wps="http://schemas.microsoft.com/office/word/2010/wordprocessingShape"
            mc:Ignorable="w14 w15 wp14">
</w:comments>'''

            # è§£æXML
            comments_element = etree.fromstring(comments_xml.encode('utf-8'))

            # åˆ›å»ºPartå¯¹è±¡
            partname = PackURI('/word/comments.xml')
            content_type = 'application/vnd.openxmlformats-officedocument.wordprocessingml.comments+xml'

            package = self.doc.part.package
            self._comments_part = XmlPart(partname, content_type, comments_element, package)

            # æ·»åŠ å…³ç³»
            self.doc.part.relate_to(self._comments_part, RT.COMMENTS)

            print("âœ“ æˆåŠŸåˆ›å»º comments.xml")

        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºcomments.xmlå¤±è´¥: {e}")
            print("   å°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼ˆæ‰¹æ³¨å¯èƒ½æ— æ³•æ˜¾ç¤ºï¼‰")

    def _get_next_comment_id(self) -> int:
        """è·å–ä¸‹ä¸€ä¸ªæ‰¹æ³¨ID"""
        cid = self._comment_id
        self._comment_id += 1
        return cid

    def create_initial_comment(self) -> bool:
        """
        åœ¨æ–‡æ¡£å¼€å¤´åˆ›å»ºä¸€ä¸ªåˆå§‹åŒ–æ‰¹æ³¨ï¼ˆç¡®ä¿comments.xmlç»“æ„å®Œæ•´ï¼‰
        """
        try:
            if not self.doc.paragraphs:
                print("âš ï¸ æ–‡æ¡£æ— æ®µè½ï¼Œæ— æ³•åˆ›å»ºåˆå§‹æ‰¹æ³¨")
                return False

            first_para = self.doc.paragraphs[0]
            if not first_para.runs:
                first_para.add_run(".")  # æ·»åŠ ä¸€ä¸ªå ä½ç¬¦

            first_run = first_para.runs[0]

            # åˆ›å»ºåˆå§‹æ‰¹æ³¨
            init_text = f"ã€ç¿»è¯‘æ ¡å¯¹ç³»ç»Ÿã€‘æ‰¹æ³¨åŠŸèƒ½å·²å¯ç”¨ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            success = self.add_comment_to_run(first_run, init_text, author="ç³»ç»Ÿ")

            if success:
                print("âœ“ å·²åˆ›å»ºåˆå§‹åŒ–æ‰¹æ³¨")

            return success

        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºåˆå§‹æ‰¹æ³¨å¤±è´¥: {e}")
            return False

    def add_comment_to_run(self, run, comment_text: str, author: str = "ç¿»è¯‘æ ¡å¯¹") -> bool:
        """
        ã€å®‰å…¨ç‰ˆã€‘æ‰¹æ³¨åªåŒ…ä¸€ä¸ªçº¯ runï¼Œä¸åš index åç§»å‡è®¾
        """
        try:
            if not self._comments_part:
                return False

            comment_id = self._get_next_comment_id()

            # commentRangeStart
            start = OxmlElement("w:commentRangeStart")
            start.set(qn("w:id"), str(comment_id))

            # commentRangeEnd
            end = OxmlElement("w:commentRangeEnd")
            end.set(qn("w:id"), str(comment_id))

            # commentReference
            ref_run = OxmlElement("w:r")
            ref = OxmlElement("w:commentReference")
            ref.set(qn("w:id"), str(comment_id))
            ref_run.append(ref)

            r_elem = run._element
            p_elem = r_elem.getparent()

            # âœ… å…³é”®ï¼šåªåœ¨ run å‰åæ’ï¼Œä¸ç®— index åç§»
            p_elem.insert(p_elem.index(r_elem), start)
            p_elem.insert(p_elem.index(r_elem) + 1, end)
            p_elem.insert(p_elem.index(r_elem) + 2, ref_run)

            self._add_comment_to_xml(comment_id, author, comment_text)
            return True

        except Exception as e:
            print(f"âš ï¸ æ·»åŠ æ‰¹æ³¨å¤±è´¥: {e}")
            return False

    def _add_comment_to_xml(self, comment_id: int, author: str, text: str):
        """å°†æ‰¹æ³¨å†…å®¹æ·»åŠ åˆ°comments.xml"""
        try:
            if not self._comments_part:
                return

            root = self._comments_part.element

            # åˆ›å»ºæ‰¹æ³¨å…ƒç´ 
            comment = OxmlElement('w:comment')
            comment.set(qn('w:id'), str(comment_id))
            comment.set(qn('w:author'), author)
            comment.set(qn('w:date'), datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'))
            comment.set(qn('w:initials'), author[:2] if len(author) >= 2 else author)

            # åˆ›å»ºæ®µè½
            p = OxmlElement('w:p')

            # æ·»åŠ æ®µè½å±æ€§ï¼ˆä½¿ç”¨æ‰¹æ³¨æ ·å¼ï¼‰
            pPr = OxmlElement('w:pPr')
            pStyle = OxmlElement('w:pStyle')
            pStyle.set(qn('w:val'), 'CommentText')
            pPr.append(pStyle)
            p.append(pPr)

            # æ·»åŠ æ–‡æœ¬run
            r = OxmlElement('w:r')

            # æ·»åŠ runå±æ€§
            rPr = OxmlElement('w:rPr')
            r.append(rPr)

            # æ·»åŠ æ–‡æœ¬
            t = OxmlElement('w:t')
            t.set(qn('xml:space'), 'preserve')
            t.text = text
            r.append(t)

            p.append(r)
            comment.append(p)

            # æ·»åŠ åˆ°commentsæ ¹å…ƒç´ 
            root.append(comment)

        except Exception as e:
            print(f"    âš ï¸ å†™å…¥æ‰¹æ³¨XMLå¤±è´¥: {e}")


def iter_all_paragraphs(doc: Document, include_headers_footers: bool = True):
    """éå†æ­£æ–‡æ®µè½ + è¡¨æ ¼å•å…ƒæ ¼æ®µè½ + (å¯é€‰)é¡µçœ‰é¡µè„šæ®µè½"""
    # æ­£æ–‡æ®µè½
    for p in doc.paragraphs:
        yield p

    # è¡¨æ ¼ï¼ˆé€’å½’ï¼‰
    def walk_tables(tables):
        for tbl in tables:
            for row in tbl.rows:
                for cell in row.cells:
                    for p in cell.paragraphs:
                        yield p
                    for p2 in walk_tables(cell.tables):
                        yield p2

    for p in walk_tables(doc.tables):
        yield p

    if include_headers_footers:
        for sec in doc.sections:
            for p in sec.header.paragraphs:
                yield p
            for p in sec.footer.paragraphs:
                yield p


def replace_and_add_comment_in_paragraph(
        paragraph,
        pattern: str,
        old_value: str,
        new_value: str,
        reason: str,
        comment_manager,
        anchor_pattern: str = None
) -> bool:
    """
    ã€100% åŸä½æ›¿æ¢ç‰ˆã€‘
    - åªæ›¿æ¢å‘½ä¸­å†…å®¹
    - ä¸æ”¹å˜å¥å­å…¶ä½™éƒ¨åˆ†
    - ä¸æ”¹å˜ run é¡ºåº
    - æ‰¹æ³¨åªä½œç”¨äº new_value
    """

    runs = list(paragraph.runs)
    if not runs:
        return False

    full_text = "".join(r.text or "" for r in runs)

    # é”šç‚¹æ ¡éªŒï¼ˆä¸é€šè¿‡ç›´æ¥è·³è¿‡ï¼‰
    if anchor_pattern:
        if not re.search(anchor_pattern, full_text, flags=re.IGNORECASE | re.DOTALL):
            return False

    m = re.search(pattern, full_text, flags=re.IGNORECASE | re.DOTALL)
    if not m:
        return False

    start, end = m.span()
    if start == end:
        return False

    # ===== 1. å»ºç«‹ run â†’ å…¨æ–‡ä½ç½®æ˜ å°„ =====
    spans = []
    cursor = 0
    for r in runs:
        text = r.text or ""
        spans.append((r, cursor, cursor + len(text)))
        cursor += len(text)

    # æ‰¾åˆ°å‘½ä¸­çš„ run åŒºé—´
    hit = [(r, s, e) for r, s, e in spans if start < e and end > s]
    if not hit:
        return False

    first_run, fs, fe = hit[0]
    last_run, ls, le = hit[-1]

    # ===== 2. è®¡ç®—å‰åæ–‡æœ¬ =====
    prefix = first_run.text[: max(0, start - fs)]
    suffix = last_run.text[max(0, end - ls):]

    # ===== 3. æ¸…ç©ºå‘½ä¸­ run =====
    for r, _, _ in hit:
        r.text = ""

    # ===== 4. é‡å»º runï¼ˆå…³é”®ï¼šåœ¨åŸä½ç½®æ’å…¥ï¼‰=====
    parent = first_run._element.getparent()
    insert_pos = parent.index(first_run._element)

    # prefix
    if prefix:
        prefix_run = paragraph.add_run(prefix)
        parent.remove(prefix_run._element)
        parent.insert(insert_pos, prefix_run._element)
        insert_pos += 1

    # new valueï¼ˆæ ¸å¿ƒæ›¿æ¢ï¼‰
    new_run = paragraph.add_run(new_value)
    parent.remove(new_run._element)
    parent.insert(insert_pos, new_run._element)
    insert_pos += 1

    # å¤åˆ¶æ ·å¼ï¼ˆä¿æŒåŸæ ¼å¼ï¼‰
    new_run.bold = first_run.bold
    new_run.italic = first_run.italic
    new_run.font.name = first_run.font.name
    new_run.font.size = first_run.font.size

    # suffix
    if suffix:
        suffix_run = paragraph.add_run(suffix)
        parent.remove(suffix_run._element)
        parent.insert(insert_pos, suffix_run._element)

    # ===== 5. æ‰¹æ³¨åªåŠ åœ¨ new_run ä¸Š =====
    comment_text = (
        f"ã€ä¿®æ”¹å»ºè®®ã€‘\n"
        f"åŸå€¼: {old_value}\n"
        f"æ–°å€¼: {new_value}\n"
        f"ä¿®æ”¹ç†ç”±: {reason}"
    )

    return comment_manager.add_comment_to_run(new_run, comment_text)


def replace_and_comment_in_docx(
        doc: Document,
        old_value: str,
        new_value: str,
        reason: str,
        comment_manager: CommentManager,
        context: str = "",
        anchor_text: str = ""
) -> Tuple[bool, str]:
    """
    å¤šç­–ç•¥æ‰§è¡Œæ›¿æ¢å¹¶æ·»åŠ æ‰¹æ³¨
    """
    old_value = (old_value or "").strip()
    new_value = (new_value or "").strip()
    # è¿™é‡Œçš„ reason å¯èƒ½æ˜¯å…ƒç»„ (æ¥è‡ª main.py)ï¼Œåšä¸€å±‚å¼ºè½¬å¤„ç†
    if isinstance(reason, (list, tuple)):
        reason = " ".join([str(i) for i in reason if i]).strip()
    reason = reason or "æ•°å€¼/æœ¯è¯­ä¸ä¸€è‡´"

    if not old_value or not new_value:
        return False, "æ•°æ®ç¼ºå¤±"

    # ===== ç­–ç•¥1ï¼šæ›¿æ¢é”šç‚¹ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰=====
    if anchor_text:
        anchor_pattern = build_smart_pattern(anchor_text, mode="balanced")
        target_pattern = build_smart_pattern(old_value, mode="strict")
        if anchor_pattern and target_pattern:
            for p in iter_all_paragraphs(doc):
                if replace_and_add_comment_in_paragraph(
                        p, target_pattern, old_value, new_value, reason, comment_manager, anchor_pattern
                ):
                    return True, f"é”šç‚¹åŒ¹é…"

    # ===== ç­–ç•¥2ï¼šä¸Šä¸‹æ–‡é”šç‚¹ =====
    if context:
        context_anchor = extract_anchor_with_target(context, old_value, window=40)
        if context_anchor:
            anchor_pattern = build_smart_pattern(context_anchor, mode="balanced")
            target_pattern = build_smart_pattern(old_value, mode="strict")
            for p in iter_all_paragraphs(doc):
                if replace_and_add_comment_in_paragraph(
                        p, target_pattern, old_value, new_value, reason, comment_manager, anchor_pattern
                ):
                    return True, f"ä¸Šä¸‹æ–‡åŒ¹é…"

    # ===== ç­–ç•¥3ï¼šä¸¥æ ¼æ¨¡å¼å…¨å±€åŒ¹é… =====
    strict_pattern = build_smart_pattern(old_value, mode="strict")
    for p in iter_all_paragraphs(doc):
        if replace_and_add_comment_in_paragraph(
                p, strict_pattern, old_value, new_value, reason, comment_manager
        ):
            return True, "ä¸¥æ ¼æ¨¡å¼"

    # ===== ç­–ç•¥4ï¼šå¹³è¡¡æ¨¡å¼ =====
    balanced_pattern = build_smart_pattern(old_value, mode="balanced")
    for p in iter_all_paragraphs(doc):
        if replace_and_add_comment_in_paragraph(
                p, balanced_pattern, old_value, new_value, reason, comment_manager
        ):
            return True, "å¹³è¡¡æ¨¡å¼"

    return False, "æœªæ‰¾åˆ°åŒ¹é…é¡¹"


# =========================
# 5) æ–‡ä»¶å¤‡ä»½åŠŸèƒ½
# =========================

def ensure_backup_copy(src_docx_path: str) -> str:
    """
    æŠŠè¯‘æ–‡æ–‡ä»¶å¤åˆ¶åˆ° backup/ ä¸‹ï¼Œç”Ÿæˆä¸é‡å¤çš„æ–°å‰¯æœ¬æ–‡ä»¶å
    """
    src_docx_path = os.path.abspath(src_docx_path)
    if not os.path.exists(src_docx_path):
        raise FileNotFoundError(f"è¯‘æ–‡æ–‡ä»¶ä¸å­˜åœ¨: {src_docx_path}")

    base_dir = os.path.dirname(src_docx_path)
    backup_dir = os.path.join(base_dir, BACKUP_DIR_NAME)
    os.makedirs(backup_dir, exist_ok=True)

    src_name = os.path.basename(src_docx_path)
    stem, ext = os.path.splitext(src_name)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    dst_name = f"{stem}_corrected_{timestamp}{ext}"
    dst_path = os.path.join(backup_dir, dst_name)

    shutil.copy2(src_docx_path, dst_path)
    return dst_path


# =========================
# 6) ä¸»æµç¨‹
# =========================
if __name__ == "__main__":
    # ç¤ºä¾‹æ–‡ä»¶è·¯å¾„
    original_path = r"C:\Users\Administrator\Desktop\project\llm\æ–‡æœ¬å¯¹æ¯”ç»“æœ\åŠ ç²—æ–œä½“æµ‹è¯•\åŸæ–‡-åŠ ç²—æ–œä½“.docx"  # è¯·æ›¿æ¢ä¸ºåŸæ–‡æ–‡ä»¶è·¯å¾„
    translated_path = r"C:\Users\Administrator\Desktop\project\llm\æ–‡æœ¬å¯¹æ¯”ç»“æœ\åŠ ç²—æ–œä½“æµ‹è¯•\è¯‘æ–‡-åŠ ç²—æ–œä½“.docx"  # è¯·æ›¿æ¢ä¸ºè¯‘æ–‡æ–‡ä»¶è·¯å¾„
    #å¤„ç†é¡µçœ‰
    original_header_text=extract_headers(original_path)
    translated_header_text=extract_headers(translated_path)
    #å¤„ç†é¡µè„š
    original_footer_text = extract_footers(original_path)
    translated_footer_text = extract_footers(translated_path)
    #å¤„ç†æ­£æ–‡(å«è„šæ³¨/è¡¨æ ¼/è‡ªåŠ¨ç¼–å·)
    original_body_text=extract_body_text(original_path)
    translated_body_text=extract_body_text(translated_path)
    print("======é¡µçœ‰===========")
    print(original_header_text)
    print(translated_header_text)
    print("======é¡µè„š===========")
    print(original_footer_text)
    print(translated_footer_text)
    print("======æ­£æ–‡===========")
    print(original_body_text)
    print(translated_body_text)

    #å®ä¾‹åŒ–å¯¹è±¡å¹¶è¿›è¡Œå¯¹æ¯”
    matcher = Match()
    #æ­£æ–‡å¯¹æ¯”
    print("======æ­£åœ¨æ£€æŸ¥æ­£æ–‡===========")
    if original_body_text and translated_body_text:
        # ä¸¤ä¸ªå€¼éƒ½ä¸ä¸ºç©ºï¼Œæ­£å¸¸æ‰§è¡Œæ¯”è¾ƒ
        body_result = matcher.compare_texts(original_body_text, translated_body_text)
    else:
        # ä»»æ„ä¸€ä¸ªä¸ºç©ºï¼Œç”Ÿæˆç©ºç»“æœ
        body_result = {}  # æˆ–è€… body_result = []ï¼Œæ ¹æ®ä½ çš„ write_json_with_timestamp å‡½æ•°æœŸæœ›çš„æ ¼å¼
        print("åŸæ–‡æˆ–è¯‘æ–‡ä¸ºç©ºï¼Œæ£€æŸ¥ç»“æœä¸ºç©º")

    body_result_name, body_result_path = write_json_with_timestamp(
        body_result,
        r"C:\Users\Administrator\Desktop\project\llm\llm_project\zhengwen\output_json"
    )
    # body_result = matcher.compare_texts(original_body_text, translated_body_text)
    # body_result_name, body_result_path = write_json_with_timestamp(body_result,r"C:\Users\Administrator\Desktop\project\llm\llm_project\zhengwen\output_json")
    # #é¡µçœ‰å¯¹æ¯”
    print("======æ­£åœ¨æ£€æŸ¥é¡µçœ‰===========")
    if original_header_text and translated_header_text:
        # ä¸¤ä¸ªå€¼éƒ½ä¸ä¸ºç©ºï¼Œæ­£å¸¸æ‰§è¡Œæ¯”è¾ƒ
        header_result = matcher.compare_texts(original_header_text, translated_header_text)
    else:
        # ä»»æ„ä¸€ä¸ªä¸ºç©ºï¼Œç”Ÿæˆç©ºç»“æœ
        header_result = {}
        print("åŸæ–‡æˆ–è¯‘æ–‡ä¸ºç©ºï¼Œæ£€æŸ¥ç»“æœä¸ºç©º")

    header_result_name, header_result_path = write_json_with_timestamp(
        header_result,
        r"C:\Users\Administrator\Desktop\project\llm\llm_project\yemei\output_json"
    )
    # header_result = matcher.compare_texts(original_header_text, translated_header_text)
    # header_result_name, header_result_path = write_json_with_timestamp(header_result, r"C:\Users\Administrator\Desktop\project\llm\llm_project\yemei\output_json")
    # #é¡µè„šå¯¹æ¯”
    print("======æ­£åœ¨æ£€æŸ¥é¡µè„š===========")
    if original_footer_text and translated_footer_text:
        # ä¸¤ä¸ªå€¼éƒ½ä¸ä¸ºç©ºï¼Œæ­£å¸¸æ‰§è¡Œæ¯”è¾ƒ
        footer_result = matcher.compare_texts(original_footer_text, translated_footer_text)
    else:
        # ä»»æ„ä¸€ä¸ªä¸ºç©ºï¼Œç”Ÿæˆç©ºç»“æœ
        footer_result = {}
        print("åŸæ–‡æˆ–è¯‘æ–‡ä¸ºç©ºï¼Œæ£€æŸ¥ç»“æœä¸ºç©º")

    footer_result_name, footer_result_path = write_json_with_timestamp(
        footer_result,
        r"C:\Users\Administrator\Desktop\project\llm\llm_project\yejiao\output_json"
    )
    # footer_result = matcher.compare_texts(original_footer_text, translated_footer_text)
    # footer_result_name, footer_result_path = write_json_with_timestamp(footer_result, r"C:\Users\Administrator\Desktop\project\llm\llm_project\yejiao\output_json")

    print("================================")
    # if not os.path.exists(error_docx_path):
    #     raise FileNotFoundError(f"é”™è¯¯æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {error_docx_path}")
    # body_result_path=r"C:\Users\Administrator\Desktop\project\llm\æ–‡æœ¬å¯¹æ¯”ç»“æœ\åŠ ç²—æ–œä½“æµ‹è¯•\AIæ£€æŸ¥ç»“æœ.json"
    # header_result_path=r"C:\Users\Administrator\Desktop\project\llm\llm_project\yemei\output_json\æ–‡æœ¬å¯¹æ¯”ç»“æœ_20260208_144950.json"
    # footer_result_path=r"C:\Users\Administrator\Desktop\project\llm\llm_project\yejiao\output_json\æ–‡æœ¬å¯¹æ¯”ç»“æœ_20260208_145004.json"

    # 1) å¤åˆ¶è¯‘æ–‡åˆ° backup/
    backup_copy_path = ensure_backup_copy(translated_path)
    print(f"âœ… å·²å¤åˆ¶è¯‘æ–‡å‰¯æœ¬åˆ°: {backup_copy_path}")

    # 2) è¯»å–é”™è¯¯æŠ¥å‘Šå¹¶è§£æ
    print("\næ­£åœ¨æå–è§£ææ­£æ–‡é”™è¯¯æŠ¥å‘Š...")
    body_errors = load_json_file(body_result_path)
    print("æ­£æ–‡é”™è¯¯æŠ¥å‘Š",body_errors)
    for err in body_errors:
        print(err)
    print("æ­£æ–‡é”™è¯¯è§£æä¸ªæ•°ï¼š",len(body_errors))

    print("\næ­£åœ¨æå–è§£æé¡µçœ‰é”™è¯¯æŠ¥å‘Š...")
    header_errors = load_json_file(header_result_path)
    print("é¡µçœ‰é”™è¯¯æŠ¥å‘Š", header_errors)
    print("é¡µçœ‰é”™è¯¯è§£æä¸ªæ•°ï¼š", len(header_errors))

    print("\næ­£åœ¨æå–è§£æé¡µè„šé”™è¯¯æŠ¥å‘Š...")
    footer_errors = load_json_file(footer_result_path)
    print("é¡µè„šé”™è¯¯æŠ¥å‘Š", footer_errors)
    print("é¡µè„šé”™è¯¯è§£æä¸ªæ•°ï¼š", len(footer_errors))


    # 3) æ‰“å¼€å‰¯æœ¬ docx
    print("æ­£åœ¨åŠ è½½æ–‡æ¡£...")
    doc = Document(backup_copy_path)

    # 4) åˆ›å»ºæ‰¹æ³¨ç®¡ç†å™¨å¹¶åˆå§‹åŒ–
    print("æ­£åœ¨åˆå§‹åŒ–æ‰¹æ³¨ç³»ç»Ÿ...")
    comment_manager = CommentManager(doc)

    # ã€å…³é”®ã€‘åˆ›å»ºåˆå§‹æ‰¹æ³¨ä»¥ç¡®ä¿ comments.xml ç»“æ„å®Œæ•´
    if comment_manager.create_initial_comment():
        print("âœ“ æ‰¹æ³¨ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ\n")
    else:
        print("âš ï¸ æ‰¹æ³¨ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œä½†å°†ç»§ç»­å°è¯•å¤„ç†\n")

    # 5) é€æ¡æ‰§è¡Œæ›¿æ¢å¹¶æ·»åŠ æ‰¹æ³¨
    print("==================== å¼€å§‹å¤„ç†æ­£æ–‡é”™è¯¯ ====================\n")
    body_success_count = 0
    body_fail_count = 0

    for idx, e in enumerate(body_errors, 1):
        err_id = e.get("é”™è¯¯ç¼–å·", "?")
        err_type = e.get("é”™è¯¯ç±»å‹", "")
        old = (e.get("è¯‘æ–‡æ•°å€¼") or "").strip()
        new = (e.get("è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼") or "").strip()
        reason = (e.get("ä¿®æ”¹ç†ç”±"),"")
        trans_context = e.get("è¯‘æ–‡ä¸Šä¸‹æ–‡", "") or ""
        anchor = e.get("æ›¿æ¢é”šç‚¹", "") or ""

        if not old or not new:
            print(f"[{idx}/{len(body_errors)}] [è·³è¿‡] é”™è¯¯ #{err_id}: old/new ç¼ºå¤±")
            body_fail_count += 1
            continue

        # æ‰§è¡Œæ›¿æ¢å¹¶æ·»åŠ æ‰¹æ³¨(æ­£æ–‡)
        ok, strategy = replace_and_comment_in_docx(
            doc, old, new, reason,comment_manager,
            context=trans_context,
            anchor_text=anchor
        )

        if ok:
            print(f"[{idx}/{len(body_errors)}] [âœ“æˆåŠŸ] é”™è¯¯ #{err_id} ({err_type})")
            print(f"    ç­–ç•¥: {strategy}")
            print(f"    ä¿®æ”¹ç†ç”±: {reason}")
            print(f"    æ“ä½œ: '{old}' â†’ '{new}' (å·²æ›¿æ¢å¹¶æ·»åŠ æ‰¹æ³¨)")
            if anchor:
                print(f"    é”šç‚¹: {anchor}...")
            elif trans_context:
                print(f"    ä¸Šä¸‹æ–‡: {trans_context}...")
            body_success_count += 1
        else:
            print(f"[{idx}/{len(body_errors)}] [âœ—å¤±è´¥] é”™è¯¯ #{err_id} ({err_type})")
            print(f"    æœªæ‰¾åˆ°åŒ¹é…: '{old}'")
            if anchor:
                print(f"    é”šç‚¹: {anchor}...")
            print(f"    ä¸Šä¸‹æ–‡: {trans_context if trans_context else 'æ— '}...")
            body_fail_count += 1
        print()

    print(f"\n==================== æ­£æ–‡å¤„ç†å®Œæˆ ====================")
    print(f"æˆåŠŸ: {body_success_count} | å¤±è´¥: {body_fail_count} | æ€»è®¡: {len(body_errors)}")
    if len(body_errors) > 0:
        print(f"æˆåŠŸç‡: {body_success_count / len(body_errors) * 100:.1f}%")
    print(f"\nâœ… å·²ä¿å­˜åˆ°: {backup_copy_path}")
    print("âš ï¸ åŸå§‹è¯‘æ–‡æ–‡ä»¶æœªè¢«ä¿®æ”¹")

    print("==================== å¼€å§‹å¤„ç†é¡µçœ‰é”™è¯¯ ====================\n")
    header_success_count = 0
    header_fail_count = 0

    for idx, e in enumerate(header_errors, 1):
        err_id = e.get("é”™è¯¯ç¼–å·", "?")
        err_type = e.get("é”™è¯¯ç±»å‹", "")
        old = (e.get("è¯‘æ–‡æ•°å€¼") or "").strip()
        new = (e.get("è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼") or "").strip()
        reason = (e.get("ä¿®æ”¹ç†ç”±"), "")
        trans_context = e.get("è¯‘æ–‡ä¸Šä¸‹æ–‡", "") or ""
        anchor = e.get("æ›¿æ¢é”šç‚¹", "") or ""

        if not old or not new:
            print(f"[{idx}/{len(header_errors)}] [è·³è¿‡] é”™è¯¯ #{err_id}: old/new ç¼ºå¤±")
            header_fail_count += 1
            continue

        # æ‰§è¡Œæ›¿æ¢å¹¶æ·»åŠ æ‰¹æ³¨(æ­£æ–‡)
        ok, strategy = replace_and_comment_in_docx(
            doc, old, new, reason,comment_manager,
            context=trans_context,
            anchor_text=anchor
        )

        if ok:
            print(f"[{idx}/{len(header_errors)}] [âœ“æˆåŠŸ] é”™è¯¯ #{err_id} ({err_type})")
            print(f"    ä¿®æ”¹ç†ç”±: {reason}")
            print(f"    ç­–ç•¥: {strategy}")
            print(f"    æ“ä½œ: '{old}' â†’ '{new}' (å·²æ›¿æ¢å¹¶æ·»åŠ æ‰¹æ³¨)")
            if anchor:
                print(f"    é”šç‚¹: {anchor}...")
            elif trans_context:
                print(f"    ä¸Šä¸‹æ–‡: {trans_context}...")
            header_success_count += 1
        else:
            print(f"[{idx}/{len(header_errors)}] [âœ—å¤±è´¥] é”™è¯¯ #{err_id} ({err_type})")
            print(f"    æœªæ‰¾åˆ°åŒ¹é…: '{old}'")
            if anchor:
                print(f"    é”šç‚¹: {anchor}...")
            print(f"    ä¸Šä¸‹æ–‡: {trans_context if trans_context else 'æ— '}...")
            header_fail_count += 1
        print()
    print(f"\n==================== é¡µçœ‰å¤„ç†å®Œæˆ ====================")
    print(f"æˆåŠŸ: {header_success_count} | å¤±è´¥: {header_fail_count} | æ€»è®¡: {len(header_errors)}")
    if len(header_errors) > 0:
        print(f"æˆåŠŸç‡: {header_success_count / len(header_errors) * 100:.1f}%")
    print(f"\nâœ… å·²ä¿å­˜åˆ°: {backup_copy_path}")
    print("âš ï¸ åŸå§‹è¯‘æ–‡æ–‡ä»¶æœªè¢«ä¿®æ”¹")

    print("==================== å¼€å§‹å¤„ç†é¡µè„šé”™è¯¯ ====================\n")
    footer_success_count = 0
    footer_fail_count = 0

    for idx, e in enumerate(footer_errors, 1):
        err_id = e.get("é”™è¯¯ç¼–å·", "?")
        err_type = e.get("é”™è¯¯ç±»å‹", "")
        old = (e.get("è¯‘æ–‡æ•°å€¼") or "").strip()
        new = (e.get("è¯‘æ–‡ä¿®æ”¹å»ºè®®å€¼") or "").strip()
        reason = (e.get("ä¿®æ”¹ç†ç”±"), "")
        trans_context = e.get("è¯‘æ–‡ä¸Šä¸‹æ–‡", "") or ""
        anchor = e.get("æ›¿æ¢é”šç‚¹", "") or ""

        if not old or not new:
            print(f"[{idx}/{len(footer_errors)}] [è·³è¿‡] é”™è¯¯ #{err_id}: old/new ç¼ºå¤±")
            footer_fail_count += 1
            continue

        # æ‰§è¡Œæ›¿æ¢å¹¶æ·»åŠ æ‰¹æ³¨(æ­£æ–‡)
        ok, strategy = replace_and_comment_in_docx(
            doc, old, new, reason,comment_manager,
            context=trans_context,
            anchor_text=anchor
        )

        if ok:
            print(f"[{idx}/{len(footer_errors)}] [âœ“æˆåŠŸ] é”™è¯¯ #{err_id} ({err_type})")
            print(f"    ä¿®æ”¹ç†ç”±: {reason}")
            print(f"    ç­–ç•¥: {strategy}")
            print(f"    æ“ä½œ: '{old}' â†’ '{new}' (å·²æ›¿æ¢å¹¶æ·»åŠ æ‰¹æ³¨)")
            if anchor:
                print(f"    é”šç‚¹: {anchor}...")
            elif trans_context:
                print(f"    ä¸Šä¸‹æ–‡: {trans_context}...")
            footer_success_count += 1
        else:
            print(f"[{idx}/{len(footer_errors)}] [âœ—å¤±è´¥] é”™è¯¯ #{err_id} ({err_type})")
            print(f"    æœªæ‰¾åˆ°åŒ¹é…: '{old}'")
            if anchor:
                print(f"    é”šç‚¹: {anchor}...")
            print(f"    ä¸Šä¸‹æ–‡: {trans_context if trans_context else 'æ— '}...")
            footer_fail_count += 1
        print()
    print(f"\n==================== é¡µè„šå¤„ç†å®Œæˆ ====================")
    print(f"æˆåŠŸ: {footer_success_count} | å¤±è´¥: {footer_fail_count} | æ€»è®¡: {len(footer_errors)}")
    if len(footer_errors) > 0:
        print(f"æˆåŠŸç‡: {footer_success_count / len(footer_errors) * 100:.1f}%")
    print(f"\nâœ… å·²ä¿å­˜åˆ°: {backup_copy_path}")
    print("âš ï¸ åŸå§‹è¯‘æ–‡æ–‡ä»¶æœªè¢«ä¿®æ”¹")

    # 6) ä¿å­˜æ–‡æ¡£
    print("æ­£åœ¨ä¿å­˜æ–‡æ¡£...")
    doc.save(backup_copy_path)

    print(f"\n==================== æ–‡ç« å¤„ç†å®Œæˆ ====================")
    # æ±‡æ€»è®¡ç®—
    total_count = len(body_errors) + len(header_errors) + len(footer_errors)
    total_success = body_success_count + header_success_count + footer_success_count
    total_fail = body_fail_count + header_fail_count + footer_fail_count
    # è®¡ç®—è¢«è·³è¿‡çš„æ•°é‡ï¼ˆå› ä¸ºæ•°å€¼ç¼ºå¤±å¯¼è‡´æ— æ³•å¤„ç†çš„é¡¹ï¼‰
    total_skipped = total_count - total_success - total_fail

    # æœ‰æ•ˆåˆ†æ¯ = æ€»è®¡ - è·³è¿‡
    effective_total = total_success + total_fail

    print(f"\n" + "=" * 50)
    print(f"ğŸ“Š æœ€ç»ˆå¤„ç†æŠ¥å‘Š (å·²ä¿å­˜è‡³å‰¯æœ¬)")
    print(f"æ€»è®¡æ•°æ®: {total_count} æ¡")
    print(f"çŠ¶æ€åˆ†å¸ƒ: [âœ“æˆåŠŸ: {total_success}] [âœ—å¤±è´¥: {total_fail}] [â©è·³è¿‡: {total_skipped}]")

    if effective_total > 0:
        success_rate = (total_success / effective_total) * 100
        print(f"æœ‰æ•ˆæˆåŠŸç‡: {success_rate:.1f}%  (è®¡ç®—å…¬å¼: æˆåŠŸ / (æˆåŠŸ+å¤±è´¥))")
    else:
        print("æœ‰æ•ˆæˆåŠŸç‡: 0% (æ— æœ‰æ•ˆå¯å¤„ç†æ•°æ®)")

    print(f"ä¿å­˜è·¯å¾„: {backup_copy_path}")
    print("=" * 50)
    print(f"\nâœ… å·²ä¿å­˜åˆ°: {backup_copy_path}")
    print("âš ï¸ åŸå§‹è¯‘æ–‡æ–‡ä»¶æœªè¢«ä¿®æ”¹")


