import os
import re
import shutil
import threading
import queue
from datetime import datetime
import pandas as pd
from lxml import etree
from docx import Document
from docx.table import Table
from docx.text.paragraph import Paragraph
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE
from openai import OpenAI
import tkinter as tk
from tkinter import ttk, filedialog, scrolledtext, messagebox
import websockets
import asyncio
import time
import json
import rsa
import base64

# ==========================================
# === âš™ï¸ å…¨å±€é…ç½® ===
# ==========================================
ROW_BUCKET = 20_000
API_KEY = "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"  # ğŸ”´ è¯·åœ¨æ­¤å¤„å¡«å…¥æ‚¨çš„ Key
BASE_URL = "https://openrouter.ai/api/v1"

# Luzhishen WebSocket é…ç½®
LUZHISHEN_API_KEY = "zd-cce061a739a64f299f5a0bb7fea39075dbdd268414d8416ca"  # ğŸ”´ è¯·åœ¨æ­¤å¤„å¡«å…¥æ‚¨çš„ Luzhishen Key
LUZHISHEN_WS_BASE_URL = "wss://api.luzhishen.chat:8001/ws_chat/"
LUZHISHEN_PUBKEY_N = 0x00d6fe5e8ce5cb85f19a0c38e0dd3fbf16a4bb74b3c7bcd55c4fd8c87e0d0a5dbee83e4f8a18b5a0f6c9ab0876e9a4d8c9d5f3c7e4c9b6e8f5e9c1b3e6f8d3c5e2a1e4c8f5e9b3e6d1c4e7f9b2e5d1c3e8f6a4e7b1c9e5f3d2a6e8c4f5e9b1e6d3c7e4f8a2e5c9d1b6e3f7c8e4a5f9d2e6b1c3e7f5a8e4d1c6b9e2f3c7e5a8d1e9b4c6f2e7d3a5c8e1b9f4e6d2c5a7e9b1f3e8c4d6a2e5b7c9f1e3d8a4e6c2f5b9e7d1c3a8e5f2b6d4c9e1a7f3e8b5d2c6e4a9f1b7e3c5d8f6a2e4c1b9e7d3f5a6c8e2b4f9d1e7c3a5e6b8f2d4c9a1e7b3f6d5c8e2a4f1b9e7c6d3a8e5f2b1c4e9d7a6f3e8b5c2d1e4f9a7b6c3e8d5f1a2e6b9c7d4f3e1a8b5c6d9f2e4a7c1b3e8f5d6a9c2e7b4f1d3c8e5a6f9b2d7c4e1a3f8b6e9c5d2a7f4e1b8c6d3a9e5f2b7c1d4e8a6f3b9c5d7e2a4f1b6c8e3d9a5f7b2

# æ¯å—â‰¤25000å­—ï¼š2ä»½=50k, 3ä»½=75k, 4ä»½=100k, 5ä»½=125k, 6ä»½=150k, 7ä»½=175k, 8ä»½=200k
THRESHOLD_2_PARTS = 25000
THRESHOLD_3_PARTS = 50000
THRESHOLD_4_PARTS = 75000
THRESHOLD_5_PARTS = 100000
THRESHOLD_6_PARTS = 125000
THRESHOLD_7_PARTS = 150000
THRESHOLD_8_PARTS = 175000
BUFFER_CHARS = 2000
OUTPUT_DIR = "Result_Output"

# ==========================================
# === ğŸ¤– å¯ç”¨æ¨¡å‹é…ç½® ===
# ==========================================
# Luzhishen æ¨¡å‹ (Gemini) - é€šè¿‡ WebSocket
LUZHISHEN_MODELS = {
    "Gemini 2.5 Flash": {
        "id": "gemini-2.5-flash",
        "description": "Gemini 2.5 Flash - å¿«é€Ÿç‰ˆæœ¬",
        "max_output": 65536,
        "provider": "luzhishen"
    },
    "Gemini 2.5 Pro": {
        "id": "gemini-2.5-pro",
        "description": "Gemini 2.5 Pro - é«˜çº§ç‰ˆæœ¬",
        "max_output": 65536,
        "provider": "luzhishen"
    },
    "Gemini 3 Pro Preview": {
        "id": "gemini-3-pro-preview",
        "description": "Gemini 3 Pro Preview - æœ€æ–°é¢„è§ˆç‰ˆ",
        "max_output": 65536,
        "provider": "luzhishen"
    },
}

# OpenRouter æ¨¡å‹
OPENROUTER_MODELS = {
    "Google Gemini 2.5 Flash": {
        "id": "google/gemini-2.5-flash",
        "description": "å»ºè®®å…ˆæ£€æŸ¥æ–‡ç« æ˜¯å¦æœ‰ç›®å½•ï¼Œå…ˆå°†ç›®å½•åˆ é™¤å†å¤„ç†",
        "max_output": 65536,
        "provider": "openrouter"
    },
    "Google Gemini 2.5 Pro": {
        "id": "google/gemini-2.5-pro",
        "description": "PPTæ¨è-å¢å¼ºï¼Œé€Ÿåº¦ç¨æ…¢ï¼Œ100ä¸‡ä¸Šä¸‹æ–‡ï¼Œ65Kè¾“å‡º",
        "max_output": 65536,
        "provider": "openrouter"
    },
    "Google: Gemini 3 Pro Preview": {
        "id": "google/gemini-3-pro-preview",
        "description": "æœ€å¼ºæ¨ç†ï¼Œ100ä¸‡ä¸Šä¸‹æ–‡ï¼Œ65Kè¾“å‡º",
        "max_output": 65536,
        "provider": "openrouter"
    },
}

# å½“å‰å¯ç”¨æ¨¡å‹ï¼ˆæ ¹æ®æä¾›å•†åˆ‡æ¢ï¼‰
AVAILABLE_MODELS = OPENROUTER_MODELS.copy()
DEFAULT_MODEL = "Google Gemini 2.5 Flash"
DEFAULT_PROVIDER = "openrouter"  # è·¯æ™ºæ·±å·²å±è”½ï¼Œé»˜è®¤ä½¿ç”¨ OpenRouter


def clean_luzhishen_response(response_text):
    """æ¸…ç† Luzhishen è¿”å›çš„ç”¨é‡ç»Ÿè®¡ä¿¡æ¯"""
    if not response_text:
        return response_text

    # åŒ¹é…æœ«å°¾çš„ JSON ç”¨é‡ç»Ÿè®¡ä¿¡æ¯
    # æ ¼å¼å¦‚: {'prompt_created': '...', 'prompt_tokens': ..., 'total_money': ...}
    import re
    # åŒ¹é…ç±»ä¼¼ {'prompt_created': ... } çš„ JSON æ ¼å¼
    pattern = r"\{['\"]prompt_created['\"]:\s*['\"][^'\"]+['\"],\s*['\"]prompt_tokens['\"]:\s*\d+.*?\}"
    cleaned = re.sub(pattern, '', response_text)

    # ä¹Ÿå¤„ç†å¯èƒ½é™„åŠ åœ¨æœ€åä¸€è¡Œå†…å®¹åé¢çš„æƒ…å†µ
    # æŸ¥æ‰¾æœ€åä¸€ä¸ªåŒ…å« ||| çš„è¡Œä¹‹åçš„ç”¨é‡ä¿¡æ¯
    lines = cleaned.strip().split('\n')
    result_lines = []
    for line in lines:
        # å¦‚æœè¡ŒåŒ…å« prompt_created ç­‰ç”¨é‡å…³é”®å­—ï¼Œè·³è¿‡
        if "'prompt_created'" in line or '"prompt_created"' in line:
            # å°è¯•åªä¿ç•™ ||| ä¹‹å‰çš„éƒ¨åˆ†
            if '|||' in line:
                idx = line.rfind('|||')
                # æ£€æŸ¥ ||| ä¹‹åæ˜¯å¦æœ‰ç”¨é‡ä¿¡æ¯
                after_sep = line[idx + 3:]
                if "'prompt_created'" in after_sep or '"prompt_created"' in after_sep:
                    # æ‰¾åˆ°ç”¨é‡ä¿¡æ¯çš„å¼€å§‹ä½ç½®
                    json_start = after_sep.find("{")
                    if json_start != -1:
                        line = line[:idx + 3] + after_sep[:json_start].strip()
            else:
                continue  # è·³è¿‡çº¯ç”¨é‡ä¿¡æ¯è¡Œ
        result_lines.append(line)

    return '\n'.join(result_lines).strip()


# Luzhishen RSA å…¬é’¥ï¼ˆä» PEM æ–‡ä»¶æˆ–å†…ç½®ï¼‰
LUZHISHEN_RSA_N = 0x00d4a0c64eb8c5d7b2d53d39a4b1e6f7c8a9d0e1f2a3b4c5d6e7f8091a2b3c4d5e6f708192a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1
LUZHISHEN_RSA_E = 65537


def get_luzhishen_pubkey():
    """è·å– Luzhishen RSA å…¬é’¥"""
    try:
        # å°è¯•ä»æ–‡ä»¶åŠ è½½
        import os
        pubkey_paths = [
            os.path.join(os.path.dirname(__file__), 'utils', 'keys', 'public.pem'),
            r'E:\PythonTools\utils\keys\public.pem',
            r'E:\Luzhishen\utils\keys\public.pem',
        ]
        for path in pubkey_paths:
            if os.path.exists(path):
                with open(path, 'rb') as f:
                    return rsa.PublicKey.load_pkcs1(f.read())

        # å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œå°è¯•ä»å†…ç½®çš„ N å’Œ E æ„é€ 
        return rsa.PublicKey(LUZHISHEN_RSA_N, LUZHISHEN_RSA_E)
    except Exception as e:
        log_manager.log_exception(f"æ— æ³•åŠ è½½ Luzhishen å…¬é’¥: {e}")
        return None


def generate_luzhishen_api_key(api_key, pubkey):
    """ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„åŠ å¯† API Key"""
    timestamp = int(time.time())
    message = f"{api_key}|{timestamp}".encode('utf8')
    encrypted_message = rsa.encrypt(message, pubkey)
    api_key_base64_urlsafe = base64.urlsafe_b64encode(encrypted_message).decode('utf-8')
    return api_key_base64_urlsafe


CHAPTER_PATTERNS = [
    r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+[ç« èŠ‚ç¯‡éƒ¨]', r'^Chapter\s*\d+', r'^CHAPTER\s*\d+',
    r'^\d+[\.ã€]\s*\S+', r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€.]\s*\S+',
    r'^Part\s*\d+', r'^PART\s*\d+', r'^Section\s*\d+',
]

ORIG_PATTERNS = ['åŸæ–‡', 'ä¸­æ–‡', 'source', 'original', 'chinese', 'cn', 'æºæ–‡']
TRANS_PATTERNS = ['è¯‘æ–‡', 'è‹±æ–‡', 'target', 'translation', 'english', 'en', 'ç¿»è¯‘']

# ==========================================
# === ğŸŒ å¤šè¯­è¨€é…ç½® ===
# ==========================================
SUPPORTED_LANGUAGES = {
    "ä¸­æ–‡": {
        "code": "zh",
        "english_name": "Chinese",
        "char_pattern": r'[\u4e00-\u9fa5]',  # ä¸­æ–‡å­—ç¬¦
        "word_based": False,  # æŒ‰å­—ç¬¦è®¡æ•°
        "description": "ä¸­æ–‡ï¼ˆç®€ä½“/ç¹ä½“ï¼‰"
    },
    "è‹±è¯­": {
        "code": "en",
        "english_name": "English",
        "char_pattern": r'\b[a-zA-Z]+\b',  # è‹±æ–‡å•è¯
        "word_based": True,  # æŒ‰è¯è®¡æ•°
        "description": "English"
    },
    "è¥¿ç­ç‰™è¯­": {
        "code": "es",
        "english_name": "Spanish",
        "char_pattern": r'\b[a-zA-ZÃ¡Ã©Ã­Ã³ÃºÃ¼Ã±ÃÃ‰ÃÃ“ÃšÃœÃ‘]+\b',
        "word_based": True,
        "description": "EspaÃ±ol"
    },
    "è‘¡è¯­": {
        "code": "pt",
        "english_name": "Portuguese",
        "char_pattern": r'\b[a-zA-ZÃ¡Ã©Ã­Ã³ÃºÃ¢ÃªÃ´Ã£ÃµÃ§ÃÃ‰ÃÃ“ÃšÃ‚ÃŠÃ”ÃƒÃ•Ã‡]+\b',
        "word_based": True,
        "description": "PortuguÃªs"
    },
    "æ—¥è¯­": {
        "code": "ja",
        "english_name": "Japanese",
        "char_pattern": r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]',  # å¹³å‡å+ç‰‡å‡å+æ±‰å­—
        "word_based": False,  # æŒ‰å­—ç¬¦è®¡æ•°
        "description": "æ—¥æœ¬èª"
    },
    "ä¿„è¯­": {
        "code": "ru",
        "english_name": "Russian",
        "char_pattern": r'\b[Ğ°-ÑĞ-Ğ¯Ñ‘Ğ]+\b',  # è¥¿é‡Œå°”å­—æ¯
        "word_based": True,
        "description": "Ğ ÑƒÑÑĞºĞ¸Ğ¹"
    },
    "éŸ©è¯­": {
        "code": "ko",
        "english_name": "Korean",
        "char_pattern": r'[\uAC00-\uD7AF\u1100-\u11FF]',  # éŸ©æ–‡éŸ³èŠ‚+å­—æ¯
        "word_based": False,  # æŒ‰å­—ç¬¦è®¡æ•°
        "description": "í•œêµ­ì–´"
    },
    "é˜¿è¯­": {
        "code": "ar",
        "english_name": "Arabic",
        "char_pattern": r'[\u0600-\u06FF\u0750-\u077F]+',  # é˜¿æ‹‰ä¼¯å­—æ¯
        "word_based": True,
        "description": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
    },
    "æ³•è¯­": {
        "code": "fr",
        "english_name": "French",
        "char_pattern": r'\b[a-zA-ZÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼Ã¿Å“Ã¦Ã§Ã€Ã‚Ã„Ã‰ÃˆÃŠÃ‹ÃÃÃ”Ã™Ã›ÃœÅ¸Å’Ã†Ã‡]+\b',
        "word_based": True,
        "description": "FranÃ§ais"
    },
    "æ³¢å…°è¯­": {
        "code": "pl",
        "english_name": "Polish",
        "char_pattern": r'\b[a-zA-ZÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼Ä„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»]+\b',
        "word_based": True,
        "description": "Polski"
    },
    "æ„å¤§åˆ©è¯­": {
        "code": "it",
        "english_name": "Italian",
        "char_pattern": r'\b[a-zA-ZÃ Ã¨Ã©Ã¬Ã­Ã®Ã²Ã³Ã¹ÃºÃ€ÃˆÃ‰ÃŒÃÃÃ’Ã“Ã™Ãš]+\b',
        "word_based": True,
        "description": "Italiano"
    },
    "å¾·è¯­": {
        "code": "de",
        "english_name": "German",
        "char_pattern": r'\b[a-zA-ZÃ¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ]+\b',
        "word_based": True,
        "description": "Deutsch"
    },
}

# é»˜è®¤è¯­è¨€è®¾ç½®
DEFAULT_SOURCE_LANG = "ä¸­æ–‡"
DEFAULT_TARGET_LANG = "è‹±è¯­"


# ==========================================
# === æç¤ºè¯ï¼ˆä¿æŒåŸç‰ˆä¸å˜ï¼‰===
# ==========================================
def get_ppt_alignment_prompt(source_lang="ä¸­æ–‡", target_lang="è‹±è¯­"):
    """ç”ŸæˆåŠ¨æ€çš„PPTå¯¹é½æç¤ºè¯"""
    return f"""
ä½ æ˜¯ä¸€ä¸ªã€ŒPPT åŒè¯­æ–‡æœ¬å¯¹é½å™¨ã€ã€‚

å½“å‰ä»»åŠ¡ï¼šå°† {source_lang} åŸæ–‡ä¸ {target_lang} è¯‘æ–‡è¿›è¡Œå¯¹é½ã€‚

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ã€æ ¸å¿ƒé“å¾‹ - è¿åä»»æ„ä¸€æ¡å³ä¸ºå¤±è´¥ã€‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. ç¦æ­¢ç¼–é€ ï¼šå·¦ä¾§åªèƒ½å¡«åŸæ–‡ä¸­å®é™…å­˜åœ¨çš„æ–‡å­—ï¼Œå³ä¾§åªèƒ½å¡«è¯‘æ–‡ä¸­å®é™…å­˜åœ¨çš„æ–‡å­—
2. ç¦æ­¢å ä½ç¬¦ï¼šç»å¯¹ä¸å…è®¸å‡ºç° XXXã€---ã€...ã€[ç©º]ã€[æ— ]ã€N/A ç­‰ä»»ä½•å ä½ç¬¦å·
3. ç¦æ­¢è·¨é¡µï¼šå¹»ç¯ç‰‡ N çš„å†…å®¹åªèƒ½ä¸å¹»ç¯ç‰‡ N çš„å†…å®¹é…å¯¹
4. åˆ†éš”ç¬¦ç‹¬ç«‹ï¼šã€Œ---- å¹»ç¯ç‰‡ N ----ã€å¿…é¡»å•ç‹¬æˆè¡Œï¼Œä¸å‚ä¸ ||| é…å¯¹

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ã€è¾“å‡ºæ ¼å¼ - ä¸¥æ ¼éµå®ˆã€‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
---- å¹»ç¯ç‰‡ 1 ----
{source_lang}è¡ŒA ||| {target_lang}è¡ŒA
{source_lang}è¡ŒB ||| {target_lang}è¡ŒB
{source_lang}è¡ŒC |||
||| {target_lang}è¡ŒD
---- å¹»ç¯ç‰‡ 2 ----
...

æ ¼å¼è¯´æ˜ï¼š
- åˆ†éš”è¡Œã€Œ---- å¹»ç¯ç‰‡ N ----ã€ç‹¬ç«‹ä¸€è¡Œï¼Œå‰åä¸åŠ  |||
- åŒ¹é…æˆåŠŸï¼šåŸæ–‡ ||| è¯‘æ–‡
- åŸæ–‡æ— å¯¹åº”è¯‘æ–‡ï¼šåŸæ–‡ |||
- è¯‘æ–‡æ— å¯¹åº”åŸæ–‡ï¼š||| è¯‘æ–‡

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ã€å¯¹é½ç­–ç•¥ - æŒ‰ä¼˜å…ˆçº§æ‰§è¡Œã€‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ç¬¬ä¸€æ­¥ï¼šé”šç‚¹è¯†åˆ«ï¼ˆåŒé¡µå†…ï¼‰
  â”œâ”€ æ•°å­—/æ—¥æœŸé”šç‚¹ï¼š12.15 â†” December 15ï¼Œ2024å¹´ â†” 2024
  â”œâ”€ ä¸“åé”šç‚¹ï¼šå…¬å¸åã€äººåã€äº§å“åï¼ˆå³ä½¿ä¸€ä¾§æ˜¯éŸ³è¯‘/æ„è¯‘ï¼‰
  â”œâ”€ ç¼–å·é”šç‚¹ï¼šâ‘  â†” 1)ï¼Œ1. â†” (1)ï¼Œæ³¨1 â†” Note 1
  â””â”€ æ ‡é¢˜é”šç‚¹ï¼šé€šå¸¸å­—ä½“å¤§ã€ä½ç½®é ä¸Šã€å†…å®¹æ¦‚æ‹¬æ€§å¼º

ç¬¬äºŒæ­¥ï¼šè„šæ³¨ç‰¹æ®Šå¤„ç†
  â”œâ”€ è¯†åˆ«è„šæ³¨åŒºåŸŸï¼šé€šå¸¸åœ¨é¡µé¢åº•éƒ¨ï¼Œå¸¦ä¸Šæ ‡æ•°å­—æˆ–ç‰¹æ®Šæ ‡è®°
  â”œâ”€ ç¼–å·ä¸å†…å®¹ç»‘å®šï¼šã€Œ1 XXXè¯´æ˜æ–‡å­—ã€æ•´ä½“è§†ä¸ºä¸€ä¸ªè„šæ³¨å•å…ƒ
  â”œâ”€ é…å¯¹åŸåˆ™ï¼šåŸæ–‡è„šæ³¨1 â†” è¯‘æ–‡è„šæ³¨1ï¼ˆæŒ‰ç¼–å·ï¼ŒéæŒ‰ä½ç½®ï¼‰
  â””â”€ ä¸å¯æ··é…ï¼šè„šæ³¨ç¼–å·ä¸èƒ½ä¸æ­£æ–‡å†…å®¹é…å¯¹

ç¬¬ä¸‰æ­¥ï¼šè¯­ä¹‰é…å¯¹
  â”œâ”€ å«ä¹‰æœ€æ¥è¿‘çš„è¡Œ 1:1 é…å¯¹
  â”œâ”€ å…è®¸åˆç†æ‹†åˆ†ï¼šä¸€é•¿å¥ â†” ä¸¤çŸ­å¥ï¼ˆè¯­ä¹‰å®Œæ•´æ—¶ï¼‰
  â”œâ”€ å…è®¸åˆç†åˆå¹¶ï¼šä¸¤çŸ­å¥ â†” ä¸€é•¿å¥ï¼ˆè¯­ä¹‰å®Œæ•´æ—¶ï¼‰
  â””â”€ æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µåªèƒ½ä½¿ç”¨ä¸€æ¬¡

ç¬¬å››æ­¥ï¼šå¤„ç†å‰©ä½™
  â”œâ”€ åŸæ–‡æœ‰ã€è¯‘æ–‡æ—  â†’ ã€ŒåŸæ–‡ |||ã€
  â”œâ”€ è¯‘æ–‡æœ‰ã€åŸæ–‡æ—  â†’ ã€Œ||| è¯‘æ–‡ã€
  â””â”€ ç»ä¸å¡«å……ä»»ä½•è™šæ„å†…å®¹

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ã€æ‰§è¡Œæ£€æŸ¥æ¸…å•ã€‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
è¾“å‡ºå‰é€é¡¹ç¡®è®¤ï¼š
â–¡ åˆ†éš”ç¬¦æ˜¯å¦ç‹¬ç«‹æˆè¡Œï¼Ÿ
â–¡ æ˜¯å¦å­˜åœ¨ä»»ä½• XXX/---/.../[ç©º] ç­‰å ä½ç¬¦ï¼Ÿ
â–¡ è„šæ³¨ç¼–å·æ˜¯å¦ä¸å¯¹åº”ç¼–å·é…å¯¹ï¼ˆè€Œéä¸æ­£æ–‡é…å¯¹ï¼‰ï¼Ÿ
â–¡ æ¯ä¸ªåŸæ–‡ç‰‡æ®µæ˜¯å¦åªå‡ºç°ä¸€æ¬¡ï¼Ÿ
â–¡ æ¯ä¸ªè¯‘æ–‡ç‰‡æ®µæ˜¯å¦åªå‡ºç°ä¸€æ¬¡ï¼Ÿ
â–¡ æ˜¯å¦æœ‰è·¨é¡µé…å¯¹ï¼Ÿ

åªè¾“å‡ºå¯¹é½ç»“æœï¼Œä¸è¾“å‡ºä»»ä½•è§£é‡Šè¯´æ˜ã€‚
"""


# ä¿ç•™é»˜è®¤çš„æç¤ºè¯ç”¨äºå…¼å®¹
PPT_ALIGNMENT_SYSTEM_PROMPT = get_ppt_alignment_prompt()


def get_docx_alignment_prompt(source_lang="ä¸­æ–‡", target_lang="è‹±è¯­"):
    """ç”ŸæˆåŠ¨æ€çš„æ–‡æ¡£å¯¹é½æç¤ºè¯"""
    source_info = SUPPORTED_LANGUAGES.get(source_lang, SUPPORTED_LANGUAGES["ä¸­æ–‡"])
    target_info = SUPPORTED_LANGUAGES.get(target_lang, SUPPORTED_LANGUAGES["è‹±è¯­"])

    # åˆ¤æ–­åŸæ–‡æ˜¯å¦ä¸ºä¸­æ—¥éŸ©ç­‰éœ€è¦ç‰¹æ®Šæ–­å¥çš„è¯­è¨€
    source_is_cjk = source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]

    # æ ¹æ®åŸæ–‡è¯­è¨€ç±»å‹ç¡®å®šæ–­å¥æ ‡ç‚¹
    if source_is_cjk:
        punctuation_marks = "ã€‚ï¼ï¼Ÿ"
        punctuation_desc = "ã€‚ï¼ï¼Ÿï¼ˆå…¨è§’ä¸­æ—¥éŸ©å¥æœ«æ ‡ç‚¹ï¼‰"
    else:
        punctuation_marks = ". ! ?"
        punctuation_desc = ". ! ?ï¼ˆåŠè§’è¥¿æ–‡å¥æœ«æ ‡ç‚¹ï¼‰"

    sentence_rule = f"""2. ğŸ¯ æ–­å¥è§„åˆ™ï¼š{source_lang}ä¸»å¯¼ï¼Œ{target_lang}è·Ÿéšï¼ˆå¼ºä¼˜å…ˆçº§ï¼‰

   ç¬¬ä¸€æ­¥ï¼šåªçœ‹{source_lang}ï¼ŒæŒ‰ä»¥ä¸‹æ ‡ç‚¹æ–­å¥
   - æ–­å¥æ ‡ç‚¹ï¼š{punctuation_desc}
   - é‡åˆ°è¿™äº›æ ‡ç‚¹å°±æ–­å¼€ï¼Œå½¢æˆä¸€ä¸ª{source_lang}ç‰‡æ®µ

   ç¬¬äºŒæ­¥ï¼š{target_lang}åŒ¹é…{source_lang}
   - åœ¨ Stream B ä¸­æ‰¾å‡ºä¸è¯¥{source_lang}ç‰‡æ®µè¯­ä¹‰å¯¹åº”çš„{target_lang}éƒ¨åˆ†
   - {target_lang}å¯èƒ½æ˜¯1å¥ã€åŠå¥ã€æˆ–å¤šå¥ï¼Œéƒ½æ²¡å…³ç³»
   - è¯‘æ–‡ä½ç½®å¯èƒ½æœ‰é”™ä½ï¼ŒæŒ‰è¯­ä¹‰åŒ¹é…å³å¯
   - åªè¦è¯­ä¹‰å®Œå…¨è¦†ç›–è¯¥{source_lang}ç‰‡æ®µå³å¯

3. è´¨é‡æ£€æŸ¥
   - å·¦è¾¹çš„æ¯ä¸ª{source_lang}ç‰‡æ®µæ˜¯å¦éƒ½ä»¥æ­£ç¡®çš„å¥æœ«æ ‡ç‚¹ï¼ˆ{punctuation_marks}ï¼‰ç»“å°¾ï¼Ÿ
   - å³è¾¹{target_lang}çš„è¯­ä¹‰æ˜¯å¦å®Œå…¨å¯¹åº”å·¦è¾¹{source_lang}ï¼Ÿ
   - æ˜¯å¦æœ‰ä»»ä½•å†…å®¹è¢«ä¿®æ”¹æˆ–è‡ªè¡Œç¿»è¯‘ï¼Ÿï¼ˆå¿…é¡»ä¸ºå¦ï¼‰"""

    return f"""ä½ æ˜¯ä¸€ä¸ªã€ŒåŒæµæ–‡æœ¬å¯¹é½åŒæ­¥å™¨ã€(Dual-Stream Aligner)ã€‚

æ ¸å¿ƒä»»åŠ¡
å°† Stream A ({source_lang}åŸæ–‡) å’Œ Stream B ({target_lang}è¯‘æ–‡) è¿›è¡Œç²¾ç¡®å¯¹é½ã€‚

â›” ç»å¯¹é“å¾‹
1. æ¥æºé”å®šï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
   - "|||" å·¦ä¾§ = å¿…é¡»100%æ¥è‡ª Stream Aï¼Œä¸€ä¸ªå­—éƒ½ä¸èƒ½æ”¹
   - "|||" å³ä¾§ = å¿…é¡»100%æ¥è‡ª Stream Bï¼Œä¸€ä¸ªå­—éƒ½ä¸èƒ½æ”¹
   - ã€Œç¦æ­¢ã€è‡ªå·±ç¿»è¯‘æˆ–ç¼–é€ ä»»ä½•å†…å®¹
   - å³ä½¿å‘ç°åŸæ–‡æœ‰é”™åˆ«å­—ã€è¯‘æ–‡æœ‰ç¿»è¯‘é”™è¯¯ï¼Œä¹Ÿå¿…é¡»åŸæ ·ä¿ç•™

{sentence_rule}

4. ç©ºè¡Œå¤„ç†è§„åˆ™
   - å¦‚æœåŸæ–‡æˆ–è¯‘æ–‡ä¸­æœ‰ç©ºè¡Œï¼ˆè¿ç»­æ¢è¡Œç¬¦ï¼‰ï¼Œè¿™äº›ç©ºè¡Œå·²ç»è¢«é€‚å½“å‹ç¼©ä¿ç•™
   - ç©ºè¡Œé€šå¸¸ç”¨äºåˆ†éš”æ®µè½æˆ–ç« èŠ‚
   - åœ¨å¯¹é½æ—¶ï¼Œå¿½ç•¥ç©ºè¡Œçš„ä½ç½®å·®å¼‚ï¼Œä¸“æ³¨äºæœ‰å†…å®¹çš„æ–‡æœ¬å¯¹é½
   - ä¸è¦è¾“å‡ºç©ºè¡Œå¯¹åº”çš„å¯¹é½ç»“æœï¼ˆç©º|||ç©ºï¼‰

è¾“å‡ºæ ¼å¼
{source_lang}ç‰‡æ®µ ||| å¯¹åº”çš„{target_lang}å†…å®¹
"""


# ä¿ç•™ä¸€ä¸ªé»˜è®¤çš„æç¤ºè¯ç”¨äºå…¼å®¹
DOCX_ALIGNMENT_SYSTEM_PROMPT = get_docx_alignment_prompt()


def get_split_row_prompt(source_lang="ä¸­æ–‡"):
    """ç”ŸæˆåŠ¨æ€çš„åˆ†å¥å¯¹é½æç¤ºè¯"""
    # åˆ¤æ–­åŸæ–‡æ˜¯å¦ä¸ºä¸­æ—¥éŸ©ç­‰éœ€è¦ç‰¹æ®Šæ–­å¥çš„è¯­è¨€
    source_is_cjk = source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]

    if source_is_cjk:
        punctuation_marks = "ã€‚ï¼ï¼Ÿ"
        punctuation_desc = "ã€‚ï¼ï¼Ÿï¼ˆå…¨è§’å¥æœ«æ ‡ç‚¹ï¼‰"
        abbreviation_rule = ""
        numbering_rule = "1. / 2. / 11. / 1ï¼‰/ 2ï¼‰/ â‘ â‘¡ ç­‰æ•°å­—ç¼–å·"
    else:
        punctuation_marks = ". ! ?"
        punctuation_desc = ". ! ?ï¼ˆåŠè§’å¥æœ«æ ‡ç‚¹ï¼‰"
        numbering_rule = """- æ•°å­—ç¼–å·ï¼š1. / 2. / 11. / (1) / (2) ç­‰
   - å­—æ¯ç¼–å·ï¼šA. / B. / C. / a. / b. / c. / (A) / (B) ç­‰
   - ç½—é©¬æ•°å­—ï¼šI. / II. / III. / i. / ii. / iii. ç­‰
   - æ··åˆç¼–å·ï¼š1.1 / 1.2 / A.1 / A.2 ç­‰ï¼ˆåœ¨æœ€åä¸€ä¸ªç‚¹åæ–­å¼€ï¼‰"""
        abbreviation_rule = """
âš ï¸ ç¼©å†™ä¾‹å¤–ï¼ˆè¿™äº›å¥å·ä¸è¡¨ç¤ºå¥æœ«ï¼Œä¸è¦åœ¨æ­¤æ–­å¼€ï¼‰ï¼š
- å­¦æœ¯å¼•ç”¨ï¼šet al. / etc. / e.g. / i.e. / vs. / cf. / ibid.
- ç§°è°“ï¼šMr. / Mrs. / Ms. / Dr. / Prof. / Jr. / Sr. / St.
- å…¬å¸ï¼šInc. / Ltd. / Co. / Corp. / L.L.C.
- å›½å®¶/ç»„ç»‡ï¼šU.S. / U.K. / U.N. / E.U.
- æ—¶é—´ï¼ša.m. / p.m. / A.M. / P.M.
- å¼•ç”¨æ ‡è®°ï¼šNo. / Vol. / Fig. / Ch. / Sec. / pp. / p.
- å…¶ä»–ï¼šapprox. / est. / max. / min. / avg.

ğŸ’¡ åŒºåˆ†ç¼–å·ä¸ç¼©å†™çš„æ–¹æ³•ï¼š
- ç¼–å·ç‰¹å¾ï¼šå•ç‹¬çš„æ•°å­—æˆ–å­—æ¯ + å¥å·ï¼Œå¦‚ "1." "A." "II."
- ç¼©å†™ç‰¹å¾ï¼šå¤šä¸ªå­—æ¯ç»„æˆçš„è¯ + å¥å·ï¼Œå¦‚ "et al." "Dr." "Inc."
- ç¼–å·åé¢é€šå¸¸è·Ÿç€æ­£æ–‡å†…å®¹ï¼Œç¼©å†™åé¢é€šå¸¸è¿˜æœ‰æ›´å¤šæ–‡å­—"""

    # é’ˆå¯¹è¥¿æ–¹è¯­è¨€æ·»åŠ æ›´è¯¦ç»†çš„æ–­å¥ç¤ºä¾‹
    if not source_is_cjk:
        example_section = """
## ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šæ•°å­—ç¼–å· + ä½œè€…ä¿¡æ¯
è¾“å…¥ï¼š
ã€åŸæ–‡ã€‘11. Piacentini MG, et al. A randomized controlled trial on the effect of Solidago virgaurea extract.
ã€è¯‘æ–‡ã€‘11.Piacentini MGï¼Œç­‰ã€‚ä¸€é¡¹å…³äºä¸€æé»„èŠ±æå–ç‰©å½±å“çš„éšæœºå¯¹ç…§è¯•éªŒã€‚

æ­£ç¡®è¾“å‡ºï¼ˆ3è¡Œï¼‰ï¼š
11. ||| 11.
Piacentini MG, et al. ||| Piacentini MGï¼Œç­‰ã€‚
A randomized controlled trial on the effect of Solidago virgaurea extract. ||| ä¸€é¡¹å…³äºä¸€æé»„èŠ±æå–ç‰©å½±å“çš„éšæœºå¯¹ç…§è¯•éªŒã€‚

### ç¤ºä¾‹2ï¼šå­—æ¯ç¼–å·
è¾“å…¥ï¼š
ã€åŸæ–‡ã€‘A. Introduction. B. Methods. C. Results.
ã€è¯‘æ–‡ã€‘A. å¼•è¨€ã€‚B. æ–¹æ³•ã€‚C. ç»“æœã€‚

æ­£ç¡®è¾“å‡ºï¼ˆ6è¡Œï¼‰ï¼š
A. ||| A.
Introduction. ||| å¼•è¨€ã€‚
B. ||| B.
Methods. ||| æ–¹æ³•ã€‚
C. ||| C.
Results. ||| ç»“æœã€‚

### ç¤ºä¾‹3ï¼šæ··åˆç¼–å·
è¾“å…¥ï¼š
ã€åŸæ–‡ã€‘1.1 Background. 1.2 Objectives.
ã€è¯‘æ–‡ã€‘1.1 èƒŒæ™¯ã€‚1.2 ç›®æ ‡ã€‚

æ­£ç¡®è¾“å‡ºï¼ˆ4è¡Œï¼‰ï¼š
1.1 ||| 1.1
Background. ||| èƒŒæ™¯ã€‚
1.2 ||| 1.2
Objectives. ||| ç›®æ ‡ã€‚

### å…³é”®åˆ¤æ–­è§„åˆ™ï¼š
- "A." "B." "1." "11." "1.1" â†’ ç¼–å·ï¼Œåé¢æ–­å¼€
- "et al." "Dr." "U.S." "Inc." â†’ ç¼©å†™ï¼Œä¸åœ¨æ­¤æ–­å¼€
- ç¼–å· = å•ç‹¬æ•°å­—/å­—æ¯ + ç‚¹å·
- ç¼©å†™ = å¤šå­—æ¯è¯æ±‡ + ç‚¹å·
"""
    else:
        example_section = ""
        numbering_rule = "1. / 2. / 11. / 1ï¼‰/ 2ï¼‰/ â‘ â‘¡ ç­‰æ•°å­—ç¼–å·"

    return f"""ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„ã€Œå•è¡Œåˆ†å¥å¯¹é½å™¨ã€ã€‚

## ä»»åŠ¡
æ ¹æ®ã€åŸæ–‡ã€‘çš„åˆ†å‰²ç‚¹ï¼Œå°†ã€è¯‘æ–‡ã€‘å¯¹åº”æ‹†åˆ†å¯¹é½ã€‚

## â›” é“å¾‹

### 1. åˆ†å‰²ç‚¹è¯†åˆ«ï¼ˆå¿…é¡»åœ¨ä»¥ä¸‹ä½ç½®æ–­å¼€ï¼‰

#### 1.1 ç¼–å·ï¼ˆç¼–å·åå¿…é¡»æ–­å¼€ï¼Œç¼–å·å•ç‹¬æˆè¡Œï¼‰
   {numbering_rule}

#### 1.2 å¥æœ«æ ‡ç‚¹ï¼ˆ{punctuation_desc}ï¼‰
   æ¯ä¸ªå¥å·/æ„Ÿå¹å·/é—®å·åéƒ½è¦æ–­å¼€
{abbreviation_rule}

### 2. ç¦æ­¢ä¿®æ”¹å†…å®¹
- å·¦ä¾§å¿…é¡»100%æ¥è‡ªåŸæ–‡ï¼Œä¸€å­—ä¸æ”¹
- å³ä¾§å¿…é¡»100%æ¥è‡ªè¯‘æ–‡ï¼Œä¸€å­—ä¸æ”¹
- ç¦æ­¢ç¿»è¯‘ã€ç¼–é€ ã€è¡¥å……ä»»ä½•å†…å®¹

### 3. è¯‘æ–‡å¯¹é½è§„åˆ™
- æ ¹æ®è¯­ä¹‰å°†è¯‘æ–‡æ‹†åˆ†ï¼ŒåŒ¹é…åŸæ–‡çš„æ¯ä¸ªåˆ†å‰²å•å…ƒ
- å¦‚æœè¯‘æ–‡æ ‡ç‚¹ä¸åŸæ–‡ä¸ä¸€è‡´ï¼ŒæŒ‰è¯­ä¹‰å¯¹é½

### 4. åˆ†å‰²åŸåˆ™
- ç¼–å·å•ç‹¬æˆè¡Œï¼ˆå¦‚ "11." / "A." / "1.1"ï¼‰
- ä½œè€…ä¿¡æ¯å•ç‹¬æˆè¡Œï¼ˆå¦‚ "Piacentini MG, et al."ï¼‰
- æ¯ä¸ªå®Œæ•´å¥å­å•ç‹¬æˆè¡Œ
- å³ä½¿ç‰‡æ®µå¾ˆçŸ­ä¹Ÿè¦ç‹¬ç«‹æˆè¡Œ
- è¾“å‡ºè¡Œæ•° = åŸæ–‡åˆ†å‰²å•å…ƒæ•°
{example_section}
## è¾“å‡ºæ ¼å¼
æ¯è¡Œä¸€å¯¹ï¼Œç”¨ ||| åˆ†éš”ï¼š
åŸæ–‡ç‰‡æ®µ1 ||| è¯‘æ–‡å¯¹åº”éƒ¨åˆ†1
åŸæ–‡ç‰‡æ®µ2 ||| è¯‘æ–‡å¯¹åº”éƒ¨åˆ†2

åªè¾“å‡ºå¯¹é½ç»“æœï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚"""


# ä¿ç•™é»˜è®¤çš„æç¤ºè¯ç”¨äºå…¼å®¹
SPLIT_ROW_SYSTEM_PROMPT = get_split_row_prompt()


def get_table_cell_split_prompt(source_lang="ä¸­æ–‡"):
    """ç”Ÿæˆè¡¨æ ¼å•å…ƒæ ¼ç»†ç²’åº¦åˆ†å¥æç¤ºè¯ - ä¸“é—¨ç”¨äºä¸­è‹±æ–¹å‘è¡¨æ ¼å¤„ç†"""
    source_is_cjk = source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]

    if source_is_cjk:
        punctuation_marks = "ã€‚ï¼ï¼Ÿ"
        punctuation_desc = "ã€‚ï¼ï¼Ÿï¼ˆå…¨è§’å¥æœ«æ ‡ç‚¹ï¼‰"
    else:
        punctuation_marks = ". ! ?"
        punctuation_desc = ". ! ?ï¼ˆåŠè§’å¥æœ«æ ‡ç‚¹ï¼‰"

    return f"""ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„ã€Œè¡¨æ ¼å•å…ƒæ ¼åˆ†å¥å¯¹é½å™¨ã€ã€‚

## ä»»åŠ¡
å¯¹è¡¨æ ¼å•å…ƒæ ¼å†…çš„åŸæ–‡å’Œè¯‘æ–‡è¿›è¡Œç»†ç²’åº¦åˆ†å¥å¯¹é½ã€‚

## åˆ†å¥è§„åˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

### 1. æ–­å¥ä½ç½®ï¼ˆåªåœ¨ä»¥ä¸‹ä½ç½®æ–­å¼€ï¼‰
- **å¥æœ«æ ‡ç‚¹**ï¼š{punctuation_desc}
- **æ¢è¡Œç¬¦**ï¼šå½“åŸæ–‡ä¸­æœ‰ç©ºè¡Œåˆ†éš”æ—¶ï¼Œåœ¨ç©ºè¡Œå¤„æ–­å¼€

### 2. ã€é‡è¦ã€‘ä¸æ˜¯æ–­å¥ç‚¹çš„æƒ…å†µ
- **åºå·æ ‡ç‚¹ä¸æ–­å¥**ï¼šå¦‚"1."ã€"2."ã€"3."ã€"â‘ "ã€"â‘¡"ã€"(1)"ã€"a."ç­‰åºå·åçš„ç‚¹å·ï¼Œè¿™æ˜¯åºå·çš„ä¸€éƒ¨åˆ†ï¼Œä¸æ˜¯å¥æœ«
- **åºå·å¿…é¡»ä¿ç•™åœ¨å¥å­å¼€å¤´**ï¼šåºå·å’Œåé¢çš„å†…å®¹å±äºåŒä¸€å¥
- åªæœ‰çœŸæ­£è¡¨ç¤ºå¥å­ç»“æŸçš„æ ‡ç‚¹ï¼ˆ{punctuation_marks}ï¼‰æ‰æ–­å¥

### 3. åˆ†å¥åŸåˆ™
- ä»¥åŸæ–‡çš„åˆ†å¥ç»“æ„ä¸ºå‡†
- æ¯ä¸ªå®Œæ•´å¥å­ï¼ˆä»¥{punctuation_marks}ç»“å°¾ï¼‰ç‹¬ç«‹æˆè¡Œ
- å½“é‡åˆ°ç©ºè¡Œåˆ†éš”çš„æ®µè½æ—¶ï¼ŒæŒ‰æ®µè½æ–­å¼€
- ä¿æŒåŸæ–‡å’Œè¯‘æ–‡çš„å¥å­ä¸€ä¸€å¯¹åº”

### 4. ç¦æ­¢ä¿®æ”¹å†…å®¹
- å·¦ä¾§å¿…é¡»100%æ¥è‡ªåŸæ–‡ï¼Œä¸€å­—ä¸æ”¹
- å³ä¾§å¿…é¡»100%æ¥è‡ªè¯‘æ–‡ï¼Œä¸€å­—ä¸æ”¹
- ç¦æ­¢ç¿»è¯‘ã€ç¼–é€ ã€è¡¥å……ä»»ä½•å†…å®¹
- ä¿ç•™æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å’Œåºå·

### 5. è¯‘æ–‡å¯¹é½è§„åˆ™
- æ ¹æ®è¯­ä¹‰å°†è¯‘æ–‡æ‹†åˆ†ï¼ŒåŒ¹é…åŸæ–‡çš„æ¯ä¸ªå¥å­
- å¦‚æœè¯‘æ–‡æ ‡ç‚¹ä¸åŸæ–‡ä¸å®Œå…¨ä¸€è‡´ï¼ŒæŒ‰è¯­ä¹‰å¯¹é½

## ç¤ºä¾‹1ï¼ˆå¸¦åºå·çš„å¤šæ¡å†…å®¹ï¼‰

### è¾“å…¥ï¼š
ã€åŸæ–‡ã€‘
1.è¿™æ˜¯ç¬¬ä¸€æ¡è¯´æ˜ã€‚éœ€è¦æ³¨æ„è¿™ä¸ªé—®é¢˜ã€‚

2.è¿™æ˜¯ç¬¬äºŒæ¡è¯´æ˜ï¼›

ã€è¯‘æ–‡ã€‘
1. This is the first instruction. Please note this issue.
2. This is the second instruction;

### æ­£ç¡®è¾“å‡ºï¼š
1.è¿™æ˜¯ç¬¬ä¸€æ¡è¯´æ˜ã€‚ ||| 1. This is the first instruction.
éœ€è¦æ³¨æ„è¿™ä¸ªé—®é¢˜ã€‚ ||| Please note this issue.
2.è¿™æ˜¯ç¬¬äºŒæ¡è¯´æ˜ï¼› ||| 2. This is the second instruction;

### é”™è¯¯è¾“å‡ºï¼ˆç»å¯¹ç¦æ­¢ï¼‰ï¼š
1. ||| 1.
è¿™æ˜¯ç¬¬ä¸€æ¡è¯´æ˜ã€‚ ||| This is the first instruction.
ï¼ˆé”™è¯¯åŸå› ï¼šæŠŠåºå·"1."å•ç‹¬æ‹†åˆ†äº†ï¼Œåºå·å¿…é¡»å’Œåé¢çš„å†…å®¹åœ¨ä¸€èµ·ï¼‰

## ç¤ºä¾‹2ï¼ˆæ™®é€šå¤šå¥å†…å®¹ï¼‰

### è¾“å…¥ï¼š
ã€åŸæ–‡ã€‘
è¿™æ˜¯ç¬¬ä¸€å¥ã€‚è¿™æ˜¯ç¬¬äºŒå¥ï¼
è¿™æ˜¯æ¢è¡Œåçš„ç¬¬ä¸‰å¥ï¼Ÿ

ã€è¯‘æ–‡ã€‘
This is the first sentence. This is the second sentence!
This is the third sentence after newline?

### æ­£ç¡®è¾“å‡ºï¼š
è¿™æ˜¯ç¬¬ä¸€å¥ã€‚ ||| This is the first sentence.
è¿™æ˜¯ç¬¬äºŒå¥ï¼ ||| This is the second sentence!
è¿™æ˜¯æ¢è¡Œåçš„ç¬¬ä¸‰å¥ï¼Ÿ ||| This is the third sentence after newline?

## è¾“å‡ºæ ¼å¼
æ¯è¡Œä¸€å¯¹ï¼Œç”¨ ||| åˆ†éš”ï¼š
åŸæ–‡å¥å­1 ||| è¯‘æ–‡å¯¹åº”éƒ¨åˆ†1
åŸæ–‡å¥å­2 ||| è¯‘æ–‡å¯¹åº”éƒ¨åˆ†2

åªè¾“å‡ºå¯¹é½ç»“æœï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚"""


# ä¿ç•™é»˜è®¤çš„è¡¨æ ¼åˆ†å¥æç¤ºè¯ç”¨äºå…¼å®¹
TABLE_CELL_SPLIT_PROMPT = get_table_cell_split_prompt()


# ==========================================
# === ğŸ“ æ—¥å¿—ç®¡ç†å™¨ ===
# ==========================================
class LogManager:
    def __init__(self):
        self.log_queue = queue.Queue()
        self.exception_queue = queue.Queue()
        self.stream_queue = queue.Queue()

    def log(self, message, level="INFO"):
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.log_queue.put(f"[{timestamp}] [{level}] {message}")

    def log_exception(self, message, data=None):
        timestamp = datetime.now().strftime("%H:%M:%S")
        exception_msg = f"[{timestamp}] âš ï¸ {message}"
        if data:
            exception_msg += f"\n    æ•°æ®: {data}"
        self.exception_queue.put(exception_msg)

    def log_stream(self, content):
        self.stream_queue.put(content)


log_manager = LogManager()


# ==========================================
# === ğŸ› ï¸ å·¥å…·å‡½æ•° ===
# ==========================================
def get_file_type(file_path):
    """è·å–æ–‡ä»¶ç±»å‹"""
    ext = os.path.splitext(file_path)[1].lower()
    if ext == '.docx':
        return 'docx'
    elif ext == '.doc':
        return 'doc'
    elif ext == '.pptx':
        return 'pptx'
    elif ext in ['.xlsx', '.xls']:
        return 'excel'
    return 'unknown'


def identify_column(df, patterns):
    """æ ¹æ®æ¨¡å¼è¯†åˆ«åˆ—å"""
    for col in df.columns:
        col_lower = str(col).lower()
        for pattern in patterns:
            if pattern in col_lower:
                return col
    return None


def convert_doc_to_docx(doc_path, output_dir=None):
    """å°† .doc æ–‡ä»¶è½¬æ¢ä¸º .docx æ–‡ä»¶"""
    try:
        import pythoncom
        from win32com import client as win32_client
        pythoncom.CoInitialize()

        if output_dir is None:
            output_dir = os.path.dirname(doc_path)

        base_name = os.path.splitext(os.path.basename(doc_path))[0]
        docx_path = os.path.join(output_dir, f"{base_name}_converted.docx")
        doc_path = os.path.abspath(doc_path)
        docx_path = os.path.abspath(docx_path)

        word = win32_client.Dispatch("Word.Application")
        word.Visible = False
        try:
            doc = word.Documents.Open(doc_path)
            doc.SaveAs2(docx_path, FileFormat=16)
            doc.Close()
            log_manager.log(f"å·²å°† .doc è½¬æ¢ä¸º .docx: {docx_path}")
            return docx_path
        finally:
            word.Quit()
            pythoncom.CoUninitialize()
    except ImportError:
        log_manager.log_exception("éœ€è¦å®‰è£… pywin32 æ¥å¤„ç† .doc æ–‡ä»¶", "pip install pywin32")
    except Exception as e:
        log_manager.log_exception(f"è½¬æ¢ .doc æ–‡ä»¶å¤±è´¥: {e}", doc_path)
    return None


def read_excel_file(file_path):
    """è¯»å– Excel æ–‡ä»¶"""
    try:
        df = pd.read_excel(file_path)
        log_manager.log(f"è¯»å– Excel æ–‡ä»¶: {file_path}")
        log_manager.log(f"åˆ—å: {list(df.columns)}, è¡Œæ•°: {len(df)}")
        return df
    except Exception as e:
        log_manager.log_exception(f"è¯»å– Excel æ–‡ä»¶å¤±è´¥: {e}", file_path)
        return None


def _parse_xlsx_with_zipfile(file_path):
    """ä½¿ç”¨ zipfile å’Œ lxml ç›´æ¥è§£æ xlsx æ–‡ä»¶ï¼Œå®Œå…¨ç»•è¿‡ openpyxl

    xlsx æ–‡ä»¶æœ¬è´¨æ˜¯ä¸€ä¸ª ZIP åŒ…ï¼ŒåŒ…å«å¤šä¸ª XML æ–‡ä»¶ï¼š
    - xl/workbook.xml: å·¥ä½œç°¿ä¿¡æ¯ï¼ˆåŒ…å«å·¥ä½œè¡¨åç§°ï¼‰
    - xl/sharedStrings.xml: å…±äº«å­—ç¬¦ä¸²è¡¨
    - xl/worksheets/sheet1.xml ç­‰: å„å·¥ä½œè¡¨æ•°æ®
    """
    import zipfile

    NS = {
        'main': 'http://schemas.openxmlformats.org/spreadsheetml/2006/main',
        'r': 'http://schemas.openxmlformats.org/officeDocument/2006/relationships',
        'rel': 'http://schemas.openxmlformats.org/package/2006/relationships'
    }

    with zipfile.ZipFile(file_path, 'r') as zf:
        # 1. è¯»å–å…±äº«å­—ç¬¦ä¸²è¡¨
        shared_strings = []
        if 'xl/sharedStrings.xml' in zf.namelist():
            with zf.open('xl/sharedStrings.xml') as f:
                tree = etree.parse(f)
                for si in tree.findall('.//main:si', NS):
                    # è·å–æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼ˆåŒ…æ‹¬å¯Œæ–‡æœ¬ï¼‰
                    texts = []
                    for t in si.iter('{http://schemas.openxmlformats.org/spreadsheetml/2006/main}t'):
                        if t.text:
                            texts.append(t.text)
                    shared_strings.append(''.join(texts))

        # 2. è¯»å–å·¥ä½œç°¿è·å–å·¥ä½œè¡¨ä¿¡æ¯
        with zf.open('xl/workbook.xml') as f:
            wb_tree = etree.parse(f)
            sheets_info = []
            for sheet in wb_tree.findall('.//main:sheet', NS):
                sheet_name = sheet.get('name')
                sheet_id = sheet.get('{http://schemas.openxmlformats.org/officeDocument/2006/relationships}id')
                sheets_info.append((sheet_name, sheet_id))

        # 3. è¯»å–å·¥ä½œè¡¨å…³ç³»æ–‡ä»¶è·å–å®é™…æ–‡ä»¶è·¯å¾„
        sheet_files = {}
        with zf.open('xl/_rels/workbook.xml.rels') as f:
            rels_tree = etree.parse(f)
            for rel in rels_tree.findall('.//rel:Relationship', NS):
                rid = rel.get('Id')
                target = rel.get('Target')
                if 'worksheet' in target.lower():
                    sheet_files[rid] = 'xl/' + target.lstrip('/')

        # 4. è¯»å–æ¯ä¸ªå·¥ä½œè¡¨çš„æ•°æ®
        sheets_dict = {}
        for sheet_name, sheet_rid in sheets_info:
            sheet_path = sheet_files.get(sheet_rid)
            if not sheet_path or sheet_path not in zf.namelist():
                # å°è¯•å¸¸è§è·¯å¾„
                for i in range(1, 100):
                    alt_path = f'xl/worksheets/sheet{i}.xml'
                    if alt_path in zf.namelist():
                        sheet_path = alt_path
                        break

            if not sheet_path or sheet_path not in zf.namelist():
                sheets_dict[sheet_name] = pd.DataFrame()
                continue

            with zf.open(sheet_path) as f:
                sheet_tree = etree.parse(f)
                rows_data = {}
                max_col = 0

                for row in sheet_tree.findall('.//main:row', NS):
                    row_idx = int(row.get('r', 0)) - 1  # è½¬ä¸º0ç´¢å¼•
                    if row_idx < 0:
                        continue

                    for cell in row.findall('main:c', NS):
                        cell_ref = cell.get('r', '')
                        cell_type = cell.get('t', '')

                        # è§£æåˆ—ç´¢å¼•
                        col_str = ''.join(c for c in cell_ref if c.isalpha())
                        col_idx = 0
                        for c in col_str.upper():
                            col_idx = col_idx * 26 + (ord(c) - ord('A') + 1)
                        col_idx -= 1  # è½¬ä¸º0ç´¢å¼•
                        max_col = max(max_col, col_idx + 1)

                        # è·å–å•å…ƒæ ¼å€¼
                        value_elem = cell.find('main:v', NS)
                        value = None
                        if value_elem is not None and value_elem.text:
                            if cell_type == 's':  # å…±äº«å­—ç¬¦ä¸²
                                try:
                                    idx = int(value_elem.text)
                                    value = shared_strings[idx] if idx < len(shared_strings) else ''
                                except (ValueError, IndexError):
                                    value = ''
                            elif cell_type == 'b':  # å¸ƒå°”å€¼
                                value = value_elem.text == '1'
                            elif cell_type == 'inlineStr':  # å†…è”å­—ç¬¦ä¸²
                                is_elem = cell.find('main:is', NS)
                                if is_elem is not None:
                                    texts = []
                                    for t in is_elem.iter(
                                            '{http://schemas.openxmlformats.org/spreadsheetml/2006/main}t'):
                                        if t.text:
                                            texts.append(t.text)
                                    value = ''.join(texts)
                            else:  # æ•°å­—æˆ–å…¶ä»–
                                try:
                                    if '.' in value_elem.text:
                                        value = float(value_elem.text)
                                    else:
                                        value = int(value_elem.text)
                                except ValueError:
                                    value = value_elem.text
                        else:
                            # æ£€æŸ¥å†…è”å­—ç¬¦ä¸²
                            is_elem = cell.find('main:is', NS)
                            if is_elem is not None:
                                texts = []
                                for t in is_elem.iter('{http://schemas.openxmlformats.org/spreadsheetml/2006/main}t'):
                                    if t.text:
                                        texts.append(t.text)
                                value = ''.join(texts)

                        if row_idx not in rows_data:
                            rows_data[row_idx] = {}
                        rows_data[row_idx][col_idx] = value

                # è½¬æ¢ä¸º DataFrame
                if rows_data:
                    max_row = max(rows_data.keys()) + 1
                    data = [[None] * max_col for _ in range(max_row)]
                    for r_idx, cols in rows_data.items():
                        for c_idx, val in cols.items():
                            if c_idx < max_col:
                                data[r_idx][c_idx] = val
                    df = pd.DataFrame(data)
                else:
                    df = pd.DataFrame()

                sheets_dict[sheet_name] = df

        return sheets_dict


def read_excel_all_sheets(file_path):
    """è¯»å– Excel æ–‡ä»¶çš„æ‰€æœ‰å·¥ä½œç°¿ï¼Œè¿”å› {sheet_name: DataFrame} å­—å…¸

    ä½¿ç”¨å¤šç§æ–¹æ³•å°è¯•è¯»å–ï¼Œä»¥åº”å¯¹ä¸åŒæ ¼å¼çš„Excelæ–‡ä»¶ï¼š
    1. é¦–å…ˆå°è¯•æ ‡å‡† pandas è¯»å–
    2. å¦‚æœå¤±è´¥ï¼ˆå¦‚ InlineFont é”™è¯¯ï¼‰ï¼Œä½¿ç”¨ zipfile+lxml ç›´æ¥è§£æ
    """
    log_manager.log(f"è¯»å– Excel æ–‡ä»¶: {file_path}")

    # æ–¹æ³•1ï¼šå°è¯•æ ‡å‡† pandas è¯»å–
    try:
        excel_file = pd.ExcelFile(file_path, engine='openpyxl')
        sheet_names = excel_file.sheet_names
        log_manager.log(f"å‘ç° {len(sheet_names)} ä¸ªå·¥ä½œç°¿: {sheet_names}")

        sheets_dict = {}
        for sheet_name in sheet_names:
            df = pd.read_excel(file_path, sheet_name=sheet_name, header=None, engine='openpyxl')
            sheets_dict[sheet_name] = df
            log_manager.log(f"  å·¥ä½œç°¿ '{sheet_name}': {df.shape[0]} è¡Œ x {df.shape[1]} åˆ—")

        return sheets_dict
    except Exception as e:
        log_manager.log(f"æ ‡å‡†è¯»å–å¤±è´¥ï¼Œå°è¯• XML è§£ææ–¹æ³•: {e}")

    # æ–¹æ³•2ï¼šä½¿ç”¨ zipfile + lxml ç›´æ¥è§£æ xlsxï¼ˆå®Œå…¨ç»•è¿‡ openpyxlï¼‰
    try:
        sheets_dict = _parse_xlsx_with_zipfile(file_path)
        sheet_names = list(sheets_dict.keys())
        log_manager.log(f"(XMLè§£æ) å‘ç° {len(sheet_names)} ä¸ªå·¥ä½œç°¿: {sheet_names}")

        for sheet_name, df in sheets_dict.items():
            log_manager.log(f"  å·¥ä½œç°¿ '{sheet_name}': {df.shape[0]} è¡Œ x {df.shape[1]} åˆ—")

        return sheets_dict
    except Exception as e2:
        log_manager.log(f"XML è§£ææ–¹æ³•å¤±è´¥ï¼Œå°è¯• openpyxl åªè¯»æ¨¡å¼: {e2}")

    # æ–¹æ³•3ï¼šä½¿ç”¨ openpyxl ç›´æ¥è¯»å–ï¼ˆåªè¯»æ¨¡å¼ï¼‰
    try:
        from openpyxl import load_workbook

        wb = load_workbook(file_path, data_only=True, read_only=True)
        sheet_names = wb.sheetnames
        log_manager.log(f"(openpyxlåªè¯») å‘ç° {len(sheet_names)} ä¸ªå·¥ä½œç°¿: {sheet_names}")

        sheets_dict = {}
        for sheet_name in sheet_names:
            ws = wb[sheet_name]
            data = []
            for row in ws.iter_rows():
                row_data = [cell.value for cell in row]
                data.append(row_data)

            if data:
                df = pd.DataFrame(data)
            else:
                df = pd.DataFrame()

            sheets_dict[sheet_name] = df
            log_manager.log(f"  å·¥ä½œç°¿ '{sheet_name}': {df.shape[0]} è¡Œ x {df.shape[1]} åˆ—")

        wb.close()
        return sheets_dict
    except Exception as e3:
        log_manager.log_exception(f"è¯»å– Excel æ–‡ä»¶å¤±è´¥ï¼ˆæ‰€æœ‰æ–¹æ³•å‡å¤±è´¥ï¼‰: {e3}", file_path)
        return None


# ==========================================
# === æ–‡æ¡£è¯»å–å‡½æ•° ===
# ==========================================
def get_all_content_elements(doc):
    """è·å–æ–‡æ¡£æ‰€æœ‰å†…å®¹å…ƒç´ """
    body_elements = []
    if hasattr(doc, 'element') and hasattr(doc.element, 'body'):
        for child in doc.element.body.iterchildren():
            if child.tag.endswith('p'):
                body_elements.append(Paragraph(child, doc))
            elif child.tag.endswith('tbl'):
                body_elements.append(Table(child, doc))
    return body_elements


def get_text_count_by_language(text, lang_name):
    """æ ¹æ®è¯­è¨€ç±»å‹ç»Ÿè®¡æ–‡æœ¬å­—æ•°/è¯æ•°"""
    if not text:
        return 0

    lang_info = SUPPORTED_LANGUAGES.get(lang_name)
    if lang_info:
        pattern = lang_info['char_pattern']
        return len(re.findall(pattern, text))

    # å…¼å®¹æ—§çš„ lang_type å‚æ•°
    if lang_name == 'Chinese':
        return len(re.findall(r'[\u4e00-\u9fa5]', text))
    else:
        return len(re.findall(r'\b[a-zA-Z0-9-]+\b', text))


def get_element_text_count(element, lang_type):
    """è·å–å…ƒç´ çš„å­—æ•°/è¯æ•°"""
    text = ""
    if isinstance(element, Paragraph):
        text = element.text
    elif isinstance(element, Table):
        for row in element.rows:
            for cell in row.cells:
                text += cell.text + " "

    return get_text_count_by_language(text, lang_type)


def read_full_docx(file_path):
    """è¯»å–å®Œæ•´çš„docxæ–‡ä»¶å†…å®¹ï¼ŒåŒ…æ‹¬è„šæ³¨ã€å°¾æ³¨ç­‰"""
    try:
        doc = Document(file_path)
        full_text = []
        consecutive_empty = 0  # è®°å½•è¿ç»­ç©ºæ®µè½æ•°

        # 1. æŒ‰æ–‡æ¡£é¡ºåºéå†æ‰€æœ‰å…ƒç´ ï¼ˆæ®µè½å’Œè¡¨æ ¼ï¼‰
        # ä½¿ç”¨ get_all_content_elements çš„æ–¹å¼æ¥ä¿æŒåŸå§‹é¡ºåº
        if hasattr(doc, 'element') and hasattr(doc.element, 'body'):
            for child in doc.element.body.iterchildren():
                if child.tag.endswith('p'):
                    # æ®µè½å…ƒç´ 
                    para = Paragraph(child, doc)
                    if para.text.strip():
                        full_text.append(para.text)
                        consecutive_empty = 0  # é‡ç½®ç©ºæ®µè½è®¡æ•°
                    else:
                        # ç©ºæ®µè½ï¼šæœ€å¤šä¿ç•™ä¸¤ä¸ªè¿ç»­ç©ºè¡Œï¼ˆç”¨äºè¡¨ç¤ºæ®µè½åˆ†éš”ï¼‰
                        consecutive_empty += 1
                        if consecutive_empty <= 2 and full_text:  # åªåœ¨æœ‰å†…å®¹åæ‰æ·»åŠ ç©ºè¡Œ
                            full_text.append("")  # ä¿ç•™ç©ºè¡Œæ ‡è®°
                elif child.tag.endswith('tbl'):
                    consecutive_empty = 0  # é‡åˆ°è¡¨æ ¼é‡ç½®è®¡æ•°
                    # è¡¨æ ¼å…ƒç´ 
                    table = Table(child, doc)
                    seen_cells = set()
                    for row in table.rows:
                        for cell in row.cells:
                            cell_text = cell.text.strip()
                            if cell_text and cell_text not in seen_cells:
                                full_text.append(cell_text)
                                seen_cells.add(cell_text)
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœæ— æ³•ä½¿ç”¨ä¸Šè¿°æ–¹æ³•ï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹å¼
            # 1. æ®µè½æ–‡æœ¬
            consecutive_empty = 0
            for para in doc.paragraphs:
                if para.text.strip():
                    full_text.append(para.text)
                    consecutive_empty = 0
                else:
                    # ç©ºæ®µè½ï¼šæœ€å¤šä¿ç•™ä¸¤ä¸ªè¿ç»­ç©ºè¡Œ
                    consecutive_empty += 1
                    if consecutive_empty <= 2 and full_text:
                        full_text.append("")

            # 2. è¡¨æ ¼å¤„ç†
            for table in doc.tables:
                seen_cells = set()
                for row in table.rows:
                    for cell in row.cells:
                        cell_text = cell.text.strip()
                        if cell_text and cell_text not in seen_cells:
                            full_text.append(cell_text)
                            seen_cells.add(cell_text)

        # 3. é¡µçœ‰é¡µè„š (åŒ…æ‹¬æ®µè½ã€è¡¨æ ¼ã€æ–‡æœ¬æ¡†)
        for section in doc.sections:
            # é¡µçœ‰æ®µè½
            for p in section.header.paragraphs:
                if p.text.strip():
                    full_text.append(p.text)
            # é¡µçœ‰è¡¨æ ¼
            for table in section.header.tables:
                seen_header_cells = set()
                for row in table.rows:
                    for cell in row.cells:
                        cell_text = cell.text.strip()
                        if cell_text and cell_text not in seen_header_cells:
                            full_text.append(cell_text)
                            seen_header_cells.add(cell_text)
            # é¡µçœ‰æ–‡æœ¬æ¡†
            if section.header._element is not None:
                header_xml = etree.tostring(section.header._element, encoding='unicode')
                header_root = etree.fromstring(header_xml.encode('utf-8'))
                nsmap_header = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
                for txbx in header_root.xpath('.//w:txbxContent', namespaces=nsmap_header):
                    txbx_text = ''.join(txbx.xpath('.//w:t/text()', namespaces=nsmap_header))
                    if txbx_text.strip():
                        full_text.append(txbx_text.strip())

            # é¡µè„šæ®µè½
            for p in section.footer.paragraphs:
                if p.text.strip():
                    full_text.append(p.text)
            # é¡µè„šè¡¨æ ¼
            for table in section.footer.tables:
                seen_footer_cells = set()
                for row in table.rows:
                    for cell in row.cells:
                        cell_text = cell.text.strip()
                        if cell_text and cell_text not in seen_footer_cells:
                            full_text.append(cell_text)
                            seen_footer_cells.add(cell_text)
            # é¡µè„šæ–‡æœ¬æ¡†
            if section.footer._element is not None:
                footer_xml = etree.tostring(section.footer._element, encoding='unicode')
                footer_root = etree.fromstring(footer_xml.encode('utf-8'))
                nsmap_footer = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
                for txbx in footer_root.xpath('.//w:txbxContent', namespaces=nsmap_footer):
                    txbx_text = ''.join(txbx.xpath('.//w:t/text()', namespaces=nsmap_footer))
                    if txbx_text.strip():
                        full_text.append(txbx_text.strip())

        # 4. æ–‡æœ¬æ¡†ï¼ˆæ”¯æŒä¼ ç»Ÿæ ¼å¼å’Œ DrawingML æ ¼å¼ï¼‰
        if hasattr(doc.element, 'xml'):
            xml = doc.element.xml
            root = etree.fromstring(xml.encode('utf-8'))

            # æ‰©å±•çš„å‘½åç©ºé—´ï¼Œæ”¯æŒå¤šç§æ–‡æœ¬æ¡†æ ¼å¼
            nsmap = {
                'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main',
                'wp': 'http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing',
                'a': 'http://schemas.openxmlformats.org/drawingml/2006/main',
                'wps': 'http://schemas.microsoft.com/office/word/2010/wordprocessingShape',
                'wpg': 'http://schemas.microsoft.com/office/word/2010/wordprocessingGroup',
                'mc': 'http://schemas.openxmlformats.org/markup-compatibility/2006',
                'w14': 'http://schemas.microsoft.com/office/word/2010/wordml',
            }

            textbox_texts = set()  # ç”¨äºå»é‡

            # æ–¹å¼1: ä¼ ç»Ÿæ–‡æœ¬æ¡† (w:txbxContent) - æŒ‰æ®µè½æå–ä¿ç•™æ¢è¡Œ
            try:
                textbox_containers = root.xpath('.//w:txbxContent', namespaces=nsmap)
                for container in textbox_containers:
                    # æŒ‰æ®µè½æå–ï¼Œä¿ç•™æ®µè½é—´çš„æ¢è¡Œ
                    paragraphs = container.xpath('.//w:p', namespaces=nsmap)
                    para_texts = []
                    for p in paragraphs:
                        # åˆå¹¶æ®µè½å†…æ‰€æœ‰ <w:t> èŠ‚ç‚¹çš„æ–‡æœ¬
                        p_text = ''.join([t.text for t in p.xpath('.//w:t', namespaces=nsmap) if t.text])
                        if p_text.strip():
                            para_texts.append(p_text.strip())
                    merged_text = '\n'.join(para_texts)
                    if merged_text.strip():
                        textbox_texts.add(merged_text.strip())
            except:
                pass

            # æ–¹å¼2: DrawingML æ–‡æœ¬æ¡† (wps:txbx) - æŒ‰æ®µè½æå–ä¿ç•™æ¢è¡Œ
            try:
                textbox_containers = root.xpath('.//wps:txbx', namespaces=nsmap)
                for container in textbox_containers:
                    # DrawingML ä½¿ç”¨ <a:p> ä½œä¸ºæ®µè½
                    paragraphs = container.xpath('.//a:p', namespaces=nsmap)
                    para_texts = []
                    for p in paragraphs:
                        # åˆå¹¶æ®µè½å†…æ‰€æœ‰ <a:t> èŠ‚ç‚¹çš„æ–‡æœ¬
                        p_text = ''.join([t.text for t in p.xpath('.//a:t', namespaces=nsmap) if t.text])
                        if p_text.strip():
                            para_texts.append(p_text.strip())
                    merged_text = '\n'.join(para_texts)
                    if merged_text.strip():
                        textbox_texts.add(merged_text.strip())
            except:
                pass

            # æ–¹å¼3: DrawingML å½¢çŠ¶ä¸­çš„æ–‡æœ¬ (a:txBody) - æŒ‰æ®µè½æå–ä¿ç•™æ¢è¡Œ
            try:
                shape_containers = root.xpath('.//a:txBody', namespaces=nsmap)
                for container in shape_containers:
                    # DrawingML ä½¿ç”¨ <a:p> ä½œä¸ºæ®µè½
                    paragraphs = container.xpath('.//a:p', namespaces=nsmap)
                    para_texts = []
                    for p in paragraphs:
                        # åˆå¹¶æ®µè½å†…æ‰€æœ‰ <a:t> èŠ‚ç‚¹çš„æ–‡æœ¬
                        p_text = ''.join([t.text for t in p.xpath('.//a:t', namespaces=nsmap) if t.text])
                        if p_text.strip():
                            para_texts.append(p_text.strip())
                    merged_text = '\n'.join(para_texts)
                    if merged_text.strip():
                        textbox_texts.add(merged_text.strip())
            except:
                pass

            # æ–¹å¼4: Word 2010+ æ ¼å¼çš„æ–‡æœ¬æ¡† (wps:wsp//wps:txbx) - æŒ‰æ®µè½æå–ä¿ç•™æ¢è¡Œ
            try:
                textbox_containers = root.xpath('.//wps:wsp//wps:txbx', namespaces=nsmap)
                for container in textbox_containers:
                    # æŒ‰æ®µè½æå–ï¼Œä¿ç•™æ®µè½é—´çš„æ¢è¡Œ
                    paragraphs = container.xpath('.//w:p', namespaces=nsmap)
                    para_texts = []
                    for p in paragraphs:
                        # åˆå¹¶æ®µè½å†…æ‰€æœ‰ <w:t> èŠ‚ç‚¹çš„æ–‡æœ¬
                        p_text = ''.join([t.text for t in p.xpath('.//w:t', namespaces=nsmap) if t.text])
                        if p_text.strip():
                            para_texts.append(p_text.strip())
                    merged_text = '\n'.join(para_texts)
                    if merged_text.strip():
                        textbox_texts.add(merged_text.strip())
            except:
                pass

            # æ·»åŠ æ‰€æœ‰æ‰¾åˆ°çš„æ–‡æœ¬æ¡†å†…å®¹
            for text in textbox_texts:
                full_text.append(text)

        # 5. è„šæ³¨ã€å°¾æ³¨
        if hasattr(doc, 'part'):
            for rel in doc.part.rels.values():
                ref = rel.target_ref
                nsmap = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}

                if "footnotes" in ref:
                    try:
                        root = etree.fromstring(rel.target_part.blob)
                        for fn in root.xpath('.//w:footnote', namespaces=nsmap):
                            fn_type = fn.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}type')
                            if fn_type in ['separator', 'continuationSeparator']:
                                continue
                            fn_id = fn.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id')
                            fn_texts = []
                            for p in fn.xpath('.//w:p', namespaces=nsmap):
                                p_text = ''.join([t.text for t in p.xpath('.//w:t', namespaces=nsmap) if t.text])
                                if p_text.strip():
                                    fn_texts.append(p_text.strip())
                            fn_text = ' '.join(fn_texts)
                            if fn_text.strip():
                                full_text.append(fn_text.strip())
                    except:
                        pass

                elif "endnotes" in ref:
                    try:
                        root = etree.fromstring(rel.target_part.blob)
                        for en in root.xpath('.//w:endnote', namespaces=nsmap):
                            en_type = en.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}type')
                            if en_type in ['separator', 'continuationSeparator']:
                                continue
                            en_id = en.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id')
                            en_texts = []
                            for p in en.xpath('.//w:p', namespaces=nsmap):
                                p_text = ''.join([t.text for t in p.xpath('.//w:t', namespaces=nsmap) if t.text])
                                if p_text.strip():
                                    en_texts.append(p_text.strip())
                            en_text = ' '.join(en_texts)
                            if en_text.strip():
                                full_text.append(en_text.strip())
                    except:
                        pass

        result = "\n".join(full_text)

        # å‹ç¼©å¤šä½™çš„è¿ç»­ç©ºè¡Œï¼ˆæœ€å¤šä¿ç•™2ä¸ªè¿ç»­æ¢è¡Œï¼Œå³1ä¸ªç©ºè¡Œï¼‰
        import re
        result = re.sub(r'\n{4,}', '\n\n\n', result)  # 3ä¸ª\n = 2ä¸ªç©ºè¡Œï¼Œå·²ç»è¶³å¤Ÿè¡¨ç¤ºæ®µè½åˆ†éš”

        if not result.strip():
            log_manager.log_exception("æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼", f"æ–‡ä»¶: {file_path}")
        return result

    except Exception as e:
        log_manager.log_exception(f"è¯»å–å†…å®¹å¤±è´¥: {e}", f"æ–‡ä»¶: {file_path}")
        return ""


def _iter_group_shapes(group_shape, base_top=0, base_left=0):
    """å±•å¼€ç»„åˆå½¢çŠ¶ï¼Œè¿”å› (top, left, text)"""
    for sub in group_shape.shapes:
        top = base_top + (sub.top or 0)
        left = base_left + (sub.left or 0)

        if sub.shape_type == MSO_SHAPE_TYPE.GROUP:
            yield from _iter_group_shapes(sub, top, left)
            continue

        if hasattr(sub, "has_text_frame") and sub.has_text_frame:
            # æŒ‰æ®µè½æå–ï¼Œä¿ç•™æ¢è¡Œ
            para_texts = []
            for paragraph in sub.text_frame.paragraphs:
                para_text = paragraph.text.strip()
                if para_text:
                    para_texts.append(para_text)
            if para_texts:
                txt = '\n'.join(para_texts)
                yield (top, left, txt)

        if hasattr(sub, "has_table") and sub.has_table:
            for r_idx, row in enumerate(sub.table.rows):
                for c_idx, cell in enumerate(row.cells):
                    # è¡¨æ ¼å•å…ƒæ ¼ä¹ŸæŒ‰æ®µè½æå–
                    para_texts = []
                    for paragraph in cell.text_frame.paragraphs:
                        para_text = paragraph.text.strip()
                        if para_text:
                            para_texts.append(para_text)
                    if para_texts:
                        txt = '\n'.join(para_texts)
                        yield (top + r_idx * 1_000, left + c_idx * 1_000, txt)


def _extract_slide_items(slide):
    """æå–å•é¡µå¹»ç¯ç‰‡çš„æ‰€æœ‰æ–‡æœ¬é¡¹ï¼Œè¿”å›å·²æ’åºçš„æ–‡æœ¬åˆ—è¡¨"""
    items = []
    for shape in slide.shapes:
        top = shape.top or 0
        left = shape.left or 0

        if hasattr(shape, "has_text_frame") and shape.has_text_frame:
            # æŒ‰æ®µè½æå–ï¼Œä¿ç•™æ¢è¡Œ
            para_texts = []
            for paragraph in shape.text_frame.paragraphs:
                para_text = paragraph.text.strip()
                if para_text:
                    para_texts.append(para_text)
            if para_texts:
                txt = '\n'.join(para_texts)
                items.append((top, left, txt))

        if hasattr(shape, "has_table") and shape.has_table:
            for r_idx, row in enumerate(shape.table.rows):
                for c_idx, cell in enumerate(row.cells):
                    # è¡¨æ ¼å•å…ƒæ ¼ä¹ŸæŒ‰æ®µè½æå–
                    para_texts = []
                    for paragraph in cell.text_frame.paragraphs:
                        para_text = paragraph.text.strip()
                        if para_text:
                            para_texts.append(para_text)
                    if para_texts:
                        txt = '\n'.join(para_texts)
                        items.append((top + r_idx * 1_000, left + c_idx * 1_000, txt))

        if shape.shape_type == MSO_SHAPE_TYPE.GROUP:
            items.extend(_iter_group_shapes(shape, top, left))

    items.sort(key=lambda t: (t[0] // ROW_BUCKET, t[1]))
    return [t[2] for t in items]


def read_full_pptx(file_path):
    """è¯»å–PPTï¼ŒæŒ‰æ˜¾ç¤ºé¡ºåºæå–æ‰€æœ‰æ–‡æœ¬"""
    try:
        prs = Presentation(file_path)
        all_lines = []
        for slide_idx, slide in enumerate(prs.slides, start=1):
            all_lines.append(f"---- å¹»ç¯ç‰‡ {slide_idx} ----")
            all_lines.extend(_extract_slide_items(slide))
        result = "\n".join(all_lines)
        if not result.strip():
            log_manager.log_exception("PPTå†…å®¹ä¸ºç©ºï¼", f"æ–‡ä»¶: {file_path}")
        return result
    except Exception as e:
        log_manager.log_exception(f"è¯»å–PPTå¤±è´¥: {e}", f"æ–‡ä»¶: {file_path}")
        return ""


def read_full_excel(file_path):
    """è¯»å– Excel æ–‡ä»¶çš„æ‰€æœ‰å†…å®¹"""
    try:
        df = pd.read_excel(file_path)
        texts = []
        for col in df.columns:
            texts.append(f"[åˆ—: {col}]")
            for val in df[col].dropna():
                if str(val).strip():
                    texts.append(str(val).strip())
        result = "\n".join(texts)
        if not result.strip():
            log_manager.log_exception("Excelå†…å®¹ä¸ºç©ºï¼", f"æ–‡ä»¶: {file_path}")
        return result
    except Exception as e:
        log_manager.log_exception(f"è¯»å–Excelå¤±è´¥: {e}", f"æ–‡ä»¶: {file_path}")
        return ""


def read_file_content(file_path):
    """ç»Ÿä¸€çš„æ–‡ä»¶è¯»å–æ¥å£"""
    file_type = get_file_type(file_path)
    if file_type == 'docx':
        return read_full_docx(file_path)
    elif file_type == 'pptx':
        return read_full_pptx(file_path)
    elif file_type == 'excel':
        return read_full_excel(file_path)
    elif file_type == 'doc':
        log_manager.log_exception(".doc æ–‡ä»¶éœ€è¦å…ˆè½¬æ¢", file_path)
        return ""
    else:
        log_manager.log_exception(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹", file_path)
        return ""


# ==========================================
# === æ–‡æ¡£åˆ†æä¸åˆ†å‰² ===
# ==========================================
def analyze_document_structure(doc_path, lang_name):
    """åˆ†ææ–‡æ¡£ç»“æ„ï¼Œæ”¯æŒå¤šè¯­è¨€"""
    file_type = get_file_type(doc_path)

    if file_type == 'pptx':
        text = read_full_pptx(doc_path)
        count = get_text_count_by_language(text, lang_name)
        return count, 0
    elif file_type == 'excel':
        df = read_excel_file(doc_path)
        if df is None:
            return 0, 0
        text = df.to_string()
        count = get_text_count_by_language(text, lang_name)
        return count, len(df)
    elif file_type == 'doc':
        log_manager.log_exception(".doc æ–‡ä»¶éœ€è¦å…ˆè½¬æ¢ä¸º .docx")
        return 0, 0
    else:
        doc = Document(doc_path)
        elements = get_all_content_elements(doc)
        total_count = sum(get_element_text_count(el, lang_name) for el in elements)
        return total_count, len(elements)


def extract_text_from_elements(elements, start_idx, end_idx):
    """ä»å…ƒç´ ä¸­æå–æ–‡æœ¬"""
    texts = []
    for i in range(start_idx, min(end_idx, len(elements))):
        elem = elements[i]
        if isinstance(elem, Paragraph):
            if elem.text.strip():
                texts.append(elem.text.strip())
        elif isinstance(elem, Table):
            for row in elem.rows:
                row_texts = [cell.text.strip() for cell in row.cells if cell.text.strip()]
                if row_texts:
                    texts.append(" | ".join(row_texts))
    return "\n".join(texts)


def delete_elements_in_range(doc, start_idx, end_idx):
    """åˆ é™¤æŒ‡å®šèŒƒå›´çš„å…ƒç´ """
    all_elements = get_all_content_elements(doc)
    elements_to_delete = []
    total = len(all_elements)

    for i in range(total):
        if start_idx <= i < end_idx:
            if isinstance(all_elements[i], Paragraph):
                elements_to_delete.append(all_elements[i]._element)
            elif isinstance(all_elements[i], Table):
                elements_to_delete.append(all_elements[i]._element)

    for el in elements_to_delete:
        parent = el.getparent()
        if parent is not None:
            parent.remove(el)


def find_element_index_by_char_count(element_counts, target_chars):
    """äºŒåˆ†æŸ¥æ‰¾ï¼šæ ¹æ®ç›®æ ‡å­—æ•°æ‰¾åˆ°æœ€æ¥è¿‘çš„å…ƒç´ ç´¢å¼•

    ä¿®å¤ï¼šåŸé€»è¾‘åªè¿”å›ç´¯è®¡å­—æ•° < target çš„æœ€åä¸€ä¸ªå…ƒç´ ï¼Œ
    å½“å­˜åœ¨è¶…å¤§å…ƒç´ ï¼ˆå¦‚å¤§è¡¨æ ¼ï¼‰æ—¶ï¼Œå¤šä¸ªç›®æ ‡å€¼ä¼šæ˜ å°„åˆ°åŒä¸€ç´¢å¼•ã€‚
    ç°åœ¨é¢å¤–æ¯”è¾ƒä¸‹ä¸€ä¸ªå…ƒç´ ï¼Œè¿”å›ç´¯è®¡å­—æ•°æ›´æ¥è¿‘ target çš„é‚£ä¸ªã€‚
    """
    left, right = 0, len(element_counts) - 1
    best_idx = 0
    while left <= right:
        mid = (left + right) // 2
        if element_counts[mid] < target_chars:
            best_idx = mid
            left = mid + 1
        else:
            right = mid - 1
    # æ£€æŸ¥ä¸‹ä¸€ä¸ªå…ƒç´ æ˜¯å¦æ›´æ¥è¿‘ç›®æ ‡å­—æ•°
    if best_idx + 1 < len(element_counts):
        diff_before = target_chars - element_counts[best_idx]
        diff_after = element_counts[best_idx + 1] - target_chars
        if diff_after < diff_before:
            return best_idx + 1
    return best_idx


def _find_buffer_end(element_counts, split_idx, buffer_chars, direction='right'):
    """åŸºäºå­—æ•°ç²¾ç¡®è®¡ç®—ç¼“å†²åŒºè¾¹ç•Œï¼ˆè€Œéç”¨å…¨å±€å¹³å‡ä¼°ç®—å…ƒç´ æ•°ï¼‰

    direction='right': ä» split_idx å‘å³æ‰©å±• buffer_chars å­—ï¼Œè¿”å›ç»“æŸç´¢å¼•
    direction='left':  ä» split_idx å‘å·¦æ‰©å±• buffer_chars å­—ï¼Œè¿”å›å¼€å§‹ç´¢å¼•
    """
    n = len(element_counts)
    if n == 0:
        return split_idx

    if direction == 'right':
        base_chars = element_counts[split_idx] if split_idx < n else element_counts[-1]
        target = base_chars + buffer_chars
        for i in range(split_idx + 1, n):
            if element_counts[i] >= target:
                return i + 1  # +1 å› ä¸º end æ˜¯å¼€åŒºé—´
        return n
    else:  # left
        base_chars = element_counts[split_idx] if split_idx < n else element_counts[-1]
        target = base_chars - buffer_chars
        if target <= 0:
            return 0
        for i in range(split_idx - 1, -1, -1):
            if element_counts[i] <= target:
                return i
        return 0


def smart_split_with_buffer(src_path, num_parts, output_dir, lang_type, buffer_chars=2000,
                            split_element_ratios=None):
    """æ™ºèƒ½åˆ†å‰²æ–‡æ¡£ï¼šæŒ‰å­—æ•°å‡åˆ† + ç¼“å†²åŒºé‡å 

    Args:
        split_element_ratios: å¯é€‰ï¼Œç”±ä¸»æ–‡æ¡£è®¡ç®—å‡ºçš„åˆ†å‰²æ¯”ä¾‹åˆ—è¡¨ï¼ˆå…ƒç´ ä½ç½®å æ¯”ï¼‰ã€‚
                              æä¾›æ—¶ï¼Œæœ¬æ–‡æ¡£æŒ‰ç›¸åŒæ¯”ä¾‹åˆ†å‰²ï¼Œç¡®ä¿åŸæ–‡/è¯‘æ–‡å†…å®¹å¯¹é½ã€‚
    Returns:
        (generated_files, part_info, element_ratios)
        element_ratios: ç†æƒ³åˆ†å‰²ç‚¹çš„å…ƒç´ ä½ç½®æ¯”ä¾‹ï¼Œå¯ä¼ ç»™å¦ä¸€æ–‡æ¡£ä»¥ä¿æŒåŒæ­¥ã€‚
    """
    doc = Document(src_path)
    elements = get_all_content_elements(doc)
    base_name = os.path.splitext(os.path.basename(src_path))[0]

    # è®¡ç®—æ¯ä¸ªå…ƒç´ çš„ç´¯è®¡å­—æ•°
    element_counts = []
    cumulative_count = 0
    for elem in elements:
        count = get_element_text_count(elem, lang_type)
        cumulative_count += count
        element_counts.append(cumulative_count)

    total_count = cumulative_count
    if total_count == 0:
        log_manager.log_exception("æ–‡æ¡£å­—æ•°ä¸º0ï¼Œæ— æ³•åˆ†å‰²")
        return [], [], []

    target_per_part = total_count // num_parts

    log_manager.log(f"æ€»å­—æ•°: {total_count:,}, ç›®æ ‡æ¯ä»½: {target_per_part:,}, ç¼“å†²åŒº: {buffer_chars:,} å­—")

    # è®¡ç®—ç†æƒ³åˆ†å‰²ç‚¹
    if split_element_ratios is not None:
        # ä»ä¸»æ–‡æ¡£çš„å…ƒç´ æ¯”ä¾‹æ˜ å°„åˆ°æœ¬æ–‡æ¡£çš„å…ƒç´ ç´¢å¼•
        log_manager.log(f"ä½¿ç”¨ä¸»æ–‡æ¡£åˆ†å‰²æ¯”ä¾‹: {[f'{r:.4f}' for r in split_element_ratios]}")
        ideal_splits = []
        for ratio in split_element_ratios:
            idx = max(0, min(int(ratio * len(elements)), len(elements) - 1))
            ideal_splits.append(idx)
    else:
        # è‡ªä¸»è®¡ç®—ï¼ˆä½œä¸ºä¸»æ–‡æ¡£ï¼‰
        ideal_splits = []
        for i in range(1, num_parts):
            target_chars = target_per_part * i
            split_idx = find_element_index_by_char_count(element_counts, target_chars)
            ideal_splits.append(split_idx)

    # ç¡®ä¿åˆ†å‰²ç‚¹ä¸¥æ ¼é€’å¢ï¼ˆé¿å…å¤§å…ƒç´ å¯¼è‡´å¤šä¸ªåˆ†å‰²ç‚¹é‡å ï¼‰
    for i in range(1, len(ideal_splits)):
        if ideal_splits[i] <= ideal_splits[i - 1]:
            ideal_splits[i] = ideal_splits[i - 1] + 1
    # ç¡®ä¿ä¸è¶Šç•Œ
    for i in range(len(ideal_splits)):
        ideal_splits[i] = min(ideal_splits[i], len(elements) - 1)

    # è®¡ç®—å…ƒç´ ä½ç½®æ¯”ä¾‹ï¼ˆä¾›å¦ä¸€æ–‡æ¡£ä½¿ç”¨ï¼‰
    element_ratios = [idx / len(elements) for idx in ideal_splits] if elements else []

    log_manager.log(f"ç†æƒ³åˆ†å‰²ç‚¹ç´¢å¼•: {ideal_splits}")
    for i, idx in enumerate(ideal_splits):
        chars_at_split = element_counts[idx] if idx < len(element_counts) else total_count
        log_manager.log(f"  åˆ†å‰²ç‚¹{i + 1}: å…ƒç´ [{idx}]/{len(elements)}, ç´¯è®¡å­—æ•°: {chars_at_split:,}")

    # ç”Ÿæˆå¸¦ç¼“å†²çš„åˆ†å‰²èŒƒå›´ï¼ˆåŸºäºå­—æ•°ç²¾ç¡®è®¡ç®—ç¼“å†²åŒºï¼Œè€Œéå…¨å±€å¹³å‡ï¼‰
    split_ranges = []
    for part_idx in range(num_parts):
        if part_idx == 0:
            start = 0
            if ideal_splits:
                end = _find_buffer_end(element_counts, ideal_splits[0], buffer_chars, 'right')
            else:
                end = len(elements)
        elif part_idx == num_parts - 1:
            start = _find_buffer_end(element_counts, ideal_splits[-1], buffer_chars, 'left')
            end = len(elements)
        else:
            start = _find_buffer_end(element_counts, ideal_splits[part_idx - 1], buffer_chars, 'left')
            end = _find_buffer_end(element_counts, ideal_splits[part_idx], buffer_chars, 'right')

        # å®‰å…¨è£å‰ª
        start = max(0, min(start, len(elements) - 1))
        end = max(start + 1, min(end, len(elements)))
        split_ranges.append((start, end))

    for i, (s, e) in enumerate(split_ranges):
        part_chars = element_counts[min(e, len(element_counts)) - 1] - (
            element_counts[s - 1] if s > 0 else 0) if e > s else 0
        log_manager.log(f"  Part{i + 1}: å…ƒç´ [{s}:{e}], çº¦ {part_chars:,} å­—")

    # ç”Ÿæˆåˆ†å‰²åçš„æ–‡ä»¶
    generated_files = []
    part_info = []

    for i, (start_idx, end_idx) in enumerate(split_ranges):
        part_num = i + 1
        dest_filename = f"{base_name}_Part{part_num}.docx"
        dest_path = os.path.join(output_dir, dest_filename)

        shutil.copy2(src_path, dest_path)
        doc_copy = Document(dest_path)

        total_elems = len(get_all_content_elements(doc_copy))
        delete_elements_in_range(doc_copy, end_idx, total_elems + 5000)
        delete_elements_in_range(doc_copy, 0, start_idx)
        doc_copy.save(dest_path)

        first_text = extract_text_from_elements(elements, start_idx, min(start_idx + 3, end_idx))
        last_text = extract_text_from_elements(elements, max(start_idx, end_idx - 3), end_idx)

        part_info.append({
            'path': dest_path,
            'first_anchor': first_text[:200] if first_text else "",
            'last_anchor': last_text[-200:] if last_text else "",
            'start_idx': start_idx,
            'end_idx': end_idx
        })

        generated_files.append(dest_path)
        log_manager.log(f"ç”Ÿæˆ: {dest_filename}")

    return generated_files, part_info, element_ratios


# ==========================================
# === ğŸ” å¯¹é½è´¨é‡æ£€æŸ¥å™¨ ===
# ==========================================
class AlignmentChecker:
    @staticmethod
    def check_language_consistency(df, source_lang="ä¸­æ–‡", target_lang="è‹±è¯­"):
        """æ£€æŸ¥è¯­è¨€ä¸€è‡´æ€§ï¼Œæ”¯æŒå¤šè¯­è¨€"""
        issues = []

        source_info = SUPPORTED_LANGUAGES.get(source_lang, SUPPORTED_LANGUAGES["ä¸­æ–‡"])
        target_info = SUPPORTED_LANGUAGES.get(target_lang, SUPPORTED_LANGUAGES["è‹±è¯­"])

        source_pattern = source_info['char_pattern']
        target_pattern = target_info['char_pattern']

        for idx, row in df.iterrows():
            original = str(row.get('åŸæ–‡', ''))
            trans = str(row.get('è¯‘æ–‡', ''))

            if not original or not trans:
                continue

            # è®¡ç®—åŸæ–‡ä¸­æºè¯­è¨€å­—ç¬¦çš„æ¯”ä¾‹
            source_chars_in_original = len(re.findall(source_pattern, original))
            target_chars_in_original = len(re.findall(target_pattern, original))

            # è®¡ç®—è¯‘æ–‡ä¸­ç›®æ ‡è¯­è¨€å­—ç¬¦çš„æ¯”ä¾‹
            source_chars_in_trans = len(re.findall(source_pattern, trans))
            target_chars_in_trans = len(re.findall(target_pattern, trans))

            total_original = len(original) if original else 1
            total_trans = len(trans) if trans else 1

            # æ£€æµ‹æ˜¯å¦å­˜åœ¨è¯­è¨€é”™ä½
            source_ratio_original = source_chars_in_original / total_original
            target_ratio_trans = target_chars_in_trans / total_trans

            # å¦‚æœåŸæ–‡ä¸­æºè¯­è¨€å æ¯”å¾ˆä½ï¼Œè€Œè¯‘æ–‡ä¸­æºè¯­è¨€å æ¯”å¾ˆé«˜ï¼Œå¯èƒ½æ˜¯é”™ä½
            source_ratio_trans = source_chars_in_trans / total_trans
            target_ratio_original = target_chars_in_original / total_original

            if source_ratio_original < 0.2 and source_ratio_trans > 0.4:
                issues.append({
                    'row': idx + 1, 'type': 'è¯­è¨€é”™ä½',
                    'detail': f'åŸæ–‡ç–‘ä¼¼{target_lang}ï¼Œè¯‘æ–‡ç–‘ä¼¼{source_lang}',
                    'original_text': original, 'trans_text': trans
                })
            elif target_ratio_original > 0.4 and target_ratio_trans < 0.2:
                issues.append({
                    'row': idx + 1, 'type': 'è¯­è¨€é”™ä½',
                    'detail': f'åŸæ–‡ç–‘ä¼¼{target_lang}ï¼Œè¯‘æ–‡ç–‘ä¼¼{source_lang}',
                    'original_text': original, 'trans_text': trans
                })
        return issues

    @staticmethod
    def check_length_anomaly(df, threshold_ratio=5):
        """æ£€æŸ¥é•¿åº¦å¼‚å¸¸"""
        issues = []
        for idx, row in df.iterrows():
            original = str(row.get('åŸæ–‡', ''))
            trans = str(row.get('è¯‘æ–‡', ''))

            len_orig = len(original)
            len_trans = len(trans)

            if len_orig == 0 or len_trans == 0:
                continue

            ratio = max(len_orig, len_trans) / min(len_orig, len_trans)
            if ratio > threshold_ratio:
                issues.append({
                    'row': idx + 1, 'type': 'é•¿åº¦å¼‚å¸¸',
                    'detail': f'é•¿åº¦æ¯” {ratio:.1f}:1',
                    'original_text': original, 'trans_text': trans
                })
        return issues

    @staticmethod
    def full_check(df, source_lang="ä¸­æ–‡", target_lang="è‹±è¯­"):
        """æ‰§è¡Œå®Œæ•´æ£€æŸ¥"""
        all_issues = []
        all_issues.extend(AlignmentChecker.check_language_consistency(df, source_lang, target_lang))
        all_issues.extend(AlignmentChecker.check_length_anomaly(df))
        return all_issues


def save_issues_report(issues, output_path):
    if not issues:
        return
    report_data = [{
        'è¡Œå·': issue.get('row', ''),
        'é—®é¢˜ç±»å‹': issue.get('type', ''),
        'é—®é¢˜è¯¦æƒ…': issue.get('detail', ''),
        'åŸæ–‡': issue.get('original_text', ''),
        'è¯‘æ–‡': issue.get('trans_text', '')
    } for issue in issues]
    pd.DataFrame(report_data).to_excel(output_path, index=False)
    log_manager.log(f"é—®é¢˜æŠ¥å‘Šå·²ä¿å­˜: {output_path}")


# ==========================================
# === ğŸ¤– æ ¸å¿ƒ AI å¯¹é½ ===
# ==========================================
def call_openrouter_stream(system_prompt, user_prompt, model_id, max_output_tokens, filename=""):
    """OpenRouter API æµå¼è°ƒç”¨"""
    client = OpenAI(base_url=BASE_URL, api_key=API_KEY)

    try:
        log_manager.log("è¯·æ±‚ OpenRouter API...")
        stream = client.chat.completions.create(
            model=model_id,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.1,
            max_tokens=max_output_tokens,
            stream=True,
            timeout=600.0,
            extra_headers={"HTTP-Referer": "local-debug", "X-Title": "Doc-Aligner"},
        )

        full_response_text = ""
        log_manager.log("æ¥æ”¶æ•°æ®æµ...")
        log_manager.log_stream("\n" + "=" * 50 + f" {filename} " + "=" * 50 + "\n")

        for chunk in stream:
            if not chunk.choices:
                continue
            delta = chunk.choices[0].delta
            if hasattr(delta, "content") and delta.content:
                content = delta.content
                log_manager.log_stream(content)
                full_response_text += content

        log_manager.log_stream("\n" + "=" * 50 + " è¾“å‡ºç»“æŸ " + "=" * 50 + "\n")
        return full_response_text

    except Exception as e:
        log_manager.log_exception(f"OpenRouter APIè°ƒç”¨å¤±è´¥", str(e))
        import traceback
        log_manager.log_exception("è¯¦ç»†å †æ ˆ", traceback.format_exc())
        return None


async def call_luzhishen_stream_async(system_prompt, user_prompt, model_id, filename=""):
    """Luzhishen WebSocket API æµå¼è°ƒç”¨ï¼ˆå¼‚æ­¥ï¼‰"""
    try:
        # è·å–å…¬é’¥å¹¶ç”ŸæˆåŠ å¯†çš„ API Key
        pubkey = get_luzhishen_pubkey()
        if pubkey is None:
            log_manager.log_exception("æ— æ³•è·å– Luzhishen å…¬é’¥")
            return None

        api_key_rsa = generate_luzhishen_api_key(LUZHISHEN_API_KEY, pubkey)
        uri = f"{LUZHISHEN_WS_BASE_URL}?apikey={api_key_rsa}&model={model_id}"

        log_manager.log(f"è¯·æ±‚ Luzhishen WebSocket API...")
        log_manager.log(f"æ¨¡å‹: {model_id}")

        data = {
            "model": model_id,
            "data": {
                "system_prompt": system_prompt,
                "messages": [
                    {"role": "user", "content": user_prompt}
                ],
                "seed": 12345
            }
        }

        full_response_text = ""
        log_manager.log_stream("\n" + "=" * 50 + f" {filename} " + "=" * 50 + "\n")

        async with websockets.connect(uri, ping_interval=30, ping_timeout=60) as websocket:
            # æ¥æ”¶éªŒè¯ API Key å’Œä½™é¢çš„å“åº”
            response = await asyncio.wait_for(websocket.recv(), timeout=30)
            log_manager.log("WebSocket è¿æ¥å·²å»ºç«‹ï¼Œå‘é€è¯·æ±‚...")

            # å‘é€è¯·æ±‚æ•°æ®
            await websocket.send(json.dumps(data))
            log_manager.log("æ¥æ”¶æ•°æ®æµ...")

            while True:
                try:
                    response = await asyncio.wait_for(websocket.recv(), timeout=300)
                    response_data = json.loads(response)

                    # å¤„ç†é”™è¯¯
                    code = response_data.get("code")
                    if code == 408 or code == 500:
                        error_msg = response_data.get("message", "æœªçŸ¥é”™è¯¯")
                        log_manager.log_exception(f"Luzhishen API é”™è¯¯ (code={code})", error_msg)
                        return None

                    # å¤„ç†æµå¼å“åº”ç‰‡æ®µ (code == 200)
                    if code == 200:
                        message_fragment = response_data.get("message", "")
                        # ç¡®ä¿ message_fragment æ˜¯å­—ç¬¦ä¸²ç±»å‹
                        if isinstance(message_fragment, dict):
                            message_fragment = str(message_fragment)
                        elif message_fragment is None:
                            message_fragment = ""
                        else:
                            message_fragment = str(message_fragment)
                        full_response_text += message_fragment
                        log_manager.log_stream(message_fragment)

                    # å¤„ç†å®Œæˆå“åº” (code == 205)
                    if code == 205:
                        summary_response = response_data.get("message", "")
                        if isinstance(summary_response, dict):
                            summary_response = str(summary_response)
                        elif summary_response is None:
                            summary_response = ""
                        else:
                            summary_response = str(summary_response)
                        if summary_response:
                            full_response_text += summary_response
                            log_manager.log_stream(summary_response)
                        break

                except asyncio.TimeoutError:
                    log_manager.log_exception("WebSocket æ¥æ”¶è¶…æ—¶")
                    break

        log_manager.log_stream("\n" + "=" * 50 + " è¾“å‡ºç»“æŸ " + "=" * 50 + "\n")

        # æ¸…ç†ç”¨é‡ç»Ÿè®¡ä¿¡æ¯
        cleaned_response = clean_luzhishen_response(full_response_text)
        return cleaned_response

    except websockets.exceptions.ConnectionClosed as e:
        log_manager.log_exception(f"WebSocket è¿æ¥å…³é—­", str(e))
        return None
    except Exception as e:
        log_manager.log_exception(f"Luzhishen WebSocket APIè°ƒç”¨å¤±è´¥", str(e))
        import traceback
        log_manager.log_exception("è¯¦ç»†å †æ ˆ", traceback.format_exc())
        return None


def call_luzhishen_stream(system_prompt, user_prompt, model_id, filename=""):
    """Luzhishen WebSocket API æµå¼è°ƒç”¨ï¼ˆåŒæ­¥åŒ…è£…ï¼‰"""
    try:
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = None

        if loop is not None:
            # å¦‚æœå·²æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çº¿ç¨‹è¿è¡Œ
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(
                    asyncio.run,
                    call_luzhishen_stream_async(system_prompt, user_prompt, model_id, filename)
                )
                return future.result(timeout=600)
        else:
            # å¦‚æœæ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥è¿è¡Œ
            return asyncio.run(call_luzhishen_stream_async(system_prompt, user_prompt, model_id, filename))
    except Exception as e:
        log_manager.log_exception(f"Luzhishen è°ƒç”¨å¤±è´¥", str(e))
        import traceback
        log_manager.log_exception("è¯¦ç»†å †æ ˆ", traceback.format_exc())
        return None


def call_llm_stream(system_prompt, user_prompt, model_id, filename=""):
    """ç»Ÿä¸€çš„LLMæµå¼è°ƒç”¨ - è‡ªåŠ¨æ£€æµ‹æä¾›å•†"""
    # è·å–æ¨¡å‹çš„æœ€å¤§è¾“å‡º tokens å’Œæä¾›å•†
    max_output_tokens = 65536
    provider = "openrouter"  # é»˜è®¤æä¾›å•†

    for model_info in AVAILABLE_MODELS.values():
        if model_info['id'] == model_id:
            max_output_tokens = model_info['max_output']
            provider = model_info.get('provider', 'openrouter')
            break

    log_manager.log(f"ä½¿ç”¨æä¾›å•†: {provider}")

    if provider == "luzhishen":
        return call_luzhishen_stream(system_prompt, user_prompt, model_id, filename)
    else:
        return call_openrouter_stream(system_prompt, user_prompt, model_id, max_output_tokens, filename)


def parse_alignment_response(response_text):
    """è§£æå¯¹é½å“åº”"""
    if not response_text:
        return []

    cleaned_text = response_text.replace('\r\n', '\n').replace('\r', '\n')
    try:
        # æ³¨é‡Šæ‰ä¼šå¯¼è‡´å¤šè¡Œåˆå¹¶çš„æ­£åˆ™ï¼Œä¿ç•™æ¯ä¸€è¡Œçš„ç‹¬ç«‹æ€§
        # cleaned_text = re.sub(r'\s*\|\|\|\s*\n+\s*', ' ||| ', cleaned_text)
        # cleaned_text = re.sub(r'\n+\s*\|\|\|\s*', ' ||| ', cleaned_text)
        cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)
    except Exception as e:
        log_manager.log_exception(f"æ­£åˆ™æ›¿æ¢å¤±è´¥", str(e))

    lines = cleaned_text.splitlines()
    data = []
    pending_line = ""

    for line_num, line in enumerate(lines, 1):
        line = line.strip()
        if not line or line.startswith('```'):
            continue

        if pending_line:
            line = pending_line + " " + line
            pending_line = ""

        if "|||" in line:
            parts = line.split("|||", 1)
            if len(parts) >= 2:
                original = parts[0].strip()
                trans = parts[1].strip()
                # original = re.sub(r'^\d+\.\s*', '', original)
                # trans = re.sub(r'^\d+\.\s*', '', trans)
                if original or trans:
                    data.append({"åŸæ–‡": original, "è¯‘æ–‡": trans})
        else:
            if len(line) < 50:
                pending_line = line
            else:
                log_manager.log_exception(f"ç¬¬ {line_num} è¡Œï¼šç¼ºå°‘åˆ†éš”ç¬¦", line[:100])

    return data


# ==========================================
# === ğŸ”¤ è‹±æ–‡åå¤„ç†åˆ†å¥ ===
# ==========================================
# å¸¸è§è‹±æ–‡ç¼©å†™ï¼ˆå¥å·ä¸è¡¨ç¤ºå¥æœ«ï¼‰
ENGLISH_ABBREVIATIONS = {
    # å­¦æœ¯/å¼•ç”¨
    'et al', 'etc', 'e.g', 'i.e', 'vs', 'cf', 'ibid', 'op', 'cit',
    # ç§°è°“
    'mr', 'mrs', 'ms', 'dr', 'prof', 'jr', 'sr', 'st',
    # å…¬å¸/ç»„ç»‡
    'inc', 'ltd', 'co', 'corp', 'llc', 'l.l.c',
    # å›½å®¶/åœ°åŒºç¼©å†™ï¼ˆå®Œæ•´å½¢å¼ï¼‰
    'u.s', 'u.k', 'u.n', 'e.u', 'u.s.a',
    # æ—¶é—´/åº¦é‡
    'a.m', 'p.m',
    # å¼•ç”¨æ ‡è®°
    'no', 'vol', 'fig', 'ch', 'sec', 'pp', 'approx', 'est', 'max', 'min', 'avg',
}

# å¤šå­—æ¯ç¼©å†™çš„ç»“å°¾å­—æ¯ï¼ˆç”¨äºæ£€æµ‹ "U.S." "U.K." ç­‰ï¼‰
# è¿™äº›æ˜¯å¸¸è§å¤šå­—æ¯ç¼©å†™ä¸­çš„æœ€åä¸€ä¸ªå­—æ¯
ABBREVIATION_ENDING_PATTERNS = {
    's': ['u.s', 'u.s.a'],  # U.S., U.S.A.
    'k': ['u.k'],  # U.K.
    'n': ['u.n'],  # U.N.
    'u': ['e.u'],  # E.U.
}


def is_abbreviation_period(text, period_pos):
    """åˆ¤æ–­å¥å·æ˜¯å¦å±äºç¼©å†™æˆ–è¿ç»­ç¼–å·çš„ä¸­é—´ç‚¹ï¼ˆä¸æ˜¯çœŸæ­£çš„å¥æœ«ï¼‰

    è¿”å› True çš„æƒ…å†µï¼ˆä¸è®¡å…¥å¥å­æ•°é‡ï¼‰ï¼š
    - ç¼©å†™ï¼šå¤šå­—æ¯è¯æ±‡ + å¥å·ï¼Œå¦‚ "et al." "Dr." "U.S."
    - è¿ç»­ç¼–å·çš„ä¸­é—´ç‚¹ï¼šå¦‚ "7.2.1" ä¸­çš„å‰ä¸¤ä¸ªç‚¹
    - ç¼–å·ä¸å†…å®¹è¿å†™ï¼šå¦‚ "8.Al-Snafi" ä¸­çš„ç‚¹ï¼ˆæ— ç©ºæ ¼åˆ†éš”ï¼‰

    è¿”å› False çš„æƒ…å†µï¼ˆè®¡å…¥å¥å­æ•°é‡ï¼Œéœ€è¦æ–­å¼€ï¼‰ï¼š
    - ç¼–å·ï¼šå•ç‹¬çš„æ•°å­—æˆ–å­—æ¯ + å¥å· + ç©ºæ ¼ï¼Œå¦‚ "1. " "A. " "II. "
    """
    if period_pos <= 0:
        return False

    # è·å–å¥å·å‰åçš„æ–‡æœ¬
    before = text[:period_pos]
    after = text[period_pos + 1:] if period_pos + 1 < len(text) else ""

    # å…³é”®æ£€æŸ¥1ï¼šå¦‚æœå¥å·åé¢ç´§è·Ÿç€æ•°å­—ï¼Œè¿™æ˜¯è¿ç»­ç¼–å·çš„ä¸­é—´ç‚¹ï¼ˆå¦‚ 7.2.1 ä¸­çš„ç‚¹ï¼‰
    if after and after[0].isdigit():
        return True

    # å…³é”®æ£€æŸ¥2ï¼šå¦‚æœå¥å·åé¢ç´§è·Ÿç€å­—æ¯ï¼ˆæ— ç©ºæ ¼ï¼‰ï¼Œè¿™æ˜¯ç¼–å·ä¸å†…å®¹è¿å†™ï¼ˆå¦‚ "8.Al-Snafi"ï¼‰
    # è¿™ç§æƒ…å†µä¸åº”è¯¥åˆ†å‰²
    if after and after[0].isalpha():
        return True

    # æ‰¾åˆ°å¥å·å‰çš„æœ€åä¸€ä¸ª"è¯"
    words = before.split()
    if not words:
        return False

    last_token = words[-1]
    last_word_lower = last_token.lower().rstrip('.,;:')

    # 1. æ£€æŸ¥æ˜¯å¦æ˜¯å¤šå­—æ¯ç¼©å†™ï¼ˆå¦‚ "et al", "Dr", "Inc"ï¼‰
    if last_word_lower in ENGLISH_ABBREVIATIONS:
        return True

    # 2. æ£€æŸ¥æ˜¯å¦æ˜¯ X.Y. ç¼©å†™æ¨¡å¼ï¼ˆå¦‚ U.S.ï¼‰
    # å¦‚æœå½“å‰ token ä¸­åŒ…å«ç‚¹å·ï¼Œå¯èƒ½æ˜¯ç¼©å†™çš„ä¸€éƒ¨åˆ†
    if '.' in last_token:
        return True

    # 3. æ£€æŸ¥æ˜¯å¦æ˜¯å•ä¸ªå­—æ¯ï¼Œä¸”å‰é¢æ˜¯ "X." æ¨¡å¼ï¼ˆå¦‚ "U.S." ä¸­çš„ "S"ï¼‰
    if len(last_word_lower) == 1 and last_word_lower.isalpha():
        if len(words) >= 2:
            prev_token = words[-2].lower()
            if re.match(r'^[a-z]\.$', prev_token):
                return True
        # ç‹¬ç«‹çš„å•ä¸ªå­—æ¯ï¼ˆå¦‚ "A" "B"ï¼‰â†’ æ˜¯ç¼–å·ï¼Œéœ€è¦æ–­å¼€
        return False

    # 4. æ£€æŸ¥æ˜¯å¦æ˜¯çº¯æ•°å­—ï¼ˆå¦‚ "11"ï¼‰â†’ ä¸æ˜¯ç¼©å†™ï¼Œéœ€è¦æ–­å¼€
    if last_word_lower.isdigit():
        return False

    # 5. æ£€æŸ¥æ˜¯å¦æ˜¯ç½—é©¬æ•°å­—
    roman_pattern = r'^(i{1,3}|iv|vi{0,3}|ix|xi{0,3}|xiv|xvi{0,3}|xix|xxi{0,3})$'
    if re.match(roman_pattern, last_word_lower):
        return False  # ç½—é©¬æ•°å­—æ˜¯ç¼–å·ï¼Œéœ€è¦æ–­å¼€

    # 6. é»˜è®¤ï¼šä¸æ˜¯å·²çŸ¥çš„ç¼©å†™
    return False


def count_real_sentences_english(text):
    """è®¡ç®—è‹±æ–‡æ–‡æœ¬ä¸­çš„çœŸå®å¥å­æ•°é‡ï¼ˆæ’é™¤ç¼©å†™ï¼‰"""
    if not text:
        return 0

    count = 0
    i = 0
    while i < len(text):
        if text[i] in '.!?':
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç¼©å†™
            if text[i] == '.' and is_abbreviation_period(text, i):
                i += 1
                continue
            count += 1
        i += 1

    return max(1, count)


def has_numbering_pattern(text):
    """æ£€æµ‹æ–‡æœ¬æ˜¯å¦åŒ…å«éœ€è¦åˆ†å‰²çš„ç¼–å·æ¨¡å¼

    æ ¸å¿ƒè§„åˆ™ï¼šåªè¦æ£€æµ‹åˆ°"æœ«å°¾æ˜¯å¥å·+ç©ºæ ¼"çš„ç¼–å·æ¨¡å¼ï¼Œå°±éœ€è¦åˆ†å‰²

    éœ€è¦åˆ†å‰²ï¼š
    - "7.2. TITLE" â†’ æ£€æµ‹åˆ° "2. "ï¼Œéœ€è¦åˆ†å‰²
    - "A. Introduction" â†’ æ£€æµ‹åˆ° "A. "ï¼Œéœ€è¦åˆ†å‰²
    - "11. Author" â†’ æ£€æµ‹åˆ° "11. "ï¼Œéœ€è¦åˆ†å‰²

    ä¸éœ€è¦åˆ†å‰²ï¼š
    - "7.2.1 TITLE" â†’ "7.2.1" æœ«å°¾æ˜¯æ•°å­—ä¸æ˜¯å¥å·ï¼Œä¸åŒ¹é…
    """
    if not text:
        return False

    # ç¼–å·æ¨¡å¼æ­£åˆ™è¡¨è¾¾å¼
    # å…³é”®ï¼šå¿…é¡»æ˜¯ "å¥å·+ç©ºæ ¼" ç»“å°¾ï¼Œä¸èƒ½æ˜¯ "æ•°å­—+ç©ºæ ¼"
    numbering_patterns = [
        # æ•°å­—ç¼–å·ï¼š1. / 11. / 7.2.ï¼ˆæœ«å°¾å¿…é¡»æ˜¯å¥å·+ç©ºæ ¼ï¼‰
        # æ³¨æ„ï¼š7.2.1 ä¸åŒ¹é…ï¼Œå› ä¸º 1 åé¢æ˜¯ç©ºæ ¼ä¸æ˜¯å¥å·
        r'\d+\.\s',

        # å­—æ¯ç¼–å·ï¼šA. / B. / a. / b.ï¼ˆå•ä¸ªå­—æ¯+å¥å·+ç©ºæ ¼ï¼‰
        # ä½¿ç”¨è´Ÿå‘åç»æ’é™¤ï¼š
        # - å‰é¢æ˜¯å­—æ¯çš„æƒ…å†µï¼ˆå¦‚ "et al." ä¸­çš„ "l"ï¼‰
        # - å‰é¢æ˜¯å¥å·çš„æƒ…å†µï¼ˆå¦‚ "U.S." ä¸­çš„ "S"ï¼‰
        r'(?<![A-Za-z.])[A-Za-z]\.\s',

        # å¸¦æ‹¬å·çš„ç¼–å·ï¼š(1) / (A) / (a)
        r'\([0-9]+\)\s',
        r'\([A-Za-z]\)\s',

        # ç½—é©¬æ•°å­—ï¼šI. / II. / III. / IV.ï¼ˆç½—é©¬æ•°å­—+å¥å·+ç©ºæ ¼ï¼‰
        r'(?<![A-Za-z])[IVXivx]+\.\s',
    ]

    # åªè¦åŒ¹é…åˆ°ä»»ä¸€ç¼–å·æ¨¡å¼ï¼Œå°±éœ€è¦åˆ†å‰²
    for pattern in numbering_patterns:
        if re.search(pattern, text):
            return True

    return False


def needs_english_post_split(orig_text, source_lang):
    """åˆ¤æ–­æ˜¯å¦éœ€è¦è‹±æ–‡åå¤„ç†åˆ†å¥

    è§¦å‘æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€å³è§¦å‘ï¼‰ï¼š
    1. æ–‡æœ¬ä¸­åŒ…å«ç¼–å·æ¨¡å¼ï¼ˆå¦‚ A. / 1. / I. ç­‰ï¼‰
    2. æ–‡æœ¬ä¸­æœ‰å¤šä¸ªçœŸå®å¥å­ï¼ˆæ’é™¤ç¼©å†™åçš„å¥å·æ•° > 1ï¼‰
    """
    # åªå¯¹éCJKè¯­è¨€è¿›è¡Œåå¤„ç†
    if source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]:
        return False

    if not orig_text:
        return False

    # æ¡ä»¶1ï¼šæ£€æµ‹æ˜¯å¦åŒ…å«ç¼–å·æ¨¡å¼
    if has_numbering_pattern(orig_text):
        return True

    # æ¡ä»¶2ï¼šè®¡ç®—çœŸå®å¥å­æ•°é‡
    real_sentence_count = count_real_sentences_english(orig_text)
    if real_sentence_count > 1:
        return True

    return False


def post_process_english_split(data, model_id, source_lang="è‹±è¯­", target_lang="ä¸­æ–‡", enable_ai_split=True):
    """
    åå¤„ç†ï¼šå¯¹è‹±æ–‡åŸæ–‡è¿›è¡Œè¿›ä¸€æ­¥çš„ç»†ç²’åº¦åˆ†å¥

    Args:
        data: è§£æåçš„å¯¹é½æ•°æ®åˆ—è¡¨ [{"åŸæ–‡": ..., "è¯‘æ–‡": ...}, ...]
        model_id: AIæ¨¡å‹IDï¼ˆç”¨äºåˆ†å¥æ—¶è°ƒç”¨ï¼‰
        source_lang: æºè¯­è¨€
        target_lang: ç›®æ ‡è¯­è¨€
        enable_ai_split: æ˜¯å¦å¯ç”¨AIåˆ†å¥ï¼ˆTrue=è°ƒç”¨AIï¼ŒFalse=ä»…æ£€æµ‹ä½†ä¸åˆ†å‰²ï¼‰

    Returns:
        å¤„ç†åçš„æ•°æ®åˆ—è¡¨
    """
    # åªå¯¹éCJKæºè¯­è¨€è¿›è¡Œåå¤„ç†
    if source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]:
        return data

    if not data:
        return data

    result = []
    split_count = 0

    for idx, row in enumerate(data):
        orig = row.get('åŸæ–‡', '')
        trans = row.get('è¯‘æ–‡', '')

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å¥
        if needs_english_post_split(orig, source_lang):
            if enable_ai_split:
                # è°ƒç”¨AIè¿›è¡Œåˆ†å¥
                log_manager.log(f"åå¤„ç†åˆ†å¥: ç¬¬ {idx + 1} è¡Œéœ€è¦è¿›ä¸€æ­¥ç»†åˆ†")
                split_results = split_row_with_ai(orig, trans, model_id, f"åå¤„ç†-{idx + 1}", source_lang)

                if split_results and len(split_results) > 1:
                    result.extend(split_results)
                    split_count += 1
                    log_manager.log(f"  âœ… 1 è¡Œ â†’ {len(split_results)} è¡Œ")
                else:
                    # AIåˆ†å¥å¤±è´¥ï¼Œä¿ç•™åŸæ•°æ®
                    result.append(row)
            else:
                # ä¸å¯ç”¨AIï¼Œåªè®°å½•éœ€è¦åˆ†å¥çš„è¡Œ
                log_manager.log_exception(f"ç¬¬ {idx + 1} è¡Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥åˆ†å¥", orig[:100])
                result.append(row)
        else:
            result.append(row)

    if split_count > 0:
        log_manager.log(f"åå¤„ç†åˆ†å¥å®Œæˆ: {split_count} è¡Œè¢«ç»†åˆ†ï¼Œæ€»è¡Œæ•° {len(data)} â†’ {len(result)}")

    return result


def run_llm_alignment(file_original_path, file_trans_path, output_excel_path, model_id,
                      anchor_info_orig=None, anchor_info_trans=None, system_prompt_override=None,
                      source_lang="ä¸­æ–‡", target_lang="è‹±è¯­", enable_post_split=True,
                      enable_table_separate_processing=False):
    """
    å¯¹é½å‡½æ•° - ç”¨äºWord/PPTæ–‡æ¡£å¯¹é½

    å‚æ•°:
        enable_table_separate_processing: æ˜¯å¦å¯ç”¨Wordè¡¨æ ¼å•ç‹¬å¤„ç†ï¼ˆæŒ‰å•å…ƒæ ¼ä½ç½®åŒ¹é… + LLMç»†ç²’åº¦åˆ†å¥ï¼‰
                                          é»˜è®¤å…³é—­ï¼Œå¯ç”¨åä¼šå°†Wordè¡¨æ ¼æŒ‰å•å…ƒæ ¼ä½ç½®åŒ¹é…å¤„ç†

    æ³¨æ„ï¼šExcelæ–‡ä»¶åº”ä½¿ç”¨ process_excel_dual_file_alignment() å‡½æ•°å¤„ç†ï¼Œä¸è¦ä½¿ç”¨æœ¬å‡½æ•°
    """
    filename = os.path.basename(file_original_path)
    file_type = get_file_type(file_original_path)
    log_manager.log(f"æ­£åœ¨ AI å¯¹é½: {filename}")
    log_manager.log(f"ä½¿ç”¨æ¨¡å‹: {model_id}")
    log_manager.log(f"è¯­è¨€å¯¹: {source_lang} â†’ {target_lang}")

    all_data = []

    # æ£€æŸ¥æ˜¯å¦ä¸º Word æ–‡æ¡£ä¸”åŒ…å«è¡¨æ ¼ï¼Œä¸”å¯ç”¨äº†è¡¨æ ¼å•ç‹¬å¤„ç†
    if file_type == 'docx' and enable_table_separate_processing:
        orig_has_tables = has_docx_tables(file_original_path)
        trans_has_tables = has_docx_tables(file_trans_path)

        if orig_has_tables or trans_has_tables:
            log_manager.log("ğŸ“Š æ£€æµ‹åˆ°è¡¨æ ¼å†…å®¹ï¼Œå¯ç”¨è¡¨æ ¼å•ç‹¬å¤„ç†æ¨¡å¼")
            log_manager.log("   å¤„ç†ç­–ç•¥ï¼šæŒ‰å•å…ƒæ ¼ç»å¯¹ä½ç½®åŒ¹é… + LLMç»†ç²’åº¦åˆ†å¥")

            # 1. å…ˆå¤„ç†è¡¨æ ¼éƒ¨åˆ†ï¼ˆæŒ‰å•å…ƒæ ¼ä½ç½®åŒ¹é… + LLMåˆ†å¥ï¼‰
            table_results = process_docx_tables_alignment(
                file_original_path, file_trans_path, model_id,
                source_lang=source_lang, target_lang=target_lang
            )

            if table_results:
                log_manager.log(f"   è¡¨æ ¼å¤„ç†ç»“æœ: {len(table_results)} è¡Œ")
                # ç§»é™¤æ¥æºåˆ—ï¼Œåªä¿ç•™åŸæ–‡å’Œè¯‘æ–‡
                for result in table_results:
                    all_data.append({
                        "åŸæ–‡": result.get("åŸæ–‡", ""),
                        "è¯‘æ–‡": result.get("è¯‘æ–‡", "")
                    })

            # 2. å†å¤„ç†éè¡¨æ ¼éƒ¨åˆ†
            text_original = read_docx_without_tables(file_original_path)
            text_trans = read_docx_without_tables(file_trans_path)

            if text_original and text_trans:
                log_manager.log("ğŸ“ å¤„ç†éè¡¨æ ¼å†…å®¹...")

                # æ ¹æ®è¯­è¨€ç”ŸæˆåŠ¨æ€æç¤ºè¯
                if system_prompt_override:
                    system_prompt = system_prompt_override
                else:
                    system_prompt = get_docx_alignment_prompt(source_lang, target_lang)

                anchor_hint = ""
                if anchor_info_orig and anchor_info_trans:
                    anchor_hint = f"""
## ä¸Šä¸‹æ–‡æç¤º
è¿™æ˜¯æ–‡æ¡£çš„ä¸€ä¸ªç‰‡æ®µï¼š
- åŸæ–‡å¼€å¤´: "{anchor_info_orig.get('first_anchor', '')[:100]}"
- è¯‘æ–‡å¼€å¤´: "{anchor_info_trans.get('first_anchor', '')[:100]}"
"""

                user_prompt = f"""{anchor_hint}

<Stream_A_Original>
{text_original}
</Stream_A_Original>

<Stream_B_Translation>
{text_trans}
</Stream_B_Translation>

è¯·ä¸¥æ ¼æŒ‰è§„åˆ™è¾“å‡ºå¯¹é½ç»“æœï¼š
"""

                response = call_llm_stream(system_prompt, user_prompt, model_id, f"{filename}-éè¡¨æ ¼")

                if response:
                    last_500 = response[-500:] if len(response) > 500 else response
                    if '|||' not in last_500:
                        log_manager.log_exception("âš ï¸ éè¡¨æ ¼éƒ¨åˆ†è¾“å‡ºå¯èƒ½è¢«æˆªæ–­")

                    paragraph_data = parse_alignment_response(response)

                    if paragraph_data:
                        # å¯¹éCJKæºè¯­è¨€è¿›è¡Œåå¤„ç†åˆ†å¥
                        if enable_post_split and source_lang not in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]:
                            log_manager.log("æ£€æŸ¥éè¡¨æ ¼éƒ¨åˆ†æ˜¯å¦éœ€è¦åå¤„ç†åˆ†å¥...")
                            paragraph_data = post_process_english_split(
                                paragraph_data, model_id, source_lang, target_lang, enable_ai_split=True
                            )

                        log_manager.log(f"   éè¡¨æ ¼å¤„ç†ç»“æœ: {len(paragraph_data)} è¡Œ")
                        all_data.extend(paragraph_data)

            if not all_data:
                log_manager.log_exception("å¤„ç†ç»“æœä¸ºç©ºï¼Œæ–‡ä»¶å¤„ç†å¤±è´¥")
                return False

            df = pd.DataFrame(all_data)

            # è´¨é‡æ£€æŸ¥
            log_manager.log("æ‰§è¡Œè´¨é‡æ£€æŸ¥...")
            issues = AlignmentChecker.full_check(df, source_lang, target_lang)

            if issues:
                log_manager.log_exception(f"å‘ç° {len(issues)} ä¸ªæ½œåœ¨é—®é¢˜ï¼ˆä»…è­¦å‘Šï¼‰")
                for issue in issues[:10]:
                    orig_text = issue.get('original_text', '')
                    orig_preview = orig_text[:80] if orig_text else ''
                    log_manager.log_exception(
                        f"è¡Œ {issue.get('row', '?')}: {issue.get('type', '')} - {issue.get('detail', '')}",
                        f"åŸæ–‡: {orig_preview}..."
                    )
                issue_path = output_excel_path.replace('.xlsx', '_é—®é¢˜æŠ¥å‘Š.xlsx')
                save_issues_report(issues, issue_path)

            df.to_excel(output_excel_path, index=False)
            log_manager.log(f"âœ… å·²ä¿å­˜: {output_excel_path}ï¼ˆ{len(df)} è¡Œï¼Œå«è¡¨æ ¼ + æ®µè½ï¼‰")
            return True

    # åŸæœ‰é€»è¾‘ï¼šæ— è¡¨æ ¼æˆ–éWordæ–‡æ¡£çš„å¤„ç†
    text_original = read_file_content(file_original_path)
    text_trans = read_file_content(file_trans_path)

    if not text_original or not text_trans:
        log_manager.log_exception("å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡æ–‡ä»¶", f"åŸæ–‡: {file_original_path}")
        return False

    # æ ¹æ®è¯­è¨€ç”ŸæˆåŠ¨æ€æç¤ºè¯
    if system_prompt_override:
        system_prompt = system_prompt_override
    else:
        system_prompt = get_docx_alignment_prompt(source_lang, target_lang)

    anchor_hint = ""
    if anchor_info_orig and anchor_info_trans:
        anchor_hint = f"""
## ä¸Šä¸‹æ–‡æç¤º
è¿™æ˜¯æ–‡æ¡£çš„ä¸€ä¸ªç‰‡æ®µï¼š
- åŸæ–‡å¼€å¤´: "{anchor_info_orig.get('first_anchor', '')[:100]}"
- è¯‘æ–‡å¼€å¤´: "{anchor_info_trans.get('first_anchor', '')[:100]}"
"""

    user_prompt = f"""{anchor_hint}

<Stream_A_Original>
{text_original}
</Stream_A_Original>

<Stream_B_Translation>
{text_trans}
</Stream_B_Translation>

è¯·ä¸¥æ ¼æŒ‰è§„åˆ™è¾“å‡ºå¯¹é½ç»“æœï¼š
"""

    response = call_llm_stream(system_prompt, user_prompt, model_id, filename)

    # æ£€æµ‹æˆªæ–­
    if response:
        last_500 = response[-500:] if len(response) > 500 else response
        if '|||' not in last_500:
            log_manager.log_exception("âš ï¸ è¾“å‡ºå¯èƒ½è¢«æˆªæ–­ï¼ˆæœ€å500å­—ç¬¦æ— åˆ†éš”ç¬¦ï¼‰")

    data = parse_alignment_response(response)

    if not data:
        log_manager.log_exception("è§£æç»“æœä¸ºç©ºï¼Œæ–‡ä»¶å¤„ç†å¤±è´¥")
        return False

    # å¯¹éCJKæºè¯­è¨€è¿›è¡Œåå¤„ç†åˆ†å¥ï¼ˆå¦‚è‹±æ–‡åŸæ–‡éœ€è¦æŒ‰å¥å·ç»†åˆ†ï¼‰
    if enable_post_split and source_lang not in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]:
        log_manager.log("æ£€æŸ¥æ˜¯å¦éœ€è¦åå¤„ç†åˆ†å¥...")
        data = post_process_english_split(data, model_id, source_lang, target_lang, enable_ai_split=True)

    df = pd.DataFrame(data)

    # è´¨é‡æ£€æŸ¥ï¼ˆä¼ é€’è¯­è¨€å‚æ•°ï¼‰
    log_manager.log("æ‰§è¡Œè´¨é‡æ£€æŸ¥...")
    issues = AlignmentChecker.full_check(df, source_lang, target_lang)

    if issues:
        log_manager.log_exception(f"å‘ç° {len(issues)} ä¸ªæ½œåœ¨é—®é¢˜ï¼ˆä»…è­¦å‘Šï¼‰")
        for issue in issues[:10]:
            orig_text = issue.get('original_text', '')
            orig_preview = orig_text[:80] if orig_text else ''
            log_manager.log_exception(
                f"è¡Œ {issue.get('row', '?')}: {issue.get('type', '')} - {issue.get('detail', '')}",
                f"åŸæ–‡: {orig_preview}..."
            )
        issue_path = output_excel_path.replace('.xlsx', '_é—®é¢˜æŠ¥å‘Š.xlsx')
        save_issues_report(issues, issue_path)

    df.to_excel(output_excel_path, index=False)
    log_manager.log(f"âœ… å·²ä¿å­˜: {output_excel_path}ï¼ˆ{len(df)} è¡Œï¼‰")
    return True


def needs_sentence_split(orig_text, trans_text, source_lang="ä¸­æ–‡"):
    """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†å¥ - æ ¹æ®åŸæ–‡è¯­è¨€æ£€æµ‹ç›¸åº”çš„å¥æœ«æ ‡ç‚¹"""
    source_is_cjk = source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]

    if source_is_cjk:
        # ä¸­æ—¥éŸ©è¯­è¨€ä½¿ç”¨å…¨è§’å¥æœ«æ ‡ç‚¹
        terminators = ['ã€‚', 'ï¼', 'ï¼Ÿ']
    else:
        # è¥¿æ–¹è¯­è¨€ä½¿ç”¨åŠè§’å¥æœ«æ ‡ç‚¹
        terminators = ['. ', '! ', '? ', '.', '!', '?']

    count = sum(orig_text.count(t) for t in terminators)
    return count > 1


def split_row_with_ai(orig_text, trans_text, model_id, row_num, source_lang="ä¸­æ–‡"):
    """ä½¿ç”¨ AI å¯¹å•è¡Œè¿›è¡Œåˆ†å¥å¯¹é½ - æ”¯æŒ OpenRouter å’Œ Luzhishen"""
    log_manager.log_stream(f"\n{'=' * 50}\n")
    log_manager.log_stream(f"ğŸ“ ç¬¬ {row_num} è¡Œ\n")
    log_manager.log_stream(f"[åŸæ–‡] {orig_text[:100]}{'...' if len(orig_text) > 100 else ''}\n")
    log_manager.log_stream(f"[è¯‘æ–‡] {trans_text[:100]}{'...' if len(trans_text) > 100 else ''}\n")
    log_manager.log_stream(f"[AIè¾“å‡º] ")

    user_prompt = f"""è¯·æ ¹æ®åŸæ–‡çš„æ ‡ç‚¹åˆ†å¥ï¼Œå°†è¯‘æ–‡å¯¹åº”æ‹†åˆ†å¯¹é½ï¼š

ã€åŸæ–‡ã€‘
{orig_text}

ã€è¯‘æ–‡ã€‘
{trans_text}

è¾“å‡ºåˆ†å¥å¯¹é½ç»“æœï¼ˆä»¥åŸæ–‡åˆ†å¥æ•°é‡ä¸ºå‡†ï¼‰ï¼š"""

    # æ ¹æ®åŸæ–‡è¯­è¨€è·å–å¯¹åº”çš„åˆ†å¥æç¤ºè¯
    system_prompt = get_split_row_prompt(source_lang)

    try:
        # ä½¿ç”¨ç»Ÿä¸€çš„ LLM è°ƒç”¨å‡½æ•°ï¼Œè‡ªåŠ¨æ£€æµ‹æä¾›å•†
        full_response = call_llm_stream(
            system_prompt,
            user_prompt,
            model_id,
            f"åˆ†å¥-ç¬¬{row_num}è¡Œ"
        )

        if not full_response:
            log_manager.log_exception(f"ç¬¬ {row_num} è¡Œ AI è¿”å›ä¸ºç©º")
            return None

        results = parse_alignment_response(full_response)

        if results:
            log_manager.log_stream(f"âœ… åˆ†å¥ç»“æœ: 1 â†’ {len(results)} å¥\n")
            log_manager.log(f"ç¬¬ {row_num} è¡Œ: 1 â†’ {len(results)} å¥")
            return results
        else:
            log_manager.log_exception(f"ç¬¬ {row_num} è¡Œ AI è¿”å›è§£æå¤±è´¥", full_response[:200])
            return None

    except Exception as e:
        log_manager.log_exception(f"ç¬¬ {row_num} è¡Œ AI å¤„ç†å¤±è´¥", str(e))
        return None


def needs_table_cell_split(text, source_lang="ä¸­æ–‡"):
    """
    åˆ¤æ–­è¡¨æ ¼å•å…ƒæ ¼å†…å®¹æ˜¯å¦éœ€è¦åˆ†å¥ - æ£€æµ‹å¥æœ«æ ‡ç‚¹ã€æ¢è¡Œã€åºå·ç­‰

    è§¦å‘LLMåˆ†å¥çš„æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€å³å¯ï¼‰ï¼š
    1. æœ‰å¤šä¸ªå¥æœ«æ ‡ç‚¹ï¼ˆã€‚ï¼ï¼Ÿï¼‰
    2. æœ‰"ï¼"æˆ–"ï¼Ÿ"
    3. æœ‰æ¢è¡Œç¬¦ï¼ˆ\nã€\rã€æˆ–å¤šä¸ªè¿ç»­ç©ºæ ¼æ¨¡æ‹Ÿçš„æ¢è¡Œï¼‰
    4. æœ‰åºå·æ¨¡å¼ï¼ˆå¦‚ï¼š1. 2. æˆ– ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰æˆ– â‘ â‘¡ ç­‰ï¼‰
    """
    if not text or not text.strip():
        return False

    import re

    source_is_cjk = source_lang in ["ä¸­æ–‡", "æ—¥è¯­", "éŸ©è¯­"]

    # æ£€æµ‹æ¢è¡Œç¬¦ï¼ˆåŒ…æ‹¬ \nã€\r\nã€\rï¼‰
    has_newline = '\n' in text or '\r' in text

    # æ£€æµ‹è¿ç»­ç©ºæ ¼ï¼ˆ2ä¸ªæˆ–ä»¥ä¸Šè¿ç»­ç©ºæ ¼ï¼Œå¯èƒ½æ˜¯æ®µè½åˆ†éš”ï¼‰
    has_consecutive_spaces = '  ' in text

    # æ£€æµ‹å¥æœ«æ ‡ç‚¹
    if source_is_cjk:
        # ä¸­æ–‡æ ‡ç‚¹
        period_count = text.count('ã€‚')
        exclamation_count = text.count('ï¼')
        question_count = text.count('ï¼Ÿ')
    else:
        # è‹±æ–‡æ ‡ç‚¹
        period_count = text.count('.')
        exclamation_count = text.count('!')
        question_count = text.count('?')

    total_punct_count = period_count + exclamation_count + question_count

    # æ£€æµ‹åºå·æ¨¡å¼
    # ä¸­æ–‡åºå·ï¼šï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰ã€ï¼ˆ1ï¼‰ï¼ˆ2ï¼‰ã€ä¸€ã€äºŒã€ç­‰
    # æ•°å­—åºå·ï¼š1. 2. æˆ– 1ã€2ã€æˆ– 1) 2)
    # åœ†åœˆåºå·ï¼šâ‘ â‘¡â‘¢
    has_numbered_list = bool(re.search(
        r'ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ï¼‰|'  # ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰ï¼ˆ1ï¼‰ï¼ˆ2ï¼‰
        r'[ï¼ˆ\(]\d+[ï¼‰\)]|'  # (1) (2)
        r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]|'  # â‘ â‘¡â‘¢
        r'\d+\.(?!\d)\s*\S|'  # 1. åé¢è·Ÿéæ•°å­—ï¼ˆé¿å…åŒ¹é…å°æ•°å¦‚1.0ã€2.5ï¼‰
        r'\d+ã€\s*\S|'  # 1ã€2ã€åé¢è·Ÿå†…å®¹
        r'\d+[ï¼‰\)]\s*\S|'  # 1) 2ï¼‰åé¢è·Ÿå†…å®¹
        r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼.]',  # ä¸€ã€äºŒã€
        text
    ))

    # è§¦å‘æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€å³å¯ï¼‰ï¼š
    # 1. æœ‰å¤šä¸ªå¥æœ«æ ‡ç‚¹ï¼ˆéœ€è¦åˆ†å¥ï¼‰
    # 2. æœ‰æ„Ÿå¹å·æˆ–é—®å·
    # 3. æœ‰æ¢è¡Œç¬¦ï¼ˆæ®µè½åˆ†éš”ï¼‰
    # 4. æœ‰è¿ç»­ç©ºæ ¼ï¼ˆå¯èƒ½æ˜¯æ®µè½åˆ†éš”ï¼‰
    # 5. æœ‰åºå·åˆ—è¡¨æ¨¡å¼
    return (
            total_punct_count > 1 or  # å¤šä¸ªå¥æœ«æ ‡ç‚¹
            exclamation_count > 0 or  # æœ‰æ„Ÿå¹å·
            question_count > 0 or  # æœ‰é—®å·
            has_newline or  # æœ‰æ¢è¡Œ
            has_consecutive_spaces or  # æœ‰è¿ç»­ç©ºæ ¼
            has_numbered_list  # æœ‰åºå·åˆ—è¡¨
    )


def split_table_cell_with_ai(orig_text, trans_text, model_id, cell_ref, source_lang="ä¸­æ–‡"):
    """ä½¿ç”¨ AI å¯¹è¡¨æ ¼å•å…ƒæ ¼å†…å®¹è¿›è¡Œç»†ç²’åº¦åˆ†å¥å¯¹é½"""
    log_manager.log_stream(f"\n{'=' * 50}\n")
    log_manager.log_stream(f"ğŸ“ å•å…ƒæ ¼ {cell_ref}\n")
    log_manager.log_stream(f"[åŸæ–‡] {orig_text[:100]}{'...' if len(orig_text) > 100 else ''}\n")
    log_manager.log_stream(f"[è¯‘æ–‡] {trans_text[:100]}{'...' if len(trans_text) > 100 else ''}\n")
    log_manager.log_stream(f"[AIåˆ†å¥] ")

    user_prompt = f"""è¯·æ ¹æ®åŸæ–‡çš„å¥æœ«æ ‡ç‚¹ï¼ˆã€‚ï¼ï¼Ÿï¼‰å’Œæ¢è¡Œç¬¦è¿›è¡Œåˆ†å¥ï¼Œå°†è¯‘æ–‡å¯¹åº”æ‹†åˆ†å¯¹é½ï¼š

ã€åŸæ–‡ã€‘
{orig_text}

ã€è¯‘æ–‡ã€‘
{trans_text}

è¾“å‡ºåˆ†å¥å¯¹é½ç»“æœï¼š"""

    # è·å–è¡¨æ ¼å•å…ƒæ ¼åˆ†å¥æç¤ºè¯
    system_prompt = get_table_cell_split_prompt(source_lang)

    try:
        # ä½¿ç”¨ç»Ÿä¸€çš„ LLM è°ƒç”¨å‡½æ•°
        full_response = call_llm_stream(
            system_prompt,
            user_prompt,
            model_id,
            f"è¡¨æ ¼åˆ†å¥-{cell_ref}"
        )

        if not full_response:
            log_manager.log_exception(f"å•å…ƒæ ¼ {cell_ref} AI è¿”å›ä¸ºç©º")
            return None

        results = parse_alignment_response(full_response)

        if results:
            log_manager.log_stream(f"âœ… åˆ†å¥ç»“æœ: 1 â†’ {len(results)} å¥\n")
            log_manager.log(f"å•å…ƒæ ¼ {cell_ref}: 1 â†’ {len(results)} å¥")
            return results
        else:
            log_manager.log_exception(f"å•å…ƒæ ¼ {cell_ref} AI è¿”å›è§£æå¤±è´¥", full_response[:200])
            return None

    except Exception as e:
        log_manager.log_exception(f"å•å…ƒæ ¼ {cell_ref} AI å¤„ç†å¤±è´¥", str(e))
        return None


def extract_docx_tables_with_position(doc_path):
    """
    ä» Word æ–‡æ¡£ä¸­æå–æ‰€æœ‰è¡¨æ ¼ï¼Œä¿ç•™å•å…ƒæ ¼ä½ç½®ä¿¡æ¯
    è¿”å›: [(table_idx, row_idx, col_idx, cell_text), ...]
    """
    try:
        doc = Document(doc_path)
        table_cells = []

        for table_idx, table in enumerate(doc.tables):
            for row_idx, row in enumerate(table.rows):
                for col_idx, cell in enumerate(row.cells):
                    cell_text = cell.text.strip()
                    if cell_text:
                        table_cells.append({
                            'table_idx': table_idx,
                            'row_idx': row_idx,
                            'col_idx': col_idx,
                            'text': cell_text,
                            'cell_ref': f"Wordè¡¨{table_idx + 1}-è¡Œ{row_idx + 1}åˆ—{col_idx + 1}"
                        })

        return table_cells
    except Exception as e:
        log_manager.log_exception(f"æå–è¡¨æ ¼å¤±è´¥", str(e))
        return []


def process_docx_tables_alignment(orig_docx_path, trans_docx_path, model_id,
                                  source_lang="ä¸­æ–‡", target_lang="è‹±è¯­"):
    """
    å¤„ç† Word æ–‡æ¡£ä¸­çš„è¡¨æ ¼ - æŒ‰å•å…ƒæ ¼ç»å¯¹ä½ç½®åŒ¹é…åè¿›è¡Œç»†ç²’åº¦åˆ†å¥

    å¤„ç†æµç¨‹ï¼š
    1. æå–åŸæ–‡å’Œè¯‘æ–‡æ–‡æ¡£ä¸­çš„æ‰€æœ‰è¡¨æ ¼å•å…ƒæ ¼ï¼ˆä¿ç•™ä½ç½®ä¿¡æ¯ï¼‰
    2. æŒ‰ç…§ (è¡¨æ ¼ç´¢å¼•, è¡Œç´¢å¼•, åˆ—ç´¢å¼•) è¿›è¡Œä½ç½®åŒ¹é…
    3. å¯¹åŒ¹é…åçš„å•å…ƒæ ¼å†…å®¹è°ƒç”¨ LLM è¿›è¡Œç»†ç²’åº¦åˆ†å¥
    4. åˆ†å¥è§„åˆ™ï¼šå¥æœ«ä¸º"ã€‚","ï¼Ÿ","ï¼"ä»¥åŠæ¢è¡Œæ—¶æ–­å¥

    è¿”å›: [(åŸæ–‡, è¯‘æ–‡), ...] æ ¼å¼çš„å¯¹é½ç»“æœåˆ—è¡¨
    """
    log_manager.log("=" * 60)
    log_manager.log("ğŸ“Š Word è¡¨æ ¼å¤„ç†ï¼ˆæŒ‰å•å…ƒæ ¼ä½ç½®åŒ¹é… + LLMç»†ç²’åº¦åˆ†å¥ï¼‰")
    log_manager.log("=" * 60)
    log_manager.log(f"è¯­è¨€å¯¹: {source_lang} â†’ {target_lang}")

    # æå–åŸæ–‡å’Œè¯‘æ–‡çš„è¡¨æ ¼å•å…ƒæ ¼
    orig_cells = extract_docx_tables_with_position(orig_docx_path)
    trans_cells = extract_docx_tables_with_position(trans_docx_path)

    log_manager.log(f"åŸæ–‡è¡¨æ ¼å•å…ƒæ ¼æ•°: {len(orig_cells)}")
    log_manager.log(f"è¯‘æ–‡è¡¨æ ¼å•å…ƒæ ¼æ•°: {len(trans_cells)}")

    if not orig_cells:
        log_manager.log("åŸæ–‡æ–‡æ¡£ä¸­æ²¡æœ‰è¡¨æ ¼å†…å®¹")
        return []

    # å»ºç«‹è¯‘æ–‡å•å…ƒæ ¼çš„ä½ç½®ç´¢å¼•
    trans_cell_map = {}
    for cell in trans_cells:
        key = (cell['table_idx'], cell['row_idx'], cell['col_idx'])
        trans_cell_map[key] = cell

    all_results = []
    total_cells_processed = 0
    total_cells_split = 0

    log_manager.log_stream("\n" + "=" * 60 + "\n")
    log_manager.log_stream(f"ğŸ“„ å¼€å§‹å¤„ç† Word æ–‡æ¡£è¡¨æ ¼ï¼ˆéExcelï¼‰\n")
    log_manager.log_stream(f"ğŸ“ å…± {len(orig_cells)} ä¸ª Word è¡¨æ ¼å•å…ƒæ ¼å¾…å¤„ç†\n")
    log_manager.log_stream("=" * 60 + "\n")

    for orig_cell in orig_cells:
        key = (orig_cell['table_idx'], orig_cell['row_idx'], orig_cell['col_idx'])
        cell_ref = orig_cell['cell_ref']
        orig_text = orig_cell['text']

        # æŸ¥æ‰¾å¯¹åº”ä½ç½®çš„è¯‘æ–‡å•å…ƒæ ¼
        trans_cell = trans_cell_map.get(key)
        trans_text = trans_cell['text'] if trans_cell else ""

        total_cells_processed += 1

        # è·³è¿‡åŸæ–‡å’Œè¯‘æ–‡éƒ½ä¸ºç©ºçš„æƒ…å†µ
        if not orig_text and not trans_text:
            continue

        # åˆ¤æ–­æ˜¯å¦éœ€è¦ç»†ç²’åº¦åˆ†å¥
        if orig_text and trans_text and needs_table_cell_split(orig_text, source_lang):
            log_manager.log(f"  {cell_ref}: éœ€è¦ç»†ç²’åº¦åˆ†å¥")
            split_results = split_table_cell_with_ai(
                orig_text, trans_text, model_id, cell_ref, source_lang
            )

            if split_results and len(split_results) > 1:
                # åˆ†å¥æˆåŠŸï¼Œæ·»åŠ æ‰€æœ‰åˆ†å¥ç»“æœ
                for result in split_results:
                    result['æ¥æº'] = cell_ref
                all_results.extend(split_results)
                total_cells_split += 1
            else:
                # åˆ†å¥å¤±è´¥æˆ–åªæœ‰ä¸€å¥ï¼Œä¿ç•™åŸå†…å®¹
                all_results.append({
                    "åŸæ–‡": orig_text,
                    "è¯‘æ–‡": trans_text,
                    "æ¥æº": cell_ref
                })
        else:
            # ä¸éœ€è¦åˆ†å¥ï¼Œç›´æ¥æ·»åŠ 
            if orig_text or trans_text:
                all_results.append({
                    "åŸæ–‡": orig_text,
                    "è¯‘æ–‡": trans_text,
                    "æ¥æº": cell_ref
                })
                if orig_text and trans_text:
                    log_manager.log_stream(f"[{cell_ref}] ç›´æ¥é…å¯¹\n")

    log_manager.log_stream("\n" + "=" * 60 + "\n")
    log_manager.log_stream(f"âœ… Word æ–‡æ¡£è¡¨æ ¼å¤„ç†å®Œæˆï¼\n")
    log_manager.log_stream(f"   å¤„ç† Word å•å…ƒæ ¼æ•°: {total_cells_processed}\n")
    log_manager.log_stream(f"   AIåˆ†å¥å¤„ç†æ•°: {total_cells_split}\n")
    log_manager.log_stream(f"   æœ€ç»ˆè¾“å‡ºè¡Œæ•°: {len(all_results)}\n")
    log_manager.log_stream("=" * 60 + "\n")

    log_manager.log(f"âœ… è¡¨æ ¼å¤„ç†å®Œæˆ: {total_cells_processed} ä¸ªå•å…ƒæ ¼ â†’ {len(all_results)} è¡Œ")

    return all_results


def read_docx_without_tables(file_path):
    """è¯»å– Word æ–‡æ¡£å†…å®¹ï¼Œæ’é™¤è¡¨æ ¼éƒ¨åˆ†ï¼ˆè¡¨æ ¼å•ç‹¬å¤„ç†ï¼‰"""
    try:
        doc = Document(file_path)
        full_text = []

        if hasattr(doc, 'element') and hasattr(doc.element, 'body'):
            for child in doc.element.body.iterchildren():
                if child.tag.endswith('p'):
                    # æ®µè½å…ƒç´ 
                    para = Paragraph(child, doc)
                    if para.text.strip():
                        full_text.append(para.text)
                # è·³è¿‡è¡¨æ ¼å…ƒç´  (tbl)ï¼Œè¡¨æ ¼å•ç‹¬å¤„ç†

        # é¡µçœ‰é¡µè„š (åŒ…æ‹¬æ®µè½ã€è¡¨æ ¼ã€æ–‡æœ¬æ¡†)
        for section in doc.sections:
            # é¡µçœ‰æ®µè½
            for p in section.header.paragraphs:
                if p.text.strip():
                    full_text.append(p.text)
            # é¡µçœ‰è¡¨æ ¼
            for table in section.header.tables:
                seen_header_cells = set()
                for row in table.rows:
                    for cell in row.cells:
                        cell_text = cell.text.strip()
                        if cell_text and cell_text not in seen_header_cells:
                            full_text.append(cell_text)
                            seen_header_cells.add(cell_text)
            # é¡µçœ‰æ–‡æœ¬æ¡†
            if section.header._element is not None:
                header_xml = etree.tostring(section.header._element, encoding='unicode')
                header_root = etree.fromstring(header_xml.encode('utf-8'))
                nsmap_header = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
                for txbx in header_root.xpath('.//w:txbxContent', namespaces=nsmap_header):
                    txbx_text = ''.join(txbx.xpath('.//w:t/text()', namespaces=nsmap_header))
                    if txbx_text.strip():
                        full_text.append(txbx_text.strip())

            # é¡µè„šæ®µè½
            for p in section.footer.paragraphs:
                if p.text.strip():
                    full_text.append(p.text)
            # é¡µè„šè¡¨æ ¼
            for table in section.footer.tables:
                seen_footer_cells = set()
                for row in table.rows:
                    for cell in row.cells:
                        cell_text = cell.text.strip()
                        if cell_text and cell_text not in seen_footer_cells:
                            full_text.append(cell_text)
                            seen_footer_cells.add(cell_text)
            # é¡µè„šæ–‡æœ¬æ¡†
            if section.footer._element is not None:
                footer_xml = etree.tostring(section.footer._element, encoding='unicode')
                footer_root = etree.fromstring(footer_xml.encode('utf-8'))
                nsmap_footer = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
                for txbx in footer_root.xpath('.//w:txbxContent', namespaces=nsmap_footer):
                    txbx_text = ''.join(txbx.xpath('.//w:t/text()', namespaces=nsmap_footer))
                    if txbx_text.strip():
                        full_text.append(txbx_text.strip())

        # æ–‡æœ¬æ¡†å¤„ç†ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
        if hasattr(doc.element, 'xml'):
            xml = doc.element.xml
            root = etree.fromstring(xml.encode('utf-8'))

            nsmap = {
                'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main',
                'wp': 'http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing',
                'a': 'http://schemas.openxmlformats.org/drawingml/2006/main',
                'wps': 'http://schemas.microsoft.com/office/word/2010/wordprocessingShape',
            }

            textbox_texts = set()

            # ä¼ ç»Ÿæ–‡æœ¬æ¡†
            try:
                textbox_containers = root.xpath('.//w:txbxContent', namespaces=nsmap)
                for container in textbox_containers:
                    paragraphs = container.xpath('.//w:p', namespaces=nsmap)
                    para_texts = []
                    for p in paragraphs:
                        p_text = ''.join([t.text for t in p.xpath('.//w:t', namespaces=nsmap) if t.text])
                        if p_text.strip():
                            para_texts.append(p_text.strip())
                    merged_text = '\n'.join(para_texts)
                    if merged_text.strip():
                        textbox_texts.add(merged_text.strip())
            except:
                pass

            for text in textbox_texts:
                full_text.append(text)

        # åˆå¹¶æ–‡æœ¬ï¼Œè¿‡æ»¤ç©ºè¡Œé¿å…åŸæ–‡è¯‘æ–‡ç©ºè¡Œä¸å¯¹åº”å¯¼è‡´çš„å¯¹é½é—®é¢˜
        result = "\n".join(full_text)
        # æŠŠè¿ç»­å¤šä¸ªæ¢è¡Œç¬¦åˆå¹¶æˆå•ä¸ªæ¢è¡Œç¬¦
        import re
        result = re.sub(r'\n{2,}', '\n', result)
        return result.strip()
    except Exception as e:
        log_manager.log_exception(f"è¯»å–æ–‡æ¡£å¤±è´¥ï¼ˆæ’é™¤è¡¨æ ¼ï¼‰", str(e))
        return ""


def has_docx_tables(file_path):
    """æ£€æŸ¥ Word æ–‡æ¡£æ˜¯å¦åŒ…å«è¡¨æ ¼"""
    try:
        doc = Document(file_path)
        return len(doc.tables) > 0
    except:
        return False


# ==========================================
# === ğŸ“Š Excel åŒæ–‡ä»¶å¤„ç†ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰===
# ==========================================
def process_excel_dual_file_alignment(orig_excel_path, trans_excel_path, output_excel_path, model_id,
                                      source_lang="ä¸­æ–‡", target_lang="è‹±è¯­"):
    """
    å¤„ç†ä¸¤ä¸ªå¯¹åº”çš„ Excel è¡¨æ ¼æ–‡ä»¶
    - æ”¯æŒå¤šå·¥ä½œç°¿
    - æŒ‰å•å…ƒæ ¼ä½ç½®å¯¹åº”
    - éœ€è¦åˆ†å¥æ—¶è°ƒç”¨ AI åˆ†å‰²
    """
    log_manager.log("=" * 60)
    log_manager.log("ğŸ“Š Excel åŒæ–‡ä»¶å¯¹é½æ¨¡å¼ï¼ˆæŒ‰å•å…ƒæ ¼ä½ç½®å¯¹åº”ï¼‰")
    log_manager.log("=" * 60)
    log_manager.log(f"è¯­è¨€å¯¹: {source_lang} â†’ {target_lang}")
    log_manager.log(f"åŸæ–‡æ–‡ä»¶: {orig_excel_path}")
    log_manager.log(f"è¯‘æ–‡æ–‡ä»¶: {trans_excel_path}")

    # è¯»å–æ‰€æœ‰å·¥ä½œç°¿
    orig_sheets = read_excel_all_sheets(orig_excel_path)
    trans_sheets = read_excel_all_sheets(trans_excel_path)

    if orig_sheets is None or trans_sheets is None:
        log_manager.log_exception("æ— æ³•è¯»å– Excel æ–‡ä»¶")
        return False

    # è·å–å…±åŒçš„å·¥ä½œç°¿åç§°
    orig_sheet_names = set(orig_sheets.keys())
    trans_sheet_names = set(trans_sheets.keys())
    common_sheets = orig_sheet_names & trans_sheet_names

    if not common_sheets:
        # å¦‚æœå·¥ä½œç°¿åç§°ä¸åŒ¹é…ï¼Œå°è¯•æŒ‰é¡ºåºå¯¹åº”
        log_manager.log("å·¥ä½œç°¿åç§°ä¸åŒ¹é…ï¼Œå°è¯•æŒ‰é¡ºåºå¯¹åº”...")
        orig_list = list(orig_sheets.keys())
        trans_list = list(trans_sheets.keys())
        sheet_pairs = list(zip(orig_list, trans_list))
        log_manager.log(f"æŒ‰é¡ºåºé…å¯¹: {sheet_pairs}")
    else:
        sheet_pairs = [(name, name) for name in sorted(common_sheets)]
        log_manager.log(f"å…±åŒå·¥ä½œç°¿: {list(common_sheets)}")

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸åŒ¹é…çš„å·¥ä½œç°¿
        only_in_orig = orig_sheet_names - trans_sheet_names
        only_in_trans = trans_sheet_names - orig_sheet_names
        if only_in_orig:
            log_manager.log_exception(f"ä»¥ä¸‹å·¥ä½œç°¿ä»…å­˜åœ¨äºåŸæ–‡æ–‡ä»¶ä¸­: {only_in_orig}")
        if only_in_trans:
            log_manager.log_exception(f"ä»¥ä¸‹å·¥ä½œç°¿ä»…å­˜åœ¨äºè¯‘æ–‡æ–‡ä»¶ä¸­: {only_in_trans}")

    all_results = []
    total_cells_processed = 0
    total_rows_split = 0

    log_manager.log_stream("\n" + "=" * 60 + "\n")
    log_manager.log_stream(f"ğŸ“Š å¼€å§‹å¤„ç† Excel åŒæ–‡ä»¶å¯¹é½\n")
    log_manager.log_stream(f"ğŸ“ å…± {len(sheet_pairs)} ä¸ªå·¥ä½œç°¿å¾…å¤„ç†\n")
    log_manager.log_stream("=" * 60 + "\n")

    for sheet_idx, (orig_sheet_name, trans_sheet_name) in enumerate(sheet_pairs):
        log_manager.log(
            f"\nå¤„ç†å·¥ä½œç°¿ {sheet_idx + 1}/{len(sheet_pairs)}: '{orig_sheet_name}' <-> '{trans_sheet_name}'")
        log_manager.log_stream(f"\n{'â”€' * 40}\n")
        log_manager.log_stream(f"ğŸ“‘ å·¥ä½œç°¿: {orig_sheet_name}\n")
        log_manager.log_stream(f"{'â”€' * 40}\n")

        df_orig = orig_sheets[orig_sheet_name]
        df_trans = trans_sheets[trans_sheet_name]

        # è·å–ä¸¤ä¸ªè¡¨æ ¼çš„æœ€å¤§è¡Œåˆ—æ•°
        max_rows = max(df_orig.shape[0], df_trans.shape[0])
        max_cols = max(df_orig.shape[1], df_trans.shape[1])

        log_manager.log(f"  åŸæ–‡è¡¨æ ¼: {df_orig.shape[0]} è¡Œ x {df_orig.shape[1]} åˆ—")
        log_manager.log(f"  è¯‘æ–‡è¡¨æ ¼: {df_trans.shape[0]} è¡Œ x {df_trans.shape[1]} åˆ—")
        log_manager.log(f"  å¤„ç†èŒƒå›´: {max_rows} è¡Œ x {max_cols} åˆ—")

        # éå†æ‰€æœ‰å•å…ƒæ ¼ä½ç½®
        cell_num = 0
        for row_idx in range(max_rows):
            for col_idx in range(max_cols):
                # è·å–åŸæ–‡å•å…ƒæ ¼å†…å®¹
                orig_text = ""
                if row_idx < df_orig.shape[0] and col_idx < df_orig.shape[1]:
                    cell_value = df_orig.iloc[row_idx, col_idx]
                    if pd.notna(cell_value):
                        orig_text = str(cell_value).strip()

                # è·å–è¯‘æ–‡å•å…ƒæ ¼å†…å®¹
                trans_text = ""
                if row_idx < df_trans.shape[0] and col_idx < df_trans.shape[1]:
                    cell_value = df_trans.iloc[row_idx, col_idx]
                    if pd.notna(cell_value):
                        trans_text = str(cell_value).strip()

                # è·³è¿‡ä¸¤è¾¹éƒ½ä¸ºç©ºçš„å•å…ƒæ ¼
                if not orig_text and not trans_text:
                    continue

                cell_num += 1
                total_cells_processed += 1
                cell_ref = f"[{orig_sheet_name}] R{row_idx + 1}C{col_idx + 1}"

                # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†å¥ï¼ˆæ ¹æ®åŸæ–‡è¯­è¨€æ£€æµ‹æ ‡ç‚¹ã€æ¢è¡Œã€åºå·ç­‰ï¼‰
                if orig_text and trans_text and needs_table_cell_split(orig_text, source_lang):
                    log_manager.log(f"  {cell_ref}: éœ€è¦ç»†ç²’åº¦åˆ†å¥")
                    split_results = split_table_cell_with_ai(
                        orig_text, trans_text, model_id, cell_ref, source_lang
                    )

                    if split_results and len(split_results) > 1:
                        # åˆ†å¥æˆåŠŸï¼Œæ·»åŠ æ‰€æœ‰åˆ†å¥ç»“æœ
                        for result in split_results:
                            result['æ¥æº'] = cell_ref
                        all_results.extend(split_results)
                        total_rows_split += 1
                    else:
                        # åˆ†å¥å¤±è´¥æˆ–åªæœ‰ä¸€å¥ï¼Œä¿ç•™åŸå†…å®¹
                        all_results.append({
                            "åŸæ–‡": orig_text,
                            "è¯‘æ–‡": trans_text,
                            "æ¥æº": cell_ref
                        })
                else:
                    # ä¸éœ€è¦åˆ†å¥ï¼Œç›´æ¥æ·»åŠ 
                    if orig_text or trans_text:
                        all_results.append({
                            "åŸæ–‡": orig_text,
                            "è¯‘æ–‡": trans_text,
                            "æ¥æº": cell_ref
                        })
                        if orig_text and trans_text:
                            log_manager.log_stream(f"[{cell_ref}] ç›´æ¥é…å¯¹\n")

        log_manager.log(f"  å·¥ä½œç°¿ '{orig_sheet_name}' å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆå•å…ƒæ ¼: {cell_num}")

    if not all_results:
        log_manager.log_exception("å¤„ç†ç»“æœä¸ºç©º")
        return False

    # åˆ›å»ºç»“æœ DataFrame
    result_df = pd.DataFrame(all_results)

    # å¯é€‰ï¼šç§»é™¤"æ¥æº"åˆ—ï¼ˆå¦‚æœä¸éœ€è¦æ˜¾ç¤ºï¼‰
    # å¦‚æœéœ€è¦ä¿ç•™æ¥æºä¿¡æ¯ç”¨äºè°ƒè¯•ï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢è¿™è¡Œ
    if 'æ¥æº' in result_df.columns:
        result_df_output = result_df[['åŸæ–‡', 'è¯‘æ–‡']].copy()
    else:
        result_df_output = result_df

    # è´¨é‡æ£€æŸ¥ï¼ˆä¼ é€’è¯­è¨€å‚æ•°ï¼‰
    log_manager.log("æ‰§è¡Œè´¨é‡æ£€æŸ¥...")
    issues = AlignmentChecker.full_check(result_df_output, source_lang, target_lang)
    if issues:
        log_manager.log_exception(f"å‘ç° {len(issues)} ä¸ªæ½œåœ¨é—®é¢˜")
        issue_path = output_excel_path.replace('.xlsx', '_é—®é¢˜æŠ¥å‘Š.xlsx')
        save_issues_report(issues, issue_path)

    # ä¿å­˜ç»“æœ
    result_df_output.to_excel(output_excel_path, index=False)

    # è¾“å‡ºç»Ÿè®¡
    log_manager.log_stream("\n" + "=" * 60 + "\n")
    log_manager.log_stream(f"âœ… Excel åŒæ–‡ä»¶å¯¹é½å®Œæˆï¼\n")
    log_manager.log_stream(f"   å¤„ç†å·¥ä½œç°¿æ•°: {len(sheet_pairs)}\n")
    log_manager.log_stream(f"   æœ‰æ•ˆå•å…ƒæ ¼æ•°: {total_cells_processed}\n")
    log_manager.log_stream(f"   AIåˆ†å¥å¤„ç†æ•°: {total_rows_split}\n")
    log_manager.log_stream(f"   æœ€ç»ˆè¾“å‡ºè¡Œæ•°: {len(result_df_output)}\n")
    log_manager.log_stream("=" * 60 + "\n")

    log_manager.log(f"âœ… å¤„ç†å®Œæˆ: {total_cells_processed} ä¸ªå•å…ƒæ ¼ â†’ {len(result_df_output)} è¡Œ")
    log_manager.log(f"ğŸ“ å·²ä¿å­˜: {output_excel_path}")

    return True


def process_excel_alignment(original_excel_path, trans_excel_path, output_excel_path, model_id,
                            source_lang="ä¸­æ–‡", target_lang="è‹±è¯­"):
    """
    å¤„ç† Excel åŒæ–‡ä»¶å¯¹é½ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
    è‡ªåŠ¨æ£€æµ‹å¹¶è°ƒç”¨æ–°çš„åŒæ–‡ä»¶å¤„ç†å‡½æ•°
    """
    # ä½¿ç”¨æ–°çš„åŒæ–‡ä»¶å¤„ç†å‡½æ•°
    return process_excel_dual_file_alignment(original_excel_path, trans_excel_path,
                                             output_excel_path, model_id, source_lang, target_lang)


def merge_and_deduplicate_excels(excel_paths, final_output_path, source_lang="ä¸­æ–‡", target_lang="è‹±è¯­"):
    """åˆå¹¶Excelæ–‡ä»¶ï¼Œé«˜äº®å•åˆ—é‡å¤è¡Œï¼Œå»é™¤å®Œå…¨ç›¸åŒçš„è¡Œ"""
    from openpyxl import load_workbook
    from openpyxl.styles import PatternFill

    log_manager.log(f"åˆå¹¶ {len(excel_paths)} ä¸ªæ–‡ä»¶...")
    log_manager.log(f"å¾…åˆå¹¶æ–‡ä»¶åˆ—è¡¨:")
    for i, path in enumerate(excel_paths):
        exists = "âœ“ å­˜åœ¨" if os.path.exists(path) else "âœ— ä¸å­˜åœ¨"
        log_manager.log(f"  {i + 1}. {os.path.basename(path)} [{exists}]")

    dfs = []
    for path in excel_paths:
        if os.path.exists(path):
            try:
                df = pd.read_excel(path)
                if not df.empty:
                    dfs.append(df)
                    log_manager.log(f"  âœ… è¯»å–æˆåŠŸ: {os.path.basename(path)} ({len(df)} è¡Œ)")
                else:
                    log_manager.log_exception(f"æ–‡ä»¶ä¸ºç©º", os.path.basename(path))
            except Exception as e:
                log_manager.log_exception(f"è¯»å–å¤±è´¥: {e}", path)
        else:
            log_manager.log_exception(f"æ–‡ä»¶ä¸å­˜åœ¨", path)

    if not dfs:
        log_manager.log_exception("æ²¡æœ‰å¯åˆå¹¶çš„æ•°æ®")
        return None

    combined_df = pd.concat(dfs, ignore_index=True)
    total_before = len(combined_df)
    log_manager.log(f"åˆå¹¶åæ€»è¡Œæ•°: {total_before}")

    combined_df.drop_duplicates(subset=['åŸæ–‡', 'è¯‘æ–‡'], keep='first', inplace=True)
    total_after_full_dedup = len(combined_df)
    full_dup_removed = total_before - total_after_full_dedup
    log_manager.log(f"å»é™¤å®Œå…¨é‡å¤: {total_before} -> {total_after_full_dedup} è¡Œ (ç§»é™¤ {full_dup_removed} è¡Œ)")

    combined_df = combined_df.reset_index(drop=True)

    orig_duplicated = combined_df.duplicated(subset=['åŸæ–‡'], keep=False)
    trans_duplicated = combined_df.duplicated(subset=['è¯‘æ–‡'], keep=False)

    highlight_mask = orig_duplicated | trans_duplicated
    highlight_rows = combined_df[highlight_mask].index.tolist()

    orig_dup_count = orig_duplicated.sum()
    trans_dup_count = trans_duplicated.sum()
    log_manager.log(f"å•åˆ—é‡å¤æ£€æµ‹: åŸæ–‡é‡å¤ {orig_dup_count} è¡Œ, è¯‘æ–‡é‡å¤ {trans_dup_count} è¡Œ")

    issues = AlignmentChecker.full_check(combined_df, source_lang, target_lang)
    if issues:
        log_manager.log_exception(f"å‘ç° {len(issues)} ä¸ªæ½œåœ¨é—®é¢˜")
        issue_path = final_output_path.replace('.xlsx', '_é—®é¢˜æŠ¥å‘Š.xlsx')
        save_issues_report(issues, issue_path)

    combined_df.to_excel(final_output_path, index=False)

    try:
        wb = load_workbook(final_output_path)
        ws = wb.active

        yellow_fill = PatternFill(start_color='FFFF00', end_color='FFFF00', fill_type='solid')

        for idx in highlight_rows:
            excel_row = idx + 2
            for col in range(1, 3):
                ws.cell(row=excel_row, column=col).fill = yellow_fill

        wb.save(final_output_path)
        log_manager.log(f"é«˜äº®æ ‡è®°: {len(highlight_rows)} è¡Œï¼ˆç¼“å†²åŒºé‡å å¯¼è‡´çš„å•åˆ—é‡å¤ï¼‰")

    except Exception as e:
        log_manager.log_exception(f"é«˜äº®å¤„ç†å¤±è´¥", str(e))

    log_manager.log(f"âœ… æœ€ç»ˆç»“æœ: {final_output_path} ({len(combined_df)} è¡Œ)")
    return final_output_path


# ==========================================
# === ğŸ–¥ï¸ GUI ç•Œé¢ ===
# ==========================================
class DocumentAlignerGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("ğŸ“„ å¤šè¯­å¯¹ç…§è®°å¿†å·¥å…· v4.3")
        self.root.geometry("1400x950")
        self.root.minsize(1100, 750)

        self.supported_filetypes = [
            ("æ”¯æŒçš„æ–‡ä»¶", "*.docx *.doc *.pptx *.xlsx *.xls"),
            ("Wordæ–‡æ¡£", "*.docx *.doc"),
            ("PowerPoint", "*.pptx"),
            ("Excelè¡¨æ ¼", "*.xlsx *.xls"),
            ("æ‰€æœ‰æ–‡ä»¶", "*.*")
        ]
        self.excel_filetypes = [("Excelè¡¨æ ¼", "*.xlsx *.xls"), ("æ‰€æœ‰æ–‡ä»¶", "*.*")]

        self.running = True
        self.processing_stopped = False

        self.setup_styles()
        self.create_widgets()
        self.update_logs()

        self.root.protocol("WM_DELETE_WINDOW", self.on_closing)

    def setup_styles(self):
        style = ttk.Style()
        style.configure("Title.TLabel", font=("Microsoft YaHei", 14, "bold"))
        style.configure("Section.TLabelframe.Label", font=("Microsoft YaHei", 10, "bold"))
        style.configure("Info.TLabel", font=("Microsoft YaHei", 9))

    def create_widgets(self):
        main_container = ttk.Frame(self.root, padding="10")
        main_container.pack(fill=tk.BOTH, expand=True)

        # è¯­è¨€é€‰æ‹©
        lang_frame = ttk.LabelFrame(main_container, text="ğŸŒ è¯­è¨€è®¾ç½®", padding="10")
        lang_frame.pack(fill=tk.X, pady=(0, 10))

        lang_options = list(SUPPORTED_LANGUAGES.keys())

        ttk.Label(lang_frame, text="åŸæ–‡è¯­è¨€:").grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)
        self.source_lang_var = tk.StringVar(value=DEFAULT_SOURCE_LANG)
        self.source_lang_combo = ttk.Combobox(lang_frame, textvariable=self.source_lang_var,
                                              values=lang_options, width=15, state="readonly")
        self.source_lang_combo.grid(row=0, column=1, padx=5, pady=5, sticky=tk.W)

        ttk.Label(lang_frame, text="è¯‘æ–‡è¯­è¨€:").grid(row=0, column=2, sticky=tk.W, padx=(20, 5), pady=5)
        self.target_lang_var = tk.StringVar(value=DEFAULT_TARGET_LANG)
        self.target_lang_combo = ttk.Combobox(lang_frame, textvariable=self.target_lang_var,
                                              values=lang_options, width=15, state="readonly")
        self.target_lang_combo.grid(row=0, column=3, padx=5, pady=5, sticky=tk.W)

        # è¯­è¨€è¯´æ˜
        self.lang_info_var = tk.StringVar(value="")
        self.lang_info_label = ttk.Label(lang_frame, textvariable=self.lang_info_var, foreground="#666666")
        self.lang_info_label.grid(row=0, column=4, padx=20, pady=5, sticky=tk.W)

        # åå¤„ç†åˆ†å¥é€‰é¡¹ï¼ˆé’ˆå¯¹è‹±æ–‡ç­‰è¥¿æ–¹è¯­è¨€ï¼‰
        self.post_split_var = tk.BooleanVar(value=True)
        self.post_split_check = ttk.Checkbutton(
            lang_frame,
            text="å¯ç”¨åå¤„ç†ç»†ç²’åº¦åˆ†å¥ï¼ˆè‹±æ–‡ç­‰è¥¿æ–¹è¯­è¨€æŒ‰å¥å·ç»†åˆ†ï¼Œè‡ªåŠ¨æ’é™¤ç¼©å†™ï¼‰",
            variable=self.post_split_var
        )
        self.post_split_check.grid(row=1, column=0, columnspan=5, sticky=tk.W, padx=5, pady=(5, 0))

        # åå¤„ç†åˆ†å¥è¯´æ˜
        post_split_info = ttk.Label(
            lang_frame,
            text="ğŸ’¡ å½“æºè¯­è¨€ä¸ºè‹±æ–‡ç­‰è¥¿æ–¹è¯­è¨€æ—¶ï¼Œä¼šåœ¨LLMå¯¹é½åè¿›ä¸€æ­¥æŒ‰å¥å·ç»†åˆ†",
            foreground="#0066cc"
        )
        post_split_info.grid(row=2, column=0, columnspan=5, sticky=tk.W, padx=5, pady=(2, 0))

        self.source_lang_combo.bind("<<ComboboxSelected>>", self.on_lang_changed)
        self.target_lang_combo.bind("<<ComboboxSelected>>", self.on_lang_changed)
        self.on_lang_changed()  # åˆå§‹åŒ–æ˜¾ç¤º

        # å¤„ç†æ¨¡å¼é€‰æ‹©
        mode_frame = ttk.LabelFrame(main_container, text="ğŸ“‹ å¤„ç†æ¨¡å¼", padding="10")
        mode_frame.pack(fill=tk.X, pady=(0, 10))

        self.mode_var = tk.StringVar(value="dual_file")
        ttk.Radiobutton(mode_frame, text="å¸¸è§„æ¨¡å¼ï¼ˆWord/PPT/Excelï¼šåˆ†åˆ«é€‰æ‹©åŸæ–‡å’Œè¯‘æ–‡æ–‡ä»¶ï¼‰",
                        variable=self.mode_var, value="dual_file",
                        command=self.on_mode_changed).pack(anchor=tk.W, pady=2)

        # æ¨¡å¼è¯´æ˜
        self.mode_info_label = ttk.Label(mode_frame,
                                         text="ğŸ’¡ åŒæ–‡ä»¶æ¨¡å¼æ”¯æŒ Excelï¼šä¸¤ä¸ªè¡¨æ ¼æŒ‰å•å…ƒæ ¼ä½ç½®å¯¹åº”ï¼Œæ”¯æŒå¤šå·¥ä½œç°¿",
                                         foreground="#0066cc")
        self.mode_info_label.pack(anchor=tk.W, pady=(5, 0))

        # æ–‡ä»¶é€‰æ‹©
        self.file_frame = ttk.LabelFrame(main_container, text="ğŸ“ æ–‡ä»¶é€‰æ‹©", padding="10")
        self.file_frame.pack(fill=tk.X, pady=(0, 10))

        self.original_label = ttk.Label(self.file_frame, text="åŸæ–‡æ–‡ä»¶ (ä¸­æ–‡):")
        self.original_label.grid(row=0, column=0, sticky=tk.W, pady=5)
        self.original_path_var = tk.StringVar()
        self.original_entry = ttk.Entry(self.file_frame, textvariable=self.original_path_var, width=80)
        self.original_entry.grid(row=0, column=1, padx=5, pady=5, sticky=tk.EW)
        self.original_button = ttk.Button(self.file_frame, text="æµè§ˆ...", command=self.browse_original)
        self.original_button.grid(row=0, column=2, padx=5)

        self.trans_label = ttk.Label(self.file_frame, text="è¯‘æ–‡æ–‡ä»¶ (è‹±æ–‡):")
        self.trans_label.grid(row=1, column=0, sticky=tk.W, pady=5)
        self.trans_path_var = tk.StringVar()
        self.trans_entry = ttk.Entry(self.file_frame, textvariable=self.trans_path_var, width=80)
        self.trans_entry.grid(row=1, column=1, padx=5, pady=5, sticky=tk.EW)
        self.trans_button = ttk.Button(self.file_frame, text="æµè§ˆ...", command=self.browse_trans)
        self.trans_button.grid(row=1, column=2, padx=5)

        ttk.Label(self.file_frame, text="è¾“å‡ºç›®å½•:").grid(row=2, column=0, sticky=tk.W, pady=5)
        self.output_path_var = tk.StringVar(value=os.path.abspath(OUTPUT_DIR))
        ttk.Entry(self.file_frame, textvariable=self.output_path_var, width=80).grid(row=2, column=1, padx=5, pady=5,
                                                                                     sticky=tk.EW)
        ttk.Button(self.file_frame, text="æµè§ˆ...", command=self.browse_output).grid(row=2, column=2, padx=5)

        self.file_frame.columnconfigure(1, weight=1)

        # æ›´æ–°æ–‡ä»¶æ ‡ç­¾ï¼ˆåœ¨æ ‡ç­¾åˆ›å»ºåï¼‰
        self.update_file_labels()

        # æ¨¡å‹é€‰æ‹©
        model_frame = ttk.LabelFrame(main_container, text="ğŸ¤– æ¨¡å‹é€‰æ‹©", padding="10")
        model_frame.pack(fill=tk.X, pady=(0, 10))

        # æä¾›å•†åˆ‡æ¢
        provider_frame = ttk.Frame(model_frame)
        provider_frame.grid(row=0, column=0, columnspan=4, sticky=tk.W, pady=(0, 10))

        ttk.Label(provider_frame, text="API æä¾›å•†:", font=("Microsoft YaHei", 9, "bold")).pack(side=tk.LEFT,
                                                                                                padx=(0, 10))

        self.provider_var = tk.StringVar(value=DEFAULT_PROVIDER)
        # è·¯æ™ºæ·± API å·²å±è”½ï¼Œä¸å†æ˜¾ç¤ºè¯¥é€‰é¡¹
        # self.luzhishen_radio = ttk.Radiobutton(provider_frame, text="ğŸš€ è·¯æ™ºæ·±",
        #                                        variable=self.provider_var, value="luzhishen",
        #                                        command=self.on_provider_changed)
        # self.luzhishen_radio.pack(side=tk.LEFT, padx=5)

        self.openrouter_radio = ttk.Radiobutton(provider_frame, text="ğŸŒ OpenRouter",
                                                variable=self.provider_var, value="openrouter",
                                                command=self.on_provider_changed)
        self.openrouter_radio.pack(side=tk.LEFT, padx=5)

        self.provider_info_label = ttk.Label(provider_frame, text="", foreground="#0066cc")
        self.provider_info_label.pack(side=tk.LEFT, padx=20)

        # æ¨¡å‹é€‰æ‹©
        ttk.Label(model_frame, text="é€‰æ‹©æ¨¡å‹:").grid(row=1, column=0, sticky=tk.W, pady=5)

        self.model_var = tk.StringVar(value=DEFAULT_MODEL)
        self.model_combo = ttk.Combobox(model_frame, textvariable=self.model_var,
                                        values=list(AVAILABLE_MODELS.keys()), width=30, state="readonly")
        self.model_combo.grid(row=1, column=1, padx=5, pady=5, sticky=tk.W)
        self.model_combo.bind("<<ComboboxSelected>>", self.on_model_selected)

        self.model_info_var = tk.StringVar()
        ttk.Label(model_frame, textvariable=self.model_info_var, foreground="#666666").grid(row=1, column=2, padx=20,
                                                                                            pady=5, sticky=tk.W)

        # åˆå§‹åŒ–æä¾›å•†ä¿¡æ¯
        detail_frame = ttk.Frame(model_frame)
        detail_frame.grid(row=2, column=0, columnspan=3, sticky=tk.EW, pady=(10, 0))

        ttk.Label(detail_frame, text="æ¨¡å‹ID:", font=("Microsoft YaHei", 9, "bold")).grid(row=0, column=0, sticky=tk.W)
        self.model_id_var = tk.StringVar()
        ttk.Label(detail_frame, textvariable=self.model_id_var, foreground="#0066cc").grid(row=0, column=1, sticky=tk.W,
                                                                                           padx=10)

        ttk.Label(detail_frame, text="æœ€å¤§è¾“å‡º:", font=("Microsoft YaHei", 9, "bold")).grid(row=0, column=2,
                                                                                            sticky=tk.W, padx=(30, 0))
        self.max_output_var = tk.StringVar()
        ttk.Label(detail_frame, textvariable=self.max_output_var, foreground="#009900").grid(row=0, column=3,
                                                                                             sticky=tk.W, padx=10)

        # åˆå§‹åŒ–æä¾›å•†å’Œæ¨¡å‹ä¿¡æ¯ï¼ˆåœ¨æ‰€æœ‰å˜é‡åˆ›å»ºä¹‹åï¼‰
        self.on_provider_changed()

        # é…ç½®é€‰é¡¹
        config_frame = ttk.LabelFrame(main_container, text="âš™ï¸ å¤„ç†é€‰é¡¹ï¼ˆä»…å¯¹ DOCX ç”Ÿæ•ˆï¼‰", padding="10")
        config_frame.pack(fill=tk.X, pady=(0, 10))

        ttk.Label(config_frame, text="åˆ†å‰²é˜ˆå€¼(å­—æ•°):").grid(row=0, column=0, sticky=tk.W, padx=5)

        threshold_frame = ttk.Frame(config_frame)
        threshold_frame.grid(row=0, column=1, columnspan=3, sticky=tk.W)

        ttk.Label(threshold_frame, text="2ä»½:").pack(side=tk.LEFT)
        self.threshold_2_var = tk.StringVar(value=str(THRESHOLD_2_PARTS))
        ttk.Entry(threshold_frame, textvariable=self.threshold_2_var, width=8).pack(side=tk.LEFT, padx=(2, 15))

        ttk.Label(threshold_frame, text="3ä»½:").pack(side=tk.LEFT)
        self.threshold_3_var = tk.StringVar(value=str(THRESHOLD_3_PARTS))
        ttk.Entry(threshold_frame, textvariable=self.threshold_3_var, width=8).pack(side=tk.LEFT, padx=(2, 15))

        ttk.Label(threshold_frame, text="4ä»½:").pack(side=tk.LEFT)
        self.threshold_4_var = tk.StringVar(value=str(THRESHOLD_4_PARTS))
        ttk.Entry(threshold_frame, textvariable=self.threshold_4_var, width=8).pack(side=tk.LEFT, padx=(2, 15))

        ttk.Label(threshold_frame, text="5ä»½:").pack(side=tk.LEFT)
        self.threshold_5_var = tk.StringVar(value=str(THRESHOLD_5_PARTS))
        ttk.Entry(threshold_frame, textvariable=self.threshold_5_var, width=8).pack(side=tk.LEFT, padx=(2, 15))
        ttk.Label(threshold_frame, text="6ä»½:").pack(side=tk.LEFT)
        self.threshold_6_var = tk.StringVar(value=str(THRESHOLD_6_PARTS))
        ttk.Entry(threshold_frame, textvariable=self.threshold_6_var, width=8).pack(side=tk.LEFT, padx=(2, 15))
        ttk.Label(threshold_frame, text="7ä»½:").pack(side=tk.LEFT)
        self.threshold_7_var = tk.StringVar(value=str(THRESHOLD_7_PARTS))
        ttk.Entry(threshold_frame, textvariable=self.threshold_7_var, width=8).pack(side=tk.LEFT, padx=(2, 15))
        ttk.Label(threshold_frame, text="8ä»½:").pack(side=tk.LEFT)
        self.threshold_8_var = tk.StringVar(value=str(THRESHOLD_8_PARTS))
        ttk.Entry(threshold_frame, textvariable=self.threshold_8_var, width=8).pack(side=tk.LEFT, padx=(2, 15))

        ttk.Label(threshold_frame, text="ç¼“å†²åŒº:").pack(side=tk.LEFT, padx=(20, 0))
        self.buffer_var = tk.StringVar(value=str(BUFFER_CHARS))
        ttk.Entry(threshold_frame, textvariable=self.buffer_var, width=6).pack(side=tk.LEFT, padx=(2, 0))
        ttk.Label(threshold_frame, text="å­—").pack(side=tk.LEFT)

        # æ§åˆ¶æŒ‰é’®
        control_frame = ttk.Frame(main_container)
        control_frame.pack(fill=tk.X, pady=10)

        self.start_button = ttk.Button(control_frame, text="ğŸš€ å¼€å§‹å¤„ç†", command=self.start_processing)
        self.start_button.pack(side=tk.LEFT, padx=5)

        self.stop_button = ttk.Button(control_frame, text="â¹ åœæ­¢", command=self.stop_processing, state=tk.DISABLED)
        self.stop_button.pack(side=tk.LEFT, padx=5)

        ttk.Button(control_frame, text="ğŸ—‘ æ¸…ç©ºæ—¥å¿—", command=self.clear_logs).pack(side=tk.LEFT, padx=5)
        ttk.Button(control_frame, text="ğŸ“‚ æ‰“å¼€è¾“å‡ºç›®å½•", command=self.open_output_dir).pack(side=tk.LEFT, padx=5)

        self.progress_var = tk.DoubleVar()
        ttk.Progressbar(control_frame, variable=self.progress_var, maximum=100, length=300).pack(side=tk.RIGHT, padx=5)

        self.status_label = ttk.Label(control_frame, text="å°±ç»ª")
        self.status_label.pack(side=tk.RIGHT, padx=10)

        # æ—¥å¿—åŒºåŸŸ
        paned = ttk.PanedWindow(main_container, orient=tk.VERTICAL)
        paned.pack(fill=tk.BOTH, expand=True)

        stream_frame = ttk.LabelFrame(paned, text="ğŸŒŠ æ¨¡å‹è¾“å‡ºæµ", padding="5")
        self.stream_text = scrolledtext.ScrolledText(stream_frame, height=12, wrap=tk.WORD,
                                                     font=("Consolas", 9), bg="#1e1e1e", fg="#00ff00")
        self.stream_text.pack(fill=tk.BOTH, expand=True)
        paned.add(stream_frame, weight=2)

        bottom_paned = ttk.PanedWindow(paned, orient=tk.HORIZONTAL)
        paned.add(bottom_paned, weight=1)

        log_frame = ttk.LabelFrame(bottom_paned, text="ğŸ“‹ è¿è¡Œæ—¥å¿—", padding="5")
        self.log_text = scrolledtext.ScrolledText(log_frame, height=10, wrap=tk.WORD,
                                                  font=("Microsoft YaHei", 9), bg="#f5f5f5")
        self.log_text.pack(fill=tk.BOTH, expand=True)
        bottom_paned.add(log_frame, weight=1)

        exception_frame = ttk.LabelFrame(bottom_paned, text="âš ï¸ å¼‚å¸¸/è­¦å‘Š", padding="5")
        self.exception_text = scrolledtext.ScrolledText(exception_frame, height=10, wrap=tk.WORD,
                                                        font=("Microsoft YaHei", 9), bg="#fff0f0", fg="#cc0000")
        self.exception_text.pack(fill=tk.BOTH, expand=True)
        bottom_paned.add(exception_frame, weight=1)

    def on_lang_changed(self, event=None):
        """è¯­è¨€é€‰æ‹©å˜åŒ–æ—¶æ›´æ–°æ˜¾ç¤º"""
        source_lang = self.source_lang_var.get()
        target_lang = self.target_lang_var.get()

        source_info = SUPPORTED_LANGUAGES.get(source_lang, {})
        target_info = SUPPORTED_LANGUAGES.get(target_lang, {})

        source_desc = source_info.get('description', source_lang)
        target_desc = target_info.get('description', target_lang)

        self.lang_info_var.set(f"{source_desc} â†’ {target_desc}")

        # æ›´æ–°æ–‡ä»¶é€‰æ‹©æ ‡ç­¾
        self.update_file_labels()

    def update_file_labels(self):
        """æ›´æ–°æ–‡ä»¶é€‰æ‹©æ ‡ç­¾"""
        source_lang = self.source_lang_var.get()
        target_lang = self.target_lang_var.get()

        # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦å·²åˆ›å»º
        if hasattr(self, 'original_label'):
            self.original_label.config(text=f"åŸæ–‡æ–‡ä»¶ ({source_lang}):")
        if hasattr(self, 'trans_label'):
            self.trans_label.config(text=f"è¯‘æ–‡æ–‡ä»¶ ({target_lang}):")

    def on_mode_changed(self):
        source_lang = self.source_lang_var.get()
        target_lang = self.target_lang_var.get()
        self.original_label.config(text=f"åŸæ–‡æ–‡ä»¶ ({source_lang}):")
        self.trans_label.config(text=f"è¯‘æ–‡æ–‡ä»¶ ({target_lang}):")
        self.mode_info_label.config(text="ğŸ’¡ åŒæ–‡ä»¶æ¨¡å¼æ”¯æŒ Excelï¼šä¸¤ä¸ªè¡¨æ ¼æŒ‰å•å…ƒæ ¼ä½ç½®å¯¹åº”ï¼Œæ”¯æŒå¤šå·¥ä½œç°¿")

    def on_provider_changed(self, event=None):
        """åˆ‡æ¢ API æä¾›å•†"""
        global AVAILABLE_MODELS
        provider = self.provider_var.get()
        # è·¯æ™ºæ·±å·²å±è”½ï¼Œè‹¥ä¸ºæ—§é…ç½®åˆ™å¼ºåˆ¶ä½¿ç”¨ OpenRouter
        if provider == "luzhishen":
            provider = "openrouter"
            self.provider_var.set("openrouter")

        AVAILABLE_MODELS = OPENROUTER_MODELS.copy()
        default_model = "Google Gemini 2.5 Flash"
        self.provider_info_label.config(text="HTTP åè®®ï¼ŒGemini æ¨¡å‹")

        # æ›´æ–°æ¨¡å‹ä¸‹æ‹‰æ¡†
        self.model_combo['values'] = list(AVAILABLE_MODELS.keys())

        # è®¾ç½®é»˜è®¤æ¨¡å‹
        if default_model in AVAILABLE_MODELS:
            self.model_var.set(default_model)
        elif AVAILABLE_MODELS:
            self.model_var.set(list(AVAILABLE_MODELS.keys())[0])

        self.update_model_info()
        self.update_model_details()
        log_manager.log(f"åˆ‡æ¢åˆ°æä¾›å•†: {provider}")

    def on_model_selected(self, event=None):
        self.update_model_info()
        self.update_model_details()

    def update_model_info(self):
        model_name = self.model_var.get()
        if model_name in AVAILABLE_MODELS:
            self.model_info_var.set(AVAILABLE_MODELS[model_name]['description'])

    def update_model_details(self):
        model_name = self.model_var.get()
        if model_name in AVAILABLE_MODELS:
            info = AVAILABLE_MODELS[model_name]
            self.model_id_var.set(info['id'])
            self.max_output_var.set(f"{info['max_output']:,} tokens")

    def browse_original(self):
        path = filedialog.askopenfilename(title="é€‰æ‹©åŸæ–‡æ–‡ä»¶", filetypes=self.supported_filetypes)
        if path:
            self.original_path_var.set(path)

    def browse_trans(self):
        path = filedialog.askopenfilename(title="é€‰æ‹©è¯‘æ–‡æ–‡ä»¶", filetypes=self.supported_filetypes)
        if path:
            self.trans_path_var.set(path)

    def browse_output(self):
        path = filedialog.askdirectory(title="é€‰æ‹©è¾“å‡ºç›®å½•")
        if path:
            self.output_path_var.set(path)

    def open_output_dir(self):
        output_dir = self.output_path_var.get()
        if os.path.exists(output_dir):
            os.startfile(output_dir)
        else:
            messagebox.showwarning("æç¤º", "è¾“å‡ºç›®å½•ä¸å­˜åœ¨")

    def clear_logs(self):
        self.log_text.delete(1.0, tk.END)
        self.exception_text.delete(1.0, tk.END)
        self.stream_text.delete(1.0, tk.END)

    def update_logs(self):
        if not self.running:
            return

        while not log_manager.log_queue.empty():
            try:
                msg = log_manager.log_queue.get_nowait()
                self.log_text.insert(tk.END, msg + "\n")
                self.log_text.see(tk.END)
            except queue.Empty:
                break

        while not log_manager.exception_queue.empty():
            try:
                msg = log_manager.exception_queue.get_nowait()
                self.exception_text.insert(tk.END, msg + "\n")
                self.exception_text.see(tk.END)
            except queue.Empty:
                break

        while not log_manager.stream_queue.empty():
            try:
                msg = log_manager.stream_queue.get_nowait()
                scrollbar_position = self.stream_text.yview()[1]
                is_at_bottom = scrollbar_position > 0.95
                self.stream_text.insert(tk.END, msg)
                if is_at_bottom:
                    self.stream_text.see(tk.END)
            except queue.Empty:
                break

        self.root.after(100, self.update_logs)

    def validate_inputs(self):
        if not API_KEY:
            messagebox.showerror("é”™è¯¯", "è¯·åœ¨ä»£ç ä¸­å¡«å…¥ API_KEYï¼")
            return False

        if not self.original_path_var.get().strip():
            messagebox.showerror("é”™è¯¯", "è¯·é€‰æ‹©æ–‡ä»¶ï¼")
            return False

        if not os.path.exists(self.original_path_var.get()):
            messagebox.showerror("é”™è¯¯", "æ–‡ä»¶ä¸å­˜åœ¨ï¼")
            return False

        if not self.trans_path_var.get().strip():
            messagebox.showerror("é”™è¯¯", "è¯·é€‰æ‹©è¯‘æ–‡æ–‡ä»¶ï¼")
            return False
        if not os.path.exists(self.trans_path_var.get()):
            messagebox.showerror("é”™è¯¯", "è¯‘æ–‡æ–‡ä»¶ä¸å­˜åœ¨ï¼")
            return False

        orig_type = get_file_type(self.original_path_var.get())
        trans_type = get_file_type(self.trans_path_var.get())

        if orig_type == 'unknown':
            messagebox.showerror("é”™è¯¯", "åŸæ–‡æ–‡ä»¶ç±»å‹ä¸æ”¯æŒï¼")
            return False
        if trans_type == 'unknown':
            messagebox.showerror("é”™è¯¯", "è¯‘æ–‡æ–‡ä»¶ç±»å‹ä¸æ”¯æŒï¼")
            return False

        # ç»Ÿä¸€ç±»å‹åˆ¤æ–­
        orig_type_normalized = 'word' if orig_type in ['doc', 'docx'] else orig_type
        trans_type_normalized = 'word' if trans_type in ['doc', 'docx'] else trans_type

        if orig_type_normalized != trans_type_normalized:
            messagebox.showerror("é”™è¯¯", "åŸæ–‡å’Œè¯‘æ–‡æ–‡ä»¶ç±»å‹å¿…é¡»ç›¸åŒï¼")
            return False

        return True

    def start_processing(self):
        if not self.validate_inputs():
            return

        self.start_button.config(state=tk.DISABLED)
        self.stop_button.config(state=tk.NORMAL)
        self.status_label.config(text="å¤„ç†ä¸­...")
        self.progress_var.set(0)
        self.processing_stopped = False

        self.processing_thread = threading.Thread(target=self.run_processing, daemon=True)
        self.processing_thread.start()

    def stop_processing(self):
        self.processing_stopped = True
        self.status_label.config(text="æ­£åœ¨åœæ­¢...")
        log_manager.log("ç”¨æˆ·è¯·æ±‚åœæ­¢...")

    def run_processing(self):
        """ä¸»å¤„ç†æµç¨‹"""
        try:
            mode = self.mode_var.get()
            original_path = self.original_path_var.get().strip()
            output_dir = self.output_path_var.get().strip()

            # è·å–è¯­è¨€è®¾ç½®
            source_lang = self.source_lang_var.get()
            target_lang = self.target_lang_var.get()
            enable_post_split = self.post_split_var.get()

            model_name = self.model_var.get()
            model_id = AVAILABLE_MODELS[model_name]['id']

            base_name = os.path.splitext(os.path.basename(original_path))[0]
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

            task_dir = os.path.join(output_dir, f"{base_name}_{timestamp}")
            temp_dir = os.path.join(task_dir, "ä¸­é—´æ–‡ä»¶")

            os.makedirs(task_dir, exist_ok=True)
            os.makedirs(temp_dir, exist_ok=True)

            log_manager.log("=" * 60)
            log_manager.log("ğŸ“„ å¤šè¯­å¯¹ç…§è®°å¿†å·¥å…· v4.3")
            log_manager.log("=" * 60)
            log_manager.log(f"å¤„ç†æ¨¡å¼: åŒæ–‡ä»¶æ¨¡å¼")
            log_manager.log(f"è¯­è¨€å¯¹: {source_lang} â†’ {target_lang}")
            log_manager.log(f"åå¤„ç†åˆ†å¥: {'å¯ç”¨' if enable_post_split else 'ç¦ç”¨'}")
            log_manager.log(f"æ¨¡å‹: {model_name}")
            log_manager.log(f"è¾“å‡ºç›®å½•: {task_dir}")
            log_manager.log("=" * 60)

            # åŒæ–‡ä»¶æ¨¡å¼
            trans_path = self.trans_path_var.get().strip()
            file_type = get_file_type(original_path)

            try:
                threshold_2 = int(self.threshold_2_var.get())
                threshold_3 = int(self.threshold_3_var.get())
                threshold_4 = int(self.threshold_4_var.get())
                threshold_5 = int(self.threshold_5_var.get())
                threshold_6 = int(self.threshold_6_var.get())
                threshold_7 = int(self.threshold_7_var.get())
                threshold_8 = int(self.threshold_8_var.get())
                buffer_chars = int(self.buffer_var.get())
            except ValueError:
                threshold_2, threshold_3, threshold_4 = THRESHOLD_2_PARTS, THRESHOLD_3_PARTS, THRESHOLD_4_PARTS
                threshold_5, threshold_6, threshold_7, threshold_8 = THRESHOLD_5_PARTS, THRESHOLD_6_PARTS, THRESHOLD_7_PARTS, THRESHOLD_8_PARTS
                buffer_chars = BUFFER_CHARS

            log_manager.log(f"æ–‡ä»¶ç±»å‹: {file_type.upper()}")

            # å¤„ç† .doc æ–‡ä»¶
            if file_type == 'doc':
                log_manager.log("æ£€æµ‹åˆ° .doc æ–‡ä»¶ï¼Œæ­£åœ¨è½¬æ¢ä¸º .docx...")
                self.progress_var.set(5)

                converted_orig = convert_doc_to_docx(original_path, temp_dir)
                converted_trans = convert_doc_to_docx(trans_path, temp_dir)

                if converted_orig is None or converted_trans is None:
                    log_manager.log_exception("æ— æ³•è½¬æ¢ .doc æ–‡ä»¶")
                    self.root.after(0, lambda: self.status_label.config(text="è½¬æ¢å¤±è´¥"))
                    return

                original_path = converted_orig
                trans_path = converted_trans
                file_type = 'docx'

            # å¤„ç† Excel åŒæ–‡ä»¶æ¨¡å¼ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
            if file_type == 'excel':
                log_manager.log("ã€Œé˜¶æ®µä¸€ã€å¤„ç† Excel è¡¨æ ¼æ–‡ä»¶ï¼ˆåŒæ–‡ä»¶æ¨¡å¼ - æŒ‰å•å…ƒæ ¼ä½ç½®å¯¹åº”ï¼‰...")
                self.progress_var.set(20)

                excel_name = f"{base_name}_å¯¹é½ç»“æœ.xlsx"
                out_path = os.path.join(task_dir, excel_name)

                success = process_excel_dual_file_alignment(original_path, trans_path, out_path, model_id,
                                                            source_lang=source_lang, target_lang=target_lang)

                self.progress_var.set(100)

                if success:
                    log_manager.log("ğŸ‰ å¤„ç†å®Œæˆï¼")
                    log_manager.log(f"ğŸ“ æœ€ç»ˆç»“æœ: {out_path}")
                    self.root.after(0, lambda: self.status_label.config(text="å®Œæˆï¼"))
                    self.root.after(0, lambda: messagebox.showinfo("å®Œæˆ", f"å¤„ç†å®Œæˆï¼\n\næœ€ç»ˆç»“æœ:\n{out_path}"))
                else:
                    self.root.after(0, lambda: self.status_label.config(text="å¤„ç†å¤±è´¥"))
                return

            # Word/PPT å¤„ç†
            log_manager.log("ã€Œé˜¶æ®µä¸€ã€æ–‡æ¡£åˆ†æ...")
            self.progress_var.set(10)

            count_a, _ = analyze_document_structure(original_path, source_lang)
            count_b, _ = analyze_document_structure(trans_path, target_lang)

            # æ ¹æ®è¯­è¨€ç±»å‹æ˜¾ç¤ºå­—/è¯
            source_info = SUPPORTED_LANGUAGES.get(source_lang, {})
            target_info = SUPPORTED_LANGUAGES.get(target_lang, {})
            source_unit = "å­—" if not source_info.get('word_based', True) else "è¯"
            target_unit = "å­—" if not target_info.get('word_based', True) else "è¯"

            log_manager.log(f"åŸæ–‡: {count_a:,} {source_unit}")
            log_manager.log(f"è¯‘æ–‡: {count_b:,} {target_unit}")

            if file_type == 'pptx':
                split_parts = 1
                log_manager.log("PPTæ–‡ä»¶ï¼šä¸è¿›è¡Œåˆ†å‰²")
            else:
                max_count = max(count_a, count_b)
                split_parts = 1
                if max_count > threshold_8:
                    split_parts = 8
                elif max_count > threshold_7:
                    split_parts = 7
                elif max_count > threshold_6:
                    split_parts = 6
                elif max_count > threshold_5:
                    split_parts = 5
                elif max_count > threshold_4:
                    split_parts = 4
                elif max_count > threshold_3:
                    split_parts = 3
                elif max_count > threshold_2:
                    split_parts = 2
                log_manager.log(f"åˆ†å‰²ç­–ç•¥: {split_parts} ä»½")

            self.progress_var.set(20)

            tasks_queue = []
            generated_excel_paths = []

            if split_parts > 1 and file_type == 'docx':
                log_manager.log(f"ã€Œé˜¶æ®µäºŒã€æ–‡ä»¶åˆ†å‰²ï¼ˆç¼“å†²åŒº: {buffer_chars} å­—ï¼‰")

                # å…ˆåˆ†å‰²åŸæ–‡ï¼Œå¾—åˆ°åˆ†å‰²æ¯”ä¾‹ï¼›å†ç”¨ç›¸åŒæ¯”ä¾‹åˆ†å‰²è¯‘æ–‡ï¼Œç¡®ä¿å†…å®¹å¯¹é½
                log_manager.log("åˆ†å‰²åŸæ–‡ï¼ˆä¸»æ–‡æ¡£ï¼Œè‡ªä¸»è®¡ç®—åˆ†å‰²ç‚¹ï¼‰...")
                files_a, part_info_a, split_ratios = smart_split_with_buffer(
                    original_path, split_parts, temp_dir, source_lang, buffer_chars)
                log_manager.log("åˆ†å‰²è¯‘æ–‡ï¼ˆä»æ–‡æ¡£ï¼Œä½¿ç”¨åŸæ–‡çš„åˆ†å‰²æ¯”ä¾‹ï¼‰...")
                files_b, part_info_b, _ = smart_split_with_buffer(
                    trans_path, split_parts, temp_dir, target_lang, buffer_chars,
                    split_element_ratios=split_ratios)

                for i in range(len(files_a)):
                    excel_name = f"Part{i + 1}_å¯¹é½ç»“æœ.xlsx"
                    out_path = os.path.join(temp_dir, excel_name)
                    tasks_queue.append({
                        'original': files_a[i],
                        'trans': files_b[i],
                        'output': out_path,
                        'anchor_orig': part_info_a[i] if part_info_a else None,
                        'anchor_trans': part_info_b[i] if part_info_b else None,
                        'source_lang': source_lang,
                        'target_lang': target_lang,
                    })
                    generated_excel_paths.append(out_path)
            else:
                excel_name = f"{base_name}_å¯¹é½ç»“æœ.xlsx"
                out_path = os.path.join(task_dir, excel_name)
                # PPT ä½¿ç”¨åŠ¨æ€ç”Ÿæˆçš„æç¤ºè¯
                ppt_prompt = get_ppt_alignment_prompt(source_lang, target_lang) if file_type == 'pptx' else None
                tasks_queue.append({
                    'original': original_path,
                    'trans': trans_path,
                    'output': out_path,
                    'anchor_orig': None,
                    'anchor_trans': None,
                    'system_prompt_override': ppt_prompt,
                    'source_lang': source_lang,
                    'target_lang': target_lang,
                })
                generated_excel_paths.append(out_path)

            self.progress_var.set(30)

            log_manager.log("ã€Œé˜¶æ®µä¸‰ã€AI å¯¹é½")
            log_manager.log(f"å¾…å¤„ç†ä»»åŠ¡æ•°: {len(tasks_queue)}")
            for i, task in enumerate(tasks_queue):
                log_manager.log(f"  ä»»åŠ¡ {i + 1}: {os.path.basename(task['output'])}")

            progress_per_task = 50 / len(tasks_queue) if tasks_queue else 50

            for idx, task in enumerate(tasks_queue):
                if self.processing_stopped:
                    log_manager.log("å¤„ç†å·²åœæ­¢")
                    break

                log_manager.log(f"")
                log_manager.log(f"{'=' * 40}")
                log_manager.log(f"å¤„ç†ä»»åŠ¡ {idx + 1}/{len(tasks_queue)}: {os.path.basename(task['output'])}")
                log_manager.log(f"åŸæ–‡æ–‡ä»¶: {os.path.basename(task['original'])}")
                log_manager.log(f"è¯‘æ–‡æ–‡ä»¶: {os.path.basename(task['trans'])}")

                success = run_llm_alignment(
                    task['original'],
                    task['trans'],
                    task['output'],
                    model_id,
                    anchor_info_orig=task['anchor_orig'],
                    anchor_info_trans=task['anchor_trans'],
                    system_prompt_override=task.get('system_prompt_override'),
                    source_lang=task.get('source_lang', source_lang),
                    target_lang=task.get('target_lang', target_lang),
                    enable_post_split=enable_post_split
                )

                # æ£€æŸ¥å¤„ç†ç»“æœ
                output_file = task['output']
                if success:
                    if os.path.exists(output_file):
                        log_manager.log(f"âœ… ä»»åŠ¡ {idx + 1} æˆåŠŸ: {os.path.basename(output_file)}")
                    else:
                        log_manager.log_exception(f"âš ï¸ ä»»åŠ¡ {idx + 1} è¿”å›æˆåŠŸä½†æ–‡ä»¶ä¸å­˜åœ¨: {output_file}")
                        if output_file in generated_excel_paths:
                            generated_excel_paths.remove(output_file)
                else:
                    log_manager.log_exception(f"âŒ ä»»åŠ¡ {idx + 1} å¤±è´¥: {os.path.basename(output_file)}")
                    if output_file in generated_excel_paths:
                        generated_excel_paths.remove(output_file)

                self.progress_var.set(30 + (idx + 1) * progress_per_task)

            # æ˜¾ç¤ºæˆåŠŸç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
            log_manager.log(f"")
            log_manager.log(f"æˆåŠŸç”Ÿæˆçš„æ–‡ä»¶æ•°: {len(generated_excel_paths)}")
            for path in generated_excel_paths:
                log_manager.log(f"  - {os.path.basename(path)}")

            final_path = None
            if not self.processing_stopped:
                if split_parts > 1 and len(generated_excel_paths) > 0:
                    log_manager.log("")
                    log_manager.log("ã€Œé˜¶æ®µå››ã€åˆå¹¶ä¸å»é‡")
                    final_path = os.path.join(task_dir, f"ã€Œæœ€ç»ˆç»“æœã€{base_name}_å¯¹é½.xlsx")
                    merge_and_deduplicate_excels(generated_excel_paths, final_path, source_lang, target_lang)
                else:
                    final_path = generated_excel_paths[0] if generated_excel_paths else None

            self.progress_var.set(100)

            if self.processing_stopped:
                self.root.after(0, lambda: self.status_label.config(text="å·²åœæ­¢"))
            else:
                log_manager.log("ğŸ‰ å¤„ç†å®Œæˆï¼")
                if final_path and os.path.exists(final_path):
                    log_manager.log(f"ğŸ“ æœ€ç»ˆç»“æœ: {final_path}")

                self.root.after(0, lambda: self.status_label.config(text="å®Œæˆï¼"))
                self.root.after(0, lambda: messagebox.showinfo(
                    "å®Œæˆ",
                    f"å¤„ç†å®Œæˆï¼\n\næœ€ç»ˆç»“æœ:\n{final_path}\n\nä»»åŠ¡ç›®å½•:\n{task_dir}"
                ))

        except Exception as e:
            log_manager.log_exception(f"å¤„ç†å‡ºé”™", str(e))
            import traceback
            log_manager.log_exception("è¯¦ç»†é”™è¯¯", traceback.format_exc())
            self.root.after(0, lambda: self.status_label.config(text="é”™è¯¯"))

        finally:
            self.root.after(0, self.reset_buttons)

    def reset_buttons(self):
        self.start_button.config(state=tk.NORMAL)
        self.stop_button.config(state=tk.DISABLED)

    def on_closing(self):
        self.running = False
        self.processing_stopped = True
        self.root.destroy()


# ==========================================
# === ğŸ¬ ä¸»ç¨‹åº ===
# ==========================================
def main():
    root = tk.Tk()
    app = DocumentAlignerGUI(root)
    root.mainloop()


if __name__ == "__main__":
    main()